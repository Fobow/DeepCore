================== Exp 0 ==================
dataset: CIFAR10, model: ResNet18, selection: GraNd, num_ex: 10, epochs: 100, fraction: 0.2, seed: 73952, lr: 0.1, save_path: ./result, resume: , device: cuda, checkpoint_name: CIFAR10_ResNet18_GraNd_exp0_epoch100_2024-03-18 15:16:19.566571_0.2_
Files already downloaded and verified
Files already downloaded and verified
called grand, with 10 ensemble and 1 epochs
=> Early Training Epoch #0
| Epoch [  0/  1] Iter[  1/391]		Loss: 2.3862
| Epoch [  0/  1] Iter[ 21/391]		Loss: 3.0040
| Epoch [  0/  1] Iter[ 41/391]		Loss: 2.3288
| Epoch [  0/  1] Iter[ 61/391]		Loss: 2.2027
| Epoch [  0/  1] Iter[ 81/391]		Loss: 2.1233
| Epoch [  0/  1] Iter[101/391]		Loss: 2.0255
| Epoch [  0/  1] Iter[121/391]		Loss: 1.9132
| Epoch [  0/  1] Iter[141/391]		Loss: 2.0567
| Epoch [  0/  1] Iter[161/391]		Loss: 2.0906
| Epoch [  0/  1] Iter[181/391]		Loss: 1.8514
| Epoch [  0/  1] Iter[201/391]		Loss: 1.8936
| Epoch [  0/  1] Iter[221/391]		Loss: 1.7786
| Epoch [  0/  1] Iter[241/391]		Loss: 1.6575
| Epoch [  0/  1] Iter[261/391]		Loss: 1.6862
| Epoch [  0/  1] Iter[281/391]		Loss: 1.7194
| Epoch [  0/  1] Iter[301/391]		Loss: 1.4746
| Epoch [  0/  1] Iter[321/391]		Loss: 1.6551
| Epoch [  0/  1] Iter[341/391]		Loss: 1.5557
| Epoch [  0/  1] Iter[361/391]		Loss: 1.5547
| Epoch [  0/  1] Iter[381/391]		Loss: 1.5598
=> Early Training Epoch #0
| Epoch [  0/  1] Iter[  1/391]		Loss: 2.5003
| Epoch [  0/  1] Iter[ 21/391]		Loss: 2.2609
| Epoch [  0/  1] Iter[ 41/391]		Loss: 2.2839
| Epoch [  0/  1] Iter[ 61/391]		Loss: 2.1060
| Epoch [  0/  1] Iter[ 81/391]		Loss: 2.0596
| Epoch [  0/  1] Iter[101/391]		Loss: 1.7810
| Epoch [  0/  1] Iter[121/391]		Loss: 1.8756
| Epoch [  0/  1] Iter[141/391]		Loss: 1.8082
| Epoch [  0/  1] Iter[161/391]		Loss: 1.8027
| Epoch [  0/  1] Iter[181/391]		Loss: 1.7798
| Epoch [  0/  1] Iter[201/391]		Loss: 1.6612
| Epoch [  0/  1] Iter[221/391]		Loss: 1.6764
| Epoch [  0/  1] Iter[241/391]		Loss: 1.6809
| Epoch [  0/  1] Iter[261/391]		Loss: 1.7022
| Epoch [  0/  1] Iter[281/391]		Loss: 1.6981
| Epoch [  0/  1] Iter[301/391]		Loss: 1.4806
| Epoch [  0/  1] Iter[321/391]		Loss: 1.5534
| Epoch [  0/  1] Iter[341/391]		Loss: 1.3547
| Epoch [  0/  1] Iter[361/391]		Loss: 1.6431
| Epoch [  0/  1] Iter[381/391]		Loss: 1.5058
=> Early Training Epoch #0
| Epoch [  0/  1] Iter[  1/391]		Loss: 2.3938
| Epoch [  0/  1] Iter[ 21/391]		Loss: 2.2868
| Epoch [  0/  1] Iter[ 41/391]		Loss: 2.0190
| Epoch [  0/  1] Iter[ 61/391]		Loss: 1.8812
| Epoch [  0/  1] Iter[ 81/391]		Loss: 2.0205
| Epoch [  0/  1] Iter[101/391]		Loss: 1.9068
| Epoch [  0/  1] Iter[121/391]		Loss: 1.7544
| Epoch [  0/  1] Iter[141/391]		Loss: 1.9098
| Epoch [  0/  1] Iter[161/391]		Loss: 1.6111
| Epoch [  0/  1] Iter[181/391]		Loss: 1.5664
| Epoch [  0/  1] Iter[201/391]		Loss: 1.4630
| Epoch [  0/  1] Iter[221/391]		Loss: 1.5127
| Epoch [  0/  1] Iter[241/391]		Loss: 1.5839
| Epoch [  0/  1] Iter[261/391]		Loss: 1.6290
| Epoch [  0/  1] Iter[281/391]		Loss: 1.3870
| Epoch [  0/  1] Iter[301/391]		Loss: 1.3445
| Epoch [  0/  1] Iter[321/391]		Loss: 1.3115
| Epoch [  0/  1] Iter[341/391]		Loss: 1.5851
| Epoch [  0/  1] Iter[361/391]		Loss: 1.3320
| Epoch [  0/  1] Iter[381/391]		Loss: 1.4377
=> Early Training Epoch #0
| Epoch [  0/  1] Iter[  1/391]		Loss: 2.4535
| Epoch [  0/  1] Iter[ 21/391]		Loss: 2.5309
| Epoch [  0/  1] Iter[ 41/391]		Loss: 2.1210
| Epoch [  0/  1] Iter[ 61/391]		Loss: 2.0082
| Epoch [  0/  1] Iter[ 81/391]		Loss: 2.0287
| Epoch [  0/  1] Iter[101/391]		Loss: 1.8516
| Epoch [  0/  1] Iter[121/391]		Loss: 1.7362
| Epoch [  0/  1] Iter[141/391]		Loss: 1.8076
| Epoch [  0/  1] Iter[161/391]		Loss: 1.5382
| Epoch [  0/  1] Iter[181/391]		Loss: 1.5313
| Epoch [  0/  1] Iter[201/391]		Loss: 1.6215
| Epoch [  0/  1] Iter[221/391]		Loss: 1.7624
| Epoch [  0/  1] Iter[241/391]		Loss: 1.4102
| Epoch [  0/  1] Iter[261/391]		Loss: 1.5013
| Epoch [  0/  1] Iter[281/391]		Loss: 1.4505
| Epoch [  0/  1] Iter[301/391]		Loss: 1.3520
| Epoch [  0/  1] Iter[321/391]		Loss: 1.3555
| Epoch [  0/  1] Iter[341/391]		Loss: 1.2256
| Epoch [  0/  1] Iter[361/391]		Loss: 1.5827
| Epoch [  0/  1] Iter[381/391]		Loss: 1.3573
=> Early Training Epoch #0
| Epoch [  0/  1] Iter[  1/391]		Loss: 2.3428
| Epoch [  0/  1] Iter[ 21/391]		Loss: 2.4581
| Epoch [  0/  1] Iter[ 41/391]		Loss: 2.1579
| Epoch [  0/  1] Iter[ 61/391]		Loss: 2.0897
| Epoch [  0/  1] Iter[ 81/391]		Loss: 1.9918
| Epoch [  0/  1] Iter[101/391]		Loss: 1.9882
| Epoch [  0/  1] Iter[121/391]		Loss: 1.9183
| Epoch [  0/  1] Iter[141/391]		Loss: 1.9977
| Epoch [  0/  1] Iter[161/391]		Loss: 1.9442
| Epoch [  0/  1] Iter[181/391]		Loss: 1.7805
| Epoch [  0/  1] Iter[201/391]		Loss: 1.8451
| Epoch [  0/  1] Iter[221/391]		Loss: 1.6342
| Epoch [  0/  1] Iter[241/391]		Loss: 1.7083
| Epoch [  0/  1] Iter[261/391]		Loss: 1.7137
| Epoch [  0/  1] Iter[281/391]		Loss: 1.6831
| Epoch [  0/  1] Iter[301/391]		Loss: 1.6939
| Epoch [  0/  1] Iter[321/391]		Loss: 1.6632
| Epoch [  0/  1] Iter[341/391]		Loss: 1.6499
| Epoch [  0/  1] Iter[361/391]		Loss: 1.5154
| Epoch [  0/  1] Iter[381/391]		Loss: 1.6031
=> Early Training Epoch #0
| Epoch [  0/  1] Iter[  1/391]		Loss: 2.3945
| Epoch [  0/  1] Iter[ 21/391]		Loss: 3.6221
| Epoch [  0/  1] Iter[ 41/391]		Loss: 2.2776
| Epoch [  0/  1] Iter[ 61/391]		Loss: 2.0756
| Epoch [  0/  1] Iter[ 81/391]		Loss: 2.0519
| Epoch [  0/  1] Iter[101/391]		Loss: 2.0125
| Epoch [  0/  1] Iter[121/391]		Loss: 1.9037
| Epoch [  0/  1] Iter[141/391]		Loss: 1.9716
| Epoch [  0/  1] Iter[161/391]		Loss: 2.1175
| Epoch [  0/  1] Iter[181/391]		Loss: 1.7717
| Epoch [  0/  1] Iter[201/391]		Loss: 1.7791
| Epoch [  0/  1] Iter[221/391]		Loss: 1.8342
| Epoch [  0/  1] Iter[241/391]		Loss: 1.5385
| Epoch [  0/  1] Iter[261/391]		Loss: 1.5691
| Epoch [  0/  1] Iter[281/391]		Loss: 1.5226
| Epoch [  0/  1] Iter[301/391]		Loss: 1.4891
| Epoch [  0/  1] Iter[321/391]		Loss: 1.5629
| Epoch [  0/  1] Iter[341/391]		Loss: 1.6032
| Epoch [  0/  1] Iter[361/391]		Loss: 1.7241
| Epoch [  0/  1] Iter[381/391]		Loss: 1.4488
=> Early Training Epoch #0
| Epoch [  0/  1] Iter[  1/391]		Loss: 2.3760
| Epoch [  0/  1] Iter[ 21/391]		Loss: 2.3338
| Epoch [  0/  1] Iter[ 41/391]		Loss: 2.2694
| Epoch [  0/  1] Iter[ 61/391]		Loss: 2.1999
| Epoch [  0/  1] Iter[ 81/391]		Loss: 2.2890
| Epoch [  0/  1] Iter[101/391]		Loss: 2.0402
| Epoch [  0/  1] Iter[121/391]		Loss: 2.0859
| Epoch [  0/  1] Iter[141/391]		Loss: 1.9028
| Epoch [  0/  1] Iter[161/391]		Loss: 2.0041
| Epoch [  0/  1] Iter[181/391]		Loss: 2.0244
| Epoch [  0/  1] Iter[201/391]		Loss: 1.8844
| Epoch [  0/  1] Iter[221/391]		Loss: 1.7946
| Epoch [  0/  1] Iter[241/391]		Loss: 1.8651
| Epoch [  0/  1] Iter[261/391]		Loss: 1.8037
| Epoch [  0/  1] Iter[281/391]		Loss: 1.7604
| Epoch [  0/  1] Iter[301/391]		Loss: 1.7131
| Epoch [  0/  1] Iter[321/391]		Loss: 1.7933
| Epoch [  0/  1] Iter[341/391]		Loss: 1.6300
| Epoch [  0/  1] Iter[361/391]		Loss: 1.7141
| Epoch [  0/  1] Iter[381/391]		Loss: 1.6943
=> Early Training Epoch #0
| Epoch [  0/  1] Iter[  1/391]		Loss: 2.3699
| Epoch [  0/  1] Iter[ 21/391]		Loss: 2.6392
| Epoch [  0/  1] Iter[ 41/391]		Loss: 2.3179
| Epoch [  0/  1] Iter[ 61/391]		Loss: 2.3513
| Epoch [  0/  1] Iter[ 81/391]		Loss: 2.2228
| Epoch [  0/  1] Iter[101/391]		Loss: 2.0746
| Epoch [  0/  1] Iter[121/391]		Loss: 1.9691
| Epoch [  0/  1] Iter[141/391]		Loss: 2.0022
| Epoch [  0/  1] Iter[161/391]		Loss: 1.9454
| Epoch [  0/  1] Iter[181/391]		Loss: 1.9615
| Epoch [  0/  1] Iter[201/391]		Loss: 1.7241
| Epoch [  0/  1] Iter[221/391]		Loss: 1.8030
| Epoch [  0/  1] Iter[241/391]		Loss: 1.6819
| Epoch [  0/  1] Iter[261/391]		Loss: 1.7853
| Epoch [  0/  1] Iter[281/391]		Loss: 1.6278
| Epoch [  0/  1] Iter[301/391]		Loss: 1.6228
| Epoch [  0/  1] Iter[321/391]		Loss: 1.7675
| Epoch [  0/  1] Iter[341/391]		Loss: 1.5339
| Epoch [  0/  1] Iter[361/391]		Loss: 1.5789
| Epoch [  0/  1] Iter[381/391]		Loss: 1.5370
=> Early Training Epoch #0
| Epoch [  0/  1] Iter[  1/391]		Loss: 2.3951
| Epoch [  0/  1] Iter[ 21/391]		Loss: 2.4644
| Epoch [  0/  1] Iter[ 41/391]		Loss: 2.2126
| Epoch [  0/  1] Iter[ 61/391]		Loss: 2.0324
| Epoch [  0/  1] Iter[ 81/391]		Loss: 1.9994
| Epoch [  0/  1] Iter[101/391]		Loss: 1.9091
| Epoch [  0/  1] Iter[121/391]		Loss: 1.9030
| Epoch [  0/  1] Iter[141/391]		Loss: 1.8275
| Epoch [  0/  1] Iter[161/391]		Loss: 1.7736
| Epoch [  0/  1] Iter[181/391]		Loss: 1.6488
| Epoch [  0/  1] Iter[201/391]		Loss: 1.7172
| Epoch [  0/  1] Iter[221/391]		Loss: 1.5138
| Epoch [  0/  1] Iter[241/391]		Loss: 1.5041
| Epoch [  0/  1] Iter[261/391]		Loss: 1.5427
| Epoch [  0/  1] Iter[281/391]		Loss: 1.5083
| Epoch [  0/  1] Iter[301/391]		Loss: 1.3511
| Epoch [  0/  1] Iter[321/391]		Loss: 1.6053
| Epoch [  0/  1] Iter[341/391]		Loss: 1.3264
| Epoch [  0/  1] Iter[361/391]		Loss: 1.4523
| Epoch [  0/  1] Iter[381/391]		Loss: 1.4406
=> Early Training Epoch #0
| Epoch [  0/  1] Iter[  1/391]		Loss: 2.4750
| Epoch [  0/  1] Iter[ 21/391]		Loss: 2.3268
| Epoch [  0/  1] Iter[ 41/391]		Loss: 2.1932
| Epoch [  0/  1] Iter[ 61/391]		Loss: 2.0152
| Epoch [  0/  1] Iter[ 81/391]		Loss: 2.0084
| Epoch [  0/  1] Iter[101/391]		Loss: 2.0052
| Epoch [  0/  1] Iter[121/391]		Loss: 2.0033
| Epoch [  0/  1] Iter[141/391]		Loss: 1.8627
| Epoch [  0/  1] Iter[161/391]		Loss: 1.8144
| Epoch [  0/  1] Iter[181/391]		Loss: 1.6915
| Epoch [  0/  1] Iter[201/391]		Loss: 1.7968
| Epoch [  0/  1] Iter[221/391]		Loss: 1.5569
| Epoch [  0/  1] Iter[241/391]		Loss: 1.3918
| Epoch [  0/  1] Iter[261/391]		Loss: 1.4288
| Epoch [  0/  1] Iter[281/391]		Loss: 1.5361
| Epoch [  0/  1] Iter[301/391]		Loss: 1.3951
| Epoch [  0/  1] Iter[321/391]		Loss: 1.4839
| Epoch [  0/  1] Iter[341/391]		Loss: 1.3172
| Epoch [  0/  1] Iter[361/391]		Loss: 1.3844
| Epoch [  0/  1] Iter[381/391]		Loss: 1.4923
=> selecting time:  163.3533968925476
=> number of seletcted samples:  10000
=> Saving checkpoint for epoch 0, with Prec@1 0.000000.
Epoch: [0][0/79]	Time 3.280 (3.280)	Loss 2.3927 (2.3927)	Prec@1 8.594 (8.594)
Epoch: [0][20/79]	Time 0.043 (0.199)	Loss 3.7497 (4.2146)	Prec@1 13.281 (10.789)
Epoch: [0][40/79]	Time 0.042 (0.125)	Loss 2.2599 (3.3674)	Prec@1 17.188 (11.414)
Epoch: [0][60/79]	Time 0.043 (0.099)	Loss 2.1558 (2.9747)	Prec@1 19.531 (13.422)
training time:  7.025264263153076
Test: [0/79]	Time 0.724 (0.724)	Loss 2.1415 (2.1415)	Prec@1 15.625 (15.625)
Test: [20/79]	Time 0.032 (0.065)	Loss 2.1288 (2.1422)	Prec@1 15.625 (19.048)
Test: [40/79]	Time 0.032 (0.049)	Loss 2.1077 (2.1429)	Prec@1 14.062 (18.445)
Test: [60/79]	Time 0.031 (0.044)	Loss 2.1030 (2.1392)	Prec@1 18.750 (18.327)
 * Prec@1 18.210
=> Saving checkpoint for epoch 0, with Prec@1 18.210000.
Epoch: [1][0/79]	Time 0.770 (0.770)	Loss 2.1623 (2.1623)	Prec@1 17.188 (17.188)
Epoch: [1][20/79]	Time 0.046 (0.081)	Loss 2.0534 (2.1311)	Prec@1 21.875 (17.708)
Epoch: [1][40/79]	Time 0.046 (0.064)	Loss 1.9458 (2.1044)	Prec@1 25.781 (18.960)
Epoch: [1][60/79]	Time 0.044 (0.058)	Loss 2.0588 (2.0852)	Prec@1 9.375 (19.096)
training time:  4.535623788833618
Test: [0/79]	Time 0.740 (0.740)	Loss 2.0939 (2.0939)	Prec@1 24.219 (24.219)
Test: [20/79]	Time 0.033 (0.066)	Loss 2.2423 (2.1827)	Prec@1 19.531 (19.754)
Test: [40/79]	Time 0.029 (0.049)	Loss 2.1734 (2.1814)	Prec@1 21.875 (19.722)
Test: [60/79]	Time 0.031 (0.044)	Loss 2.0967 (2.1725)	Prec@1 21.094 (19.787)
 * Prec@1 19.890
=> Saving checkpoint for epoch 1, with Prec@1 19.890000.
Epoch: [2][0/79]	Time 0.757 (0.757)	Loss 2.1314 (2.1314)	Prec@1 15.625 (15.625)
Epoch: [2][20/79]	Time 0.045 (0.080)	Loss 1.9696 (2.0204)	Prec@1 23.438 (20.126)
Epoch: [2][40/79]	Time 0.045 (0.063)	Loss 1.9775 (2.0074)	Prec@1 25.781 (21.132)
Epoch: [2][60/79]	Time 0.045 (0.058)	Loss 1.9388 (1.9964)	Prec@1 28.906 (21.939)
training time:  4.488401889801025
Test: [0/79]	Time 0.732 (0.732)	Loss 2.0972 (2.0972)	Prec@1 14.844 (14.844)
Test: [20/79]	Time 0.033 (0.064)	Loss 2.1434 (2.1063)	Prec@1 11.719 (14.025)
Test: [40/79]	Time 0.030 (0.047)	Loss 2.1046 (2.1061)	Prec@1 9.375 (14.272)
Test: [60/79]	Time 0.032 (0.041)	Loss 2.0932 (2.1000)	Prec@1 12.500 (14.626)
 * Prec@1 14.590
Epoch: [3][0/79]	Time 0.785 (0.785)	Loss 2.0950 (2.0950)	Prec@1 17.969 (17.969)
Epoch: [3][20/79]	Time 0.046 (0.081)	Loss 1.9716 (2.0072)	Prec@1 22.656 (20.796)
Epoch: [3][40/79]	Time 0.047 (0.064)	Loss 1.8516 (1.9779)	Prec@1 25.781 (22.027)
Epoch: [3][60/79]	Time 0.046 (0.058)	Loss 1.9378 (1.9667)	Prec@1 19.531 (22.541)
training time:  4.642467021942139
Test: [0/79]	Time 0.764 (0.764)	Loss 2.1560 (2.1560)	Prec@1 20.312 (20.312)
Test: [20/79]	Time 0.032 (0.067)	Loss 2.2022 (2.1529)	Prec@1 12.500 (15.216)
Test: [40/79]	Time 0.032 (0.050)	Loss 2.1290 (2.1425)	Prec@1 26.562 (15.663)
Test: [60/79]	Time 0.032 (0.044)	Loss 2.0470 (2.1346)	Prec@1 21.875 (15.971)
 * Prec@1 15.790
Epoch: [4][0/79]	Time 0.813 (0.813)	Loss 2.0423 (2.0423)	Prec@1 25.781 (25.781)
Epoch: [4][20/79]	Time 0.046 (0.083)	Loss 1.8663 (1.9352)	Prec@1 29.688 (24.368)
Epoch: [4][40/79]	Time 0.047 (0.065)	Loss 1.9596 (1.9326)	Prec@1 21.875 (24.066)
Epoch: [4][60/79]	Time 0.047 (0.059)	Loss 1.9305 (1.9187)	Prec@1 17.969 (24.744)
training time:  4.555941343307495
Test: [0/79]	Time 0.779 (0.779)	Loss 2.0255 (2.0255)	Prec@1 22.656 (22.656)
Test: [20/79]	Time 0.028 (0.065)	Loss 2.0553 (1.9836)	Prec@1 15.625 (18.452)
Test: [40/79]	Time 0.029 (0.048)	Loss 1.9676 (1.9748)	Prec@1 16.406 (18.331)
Test: [60/79]	Time 0.031 (0.043)	Loss 1.9136 (1.9758)	Prec@1 23.438 (18.199)
 * Prec@1 18.400
Epoch: [5][0/79]	Time 0.778 (0.778)	Loss 1.8598 (1.8598)	Prec@1 28.906 (28.906)
Epoch: [5][20/79]	Time 0.046 (0.082)	Loss 1.8783 (1.9007)	Prec@1 26.562 (26.600)
Epoch: [5][40/79]	Time 0.047 (0.064)	Loss 1.9384 (1.8866)	Prec@1 21.094 (26.639)
Epoch: [5][60/79]	Time 0.047 (0.058)	Loss 1.9730 (1.8963)	Prec@1 26.562 (26.230)
training time:  4.496842622756958
Test: [0/79]	Time 0.756 (0.756)	Loss 2.0265 (2.0265)	Prec@1 21.875 (21.875)
Test: [20/79]	Time 0.034 (0.067)	Loss 2.1274 (2.0409)	Prec@1 17.188 (20.015)
Test: [40/79]	Time 0.033 (0.050)	Loss 2.0656 (2.0346)	Prec@1 19.531 (19.703)
Test: [60/79]	Time 0.032 (0.044)	Loss 2.0519 (2.0382)	Prec@1 19.531 (19.442)
 * Prec@1 19.720
Epoch: [6][0/79]	Time 0.786 (0.786)	Loss 2.0076 (2.0076)	Prec@1 18.750 (18.750)
Epoch: [6][20/79]	Time 0.043 (0.082)	Loss 1.8502 (1.8776)	Prec@1 28.125 (27.381)
Epoch: [6][40/79]	Time 0.046 (0.064)	Loss 2.0090 (1.8720)	Prec@1 24.219 (27.363)
Epoch: [6][60/79]	Time 0.046 (0.058)	Loss 1.8197 (1.8670)	Prec@1 28.125 (27.433)
training time:  4.565276861190796
Test: [0/79]	Time 0.752 (0.752)	Loss 1.9465 (1.9465)	Prec@1 21.875 (21.875)
Test: [20/79]	Time 0.032 (0.066)	Loss 2.0931 (1.9772)	Prec@1 17.969 (19.457)
Test: [40/79]	Time 0.032 (0.050)	Loss 2.0006 (1.9755)	Prec@1 11.719 (18.998)
Test: [60/79]	Time 0.031 (0.044)	Loss 1.9027 (1.9732)	Prec@1 14.062 (18.929)
 * Prec@1 19.060
Epoch: [7][0/79]	Time 0.789 (0.789)	Loss 2.0211 (2.0211)	Prec@1 18.750 (18.750)
Epoch: [7][20/79]	Time 0.045 (0.082)	Loss 2.0419 (1.8786)	Prec@1 22.656 (25.595)
Epoch: [7][40/79]	Time 0.046 (0.064)	Loss 1.7990 (1.8795)	Prec@1 25.781 (25.877)
Epoch: [7][60/79]	Time 0.045 (0.058)	Loss 1.8573 (1.8598)	Prec@1 32.031 (27.369)
training time:  4.542172193527222
Test: [0/79]	Time 0.785 (0.785)	Loss 2.5602 (2.5602)	Prec@1 18.750 (18.750)
Test: [20/79]	Time 0.029 (0.067)	Loss 2.6056 (2.4788)	Prec@1 14.844 (17.671)
Test: [40/79]	Time 0.029 (0.050)	Loss 2.6205 (2.4711)	Prec@1 15.625 (17.759)
Test: [60/79]	Time 0.032 (0.044)	Loss 2.2928 (2.4867)	Prec@1 21.094 (17.495)
 * Prec@1 17.500
Epoch: [8][0/79]	Time 0.839 (0.839)	Loss 2.1013 (2.1013)	Prec@1 21.094 (21.094)
Epoch: [8][20/79]	Time 0.044 (0.084)	Loss 1.8055 (1.9317)	Prec@1 34.375 (25.223)
Epoch: [8][40/79]	Time 0.047 (0.066)	Loss 1.7975 (1.8885)	Prec@1 28.906 (27.096)
Epoch: [8][60/79]	Time 0.047 (0.061)	Loss 1.8511 (1.8609)	Prec@1 29.688 (28.061)
training time:  4.708003759384155
Test: [0/79]	Time 0.780 (0.780)	Loss 2.0489 (2.0489)	Prec@1 17.969 (17.969)
Test: [20/79]	Time 0.031 (0.068)	Loss 2.1214 (2.0503)	Prec@1 16.406 (19.457)
Test: [40/79]	Time 0.031 (0.050)	Loss 1.9950 (2.0436)	Prec@1 19.531 (19.646)
Test: [60/79]	Time 0.031 (0.044)	Loss 1.9214 (2.0422)	Prec@1 25.000 (19.314)
 * Prec@1 19.450
Epoch: [9][0/79]	Time 0.829 (0.829)	Loss 1.9237 (1.9237)	Prec@1 24.219 (24.219)
Epoch: [9][20/79]	Time 0.048 (0.083)	Loss 1.6730 (1.8362)	Prec@1 32.812 (28.199)
Epoch: [9][40/79]	Time 0.045 (0.065)	Loss 1.7755 (1.8164)	Prec@1 27.344 (29.954)
Epoch: [9][60/79]	Time 0.045 (0.059)	Loss 1.8457 (1.8213)	Prec@1 23.438 (29.841)
training time:  4.606935262680054
Test: [0/79]	Time 0.792 (0.792)	Loss 2.0121 (2.0121)	Prec@1 23.438 (23.438)
Test: [20/79]	Time 0.029 (0.066)	Loss 2.1693 (2.0593)	Prec@1 21.875 (20.536)
Test: [40/79]	Time 0.032 (0.048)	Loss 2.0364 (2.0539)	Prec@1 18.750 (20.332)
Test: [60/79]	Time 0.032 (0.043)	Loss 2.0856 (2.0581)	Prec@1 17.188 (20.018)
 * Prec@1 20.150
=> Saving checkpoint for epoch 9, with Prec@1 20.150000.
Epoch: [10][0/79]	Time 0.826 (0.826)	Loss 1.7387 (1.7387)	Prec@1 37.500 (37.500)
Epoch: [10][20/79]	Time 0.046 (0.083)	Loss 1.8189 (1.8034)	Prec@1 29.688 (30.729)
Epoch: [10][40/79]	Time 0.048 (0.065)	Loss 1.8654 (1.7882)	Prec@1 27.344 (31.307)
Epoch: [10][60/79]	Time 0.046 (0.059)	Loss 1.7172 (1.7828)	Prec@1 34.375 (31.890)
training time:  4.580093145370483
Test: [0/79]	Time 0.790 (0.790)	Loss 2.0734 (2.0734)	Prec@1 21.094 (21.094)
Test: [20/79]	Time 0.029 (0.066)	Loss 2.2060 (2.1065)	Prec@1 12.500 (20.424)
Test: [40/79]	Time 0.032 (0.049)	Loss 2.1017 (2.0888)	Prec@1 24.219 (21.132)
Test: [60/79]	Time 0.031 (0.043)	Loss 2.0479 (2.0843)	Prec@1 21.094 (20.914)
 * Prec@1 20.860
=> Saving checkpoint for epoch 10, with Prec@1 20.860000.
Epoch: [11][0/79]	Time 0.831 (0.831)	Loss 1.8726 (1.8726)	Prec@1 22.656 (22.656)
Epoch: [11][20/79]	Time 0.047 (0.084)	Loss 1.6629 (1.7687)	Prec@1 37.500 (33.110)
Epoch: [11][40/79]	Time 0.046 (0.065)	Loss 1.6748 (1.7485)	Prec@1 36.719 (33.975)
Epoch: [11][60/79]	Time 0.044 (0.059)	Loss 1.6740 (1.7413)	Prec@1 32.812 (33.978)
training time:  4.537356615066528
Test: [0/79]	Time 0.790 (0.790)	Loss 1.9846 (1.9846)	Prec@1 27.344 (27.344)
Test: [20/79]	Time 0.032 (0.068)	Loss 2.0568 (1.9570)	Prec@1 20.312 (24.888)
Test: [40/79]	Time 0.031 (0.050)	Loss 1.8866 (1.9565)	Prec@1 32.031 (24.257)
Test: [60/79]	Time 0.031 (0.044)	Loss 1.9164 (1.9548)	Prec@1 17.969 (23.783)
 * Prec@1 24.000
=> Saving checkpoint for epoch 11, with Prec@1 24.000000.
Epoch: [12][0/79]	Time 0.771 (0.771)	Loss 1.7899 (1.7899)	Prec@1 31.250 (31.250)
Epoch: [12][20/79]	Time 0.047 (0.081)	Loss 1.8966 (1.7366)	Prec@1 26.562 (34.449)
Epoch: [12][40/79]	Time 0.046 (0.064)	Loss 1.8240 (1.7170)	Prec@1 37.500 (35.061)
Epoch: [12][60/79]	Time 0.046 (0.064)	Loss 1.6336 (1.7134)	Prec@1 37.500 (35.092)
training time:  4.869695663452148
Test: [0/79]	Time 0.760 (0.760)	Loss 1.8838 (1.8838)	Prec@1 25.781 (25.781)
Test: [20/79]	Time 0.032 (0.067)	Loss 2.0766 (1.9551)	Prec@1 20.312 (25.000)
Test: [40/79]	Time 0.032 (0.050)	Loss 1.9361 (1.9549)	Prec@1 32.031 (25.019)
Test: [60/79]	Time 0.030 (0.044)	Loss 2.0101 (1.9580)	Prec@1 20.312 (24.757)
 * Prec@1 25.140
=> Saving checkpoint for epoch 12, with Prec@1 25.140000.
Epoch: [13][0/79]	Time 0.799 (0.799)	Loss 1.7283 (1.7283)	Prec@1 37.500 (37.500)
Epoch: [13][20/79]	Time 0.047 (0.082)	Loss 1.5824 (1.7157)	Prec@1 36.719 (34.226)
Epoch: [13][40/79]	Time 0.047 (0.065)	Loss 1.8470 (1.6913)	Prec@1 32.031 (35.842)
Epoch: [13][60/79]	Time 0.047 (0.059)	Loss 1.8497 (1.6800)	Prec@1 33.594 (36.475)
training time:  4.547630786895752
Test: [0/79]	Time 0.768 (0.768)	Loss 1.9661 (1.9661)	Prec@1 29.688 (29.688)
Test: [20/79]	Time 0.031 (0.067)	Loss 2.0647 (2.0057)	Prec@1 22.656 (25.930)
Test: [40/79]	Time 0.031 (0.050)	Loss 2.0307 (2.0198)	Prec@1 31.250 (25.191)
Test: [60/79]	Time 0.028 (0.044)	Loss 2.0685 (2.0240)	Prec@1 20.312 (24.898)
 * Prec@1 25.160
=> Saving checkpoint for epoch 13, with Prec@1 25.160000.
Epoch: [14][0/79]	Time 0.794 (0.794)	Loss 1.7641 (1.7641)	Prec@1 35.938 (35.938)
Epoch: [14][20/79]	Time 0.045 (0.079)	Loss 1.6164 (1.6604)	Prec@1 39.062 (37.760)
Epoch: [14][40/79]	Time 0.045 (0.063)	Loss 1.6235 (1.6560)	Prec@1 35.156 (37.652)
Epoch: [14][60/79]	Time 0.044 (0.057)	Loss 1.5024 (1.6464)	Prec@1 39.844 (38.102)
training time:  4.458026647567749
Test: [0/79]	Time 0.762 (0.762)	Loss 2.4424 (2.4424)	Prec@1 16.406 (16.406)
Test: [20/79]	Time 0.032 (0.065)	Loss 2.3742 (2.3624)	Prec@1 18.750 (19.531)
Test: [40/79]	Time 0.032 (0.049)	Loss 2.2082 (2.3272)	Prec@1 22.656 (20.274)
Test: [60/79]	Time 0.031 (0.043)	Loss 2.6016 (2.3187)	Prec@1 17.188 (20.530)
 * Prec@1 20.860
Epoch: [15][0/79]	Time 0.806 (0.806)	Loss 1.7292 (1.7292)	Prec@1 40.625 (40.625)
Epoch: [15][20/79]	Time 0.047 (0.082)	Loss 1.6375 (1.6249)	Prec@1 39.844 (38.579)
Epoch: [15][40/79]	Time 0.047 (0.064)	Loss 1.4221 (1.6135)	Prec@1 42.969 (38.357)
Epoch: [15][60/79]	Time 0.047 (0.058)	Loss 1.5119 (1.6030)	Prec@1 46.875 (39.524)
training time:  4.5152058601379395
Test: [0/79]	Time 0.768 (0.768)	Loss 1.8516 (1.8516)	Prec@1 32.031 (32.031)
Test: [20/79]	Time 0.031 (0.067)	Loss 1.9759 (1.9342)	Prec@1 27.344 (26.414)
Test: [40/79]	Time 0.032 (0.050)	Loss 1.9162 (1.9345)	Prec@1 29.688 (26.601)
Test: [60/79]	Time 0.031 (0.044)	Loss 1.9762 (1.9355)	Prec@1 26.562 (26.550)
 * Prec@1 27.240
=> Saving checkpoint for epoch 15, with Prec@1 27.240000.
Epoch: [16][0/79]	Time 0.796 (0.796)	Loss 1.6143 (1.6143)	Prec@1 45.312 (45.312)
Epoch: [16][20/79]	Time 0.043 (0.082)	Loss 1.5492 (1.5527)	Prec@1 42.188 (41.741)
Epoch: [16][40/79]	Time 0.043 (0.068)	Loss 1.4104 (1.5499)	Prec@1 55.469 (42.245)
Epoch: [16][60/79]	Time 0.045 (0.061)	Loss 1.5635 (1.5518)	Prec@1 41.406 (42.200)
training time:  4.684269905090332
Test: [0/79]	Time 0.762 (0.762)	Loss 2.0221 (2.0221)	Prec@1 28.906 (28.906)
Test: [20/79]	Time 0.032 (0.065)	Loss 2.1355 (2.0331)	Prec@1 28.906 (29.576)
Test: [40/79]	Time 0.032 (0.049)	Loss 2.1689 (2.0706)	Prec@1 29.688 (28.811)
Test: [60/79]	Time 0.031 (0.044)	Loss 2.1182 (2.0849)	Prec@1 25.781 (27.869)
 * Prec@1 28.090
=> Saving checkpoint for epoch 16, with Prec@1 28.090000.
Epoch: [17][0/79]	Time 0.790 (0.790)	Loss 1.5363 (1.5363)	Prec@1 46.094 (46.094)
Epoch: [17][20/79]	Time 0.042 (0.079)	Loss 1.5496 (1.5466)	Prec@1 40.625 (42.671)
Epoch: [17][40/79]	Time 0.045 (0.062)	Loss 1.6171 (1.5037)	Prec@1 42.188 (44.284)
Epoch: [17][60/79]	Time 0.047 (0.057)	Loss 1.4688 (1.4995)	Prec@1 52.344 (44.301)
training time:  4.442709684371948
Test: [0/79]	Time 0.770 (0.770)	Loss 1.6865 (1.6865)	Prec@1 39.062 (39.062)
Test: [20/79]	Time 0.029 (0.067)	Loss 1.8327 (1.7714)	Prec@1 33.594 (34.226)
Test: [40/79]	Time 0.032 (0.050)	Loss 1.7472 (1.7701)	Prec@1 32.812 (34.432)
Test: [60/79]	Time 0.031 (0.044)	Loss 1.7705 (1.7717)	Prec@1 39.062 (34.695)
 * Prec@1 35.090
=> Saving checkpoint for epoch 17, with Prec@1 35.090000.
Epoch: [18][0/79]	Time 0.791 (0.791)	Loss 1.2930 (1.2930)	Prec@1 53.906 (53.906)
Epoch: [18][20/79]	Time 0.046 (0.081)	Loss 1.4958 (1.4443)	Prec@1 46.094 (46.912)
Epoch: [18][40/79]	Time 0.047 (0.064)	Loss 1.4067 (1.4221)	Prec@1 47.656 (47.828)
Epoch: [18][60/79]	Time 0.046 (0.058)	Loss 1.2176 (1.4192)	Prec@1 53.906 (47.861)
training time:  4.547564268112183
Test: [0/79]	Time 0.761 (0.761)	Loss 1.8638 (1.8638)	Prec@1 37.500 (37.500)
Test: [20/79]	Time 0.028 (0.065)	Loss 1.8470 (1.9222)	Prec@1 35.938 (32.143)
Test: [40/79]	Time 0.029 (0.049)	Loss 1.9430 (1.9259)	Prec@1 34.375 (31.841)
Test: [60/79]	Time 0.031 (0.043)	Loss 1.9488 (1.9324)	Prec@1 32.031 (31.954)
 * Prec@1 32.330
Epoch: [19][0/79]	Time 0.805 (0.805)	Loss 1.5548 (1.5548)	Prec@1 41.406 (41.406)
Epoch: [19][20/79]	Time 0.043 (0.082)	Loss 1.3905 (1.3756)	Prec@1 50.781 (48.921)
Epoch: [19][40/79]	Time 0.045 (0.064)	Loss 1.3654 (1.3764)	Prec@1 50.781 (49.181)
Epoch: [19][60/79]	Time 0.046 (0.058)	Loss 1.3865 (1.3611)	Prec@1 47.656 (49.731)
training time:  4.523955821990967
Test: [0/79]	Time 0.761 (0.761)	Loss 1.8969 (1.8969)	Prec@1 34.375 (34.375)
Test: [20/79]	Time 0.032 (0.067)	Loss 2.0847 (1.9761)	Prec@1 29.688 (33.408)
Test: [40/79]	Time 0.032 (0.050)	Loss 1.9184 (1.9767)	Prec@1 29.688 (33.136)
Test: [60/79]	Time 0.032 (0.044)	Loss 1.8054 (1.9690)	Prec@1 39.844 (33.325)
 * Prec@1 33.560
Epoch: [20][0/79]	Time 0.807 (0.807)	Loss 1.3985 (1.3985)	Prec@1 45.312 (45.312)
Epoch: [20][20/79]	Time 0.046 (0.081)	Loss 1.4484 (1.3619)	Prec@1 54.688 (51.079)
Epoch: [20][40/79]	Time 0.045 (0.064)	Loss 1.2904 (1.3301)	Prec@1 50.781 (51.772)
Epoch: [20][60/79]	Time 0.046 (0.058)	Loss 1.2821 (1.3159)	Prec@1 52.344 (51.819)
training time:  4.51588773727417
Test: [0/79]	Time 0.746 (0.746)	Loss 1.5949 (1.5949)	Prec@1 42.969 (42.969)
Test: [20/79]	Time 0.031 (0.064)	Loss 1.7620 (1.7082)	Prec@1 35.938 (38.876)
Test: [40/79]	Time 0.032 (0.049)	Loss 1.6731 (1.7041)	Prec@1 43.750 (38.853)
Test: [60/79]	Time 0.031 (0.043)	Loss 1.6409 (1.7171)	Prec@1 46.875 (38.448)
 * Prec@1 38.840
=> Saving checkpoint for epoch 20, with Prec@1 38.840000.
Epoch: [21][0/79]	Time 0.794 (0.794)	Loss 1.3968 (1.3968)	Prec@1 53.125 (53.125)
Epoch: [21][20/79]	Time 0.048 (0.081)	Loss 1.3184 (1.3215)	Prec@1 51.562 (52.307)
Epoch: [21][40/79]	Time 0.044 (0.063)	Loss 1.2330 (1.2850)	Prec@1 53.906 (53.849)
Epoch: [21][60/79]	Time 0.044 (0.057)	Loss 1.0953 (1.2638)	Prec@1 55.469 (54.393)
training time:  4.4197938442230225
Test: [0/79]	Time 0.760 (0.760)	Loss 1.8556 (1.8556)	Prec@1 37.500 (37.500)
Test: [20/79]	Time 0.032 (0.065)	Loss 1.9856 (2.0733)	Prec@1 34.375 (31.399)
Test: [40/79]	Time 0.032 (0.052)	Loss 2.0517 (2.0911)	Prec@1 31.250 (31.193)
Test: [60/79]	Time 0.029 (0.045)	Loss 2.0624 (2.1077)	Prec@1 30.469 (30.520)
 * Prec@1 30.450
Epoch: [22][0/79]	Time 0.789 (0.789)	Loss 1.3124 (1.3124)	Prec@1 50.781 (50.781)
Epoch: [22][20/79]	Time 0.045 (0.082)	Loss 1.1183 (1.2716)	Prec@1 58.594 (54.874)
Epoch: [22][40/79]	Time 0.047 (0.065)	Loss 1.2353 (1.2352)	Prec@1 53.906 (56.002)
Epoch: [22][60/79]	Time 0.046 (0.058)	Loss 1.2497 (1.2214)	Prec@1 57.812 (56.519)
training time:  4.539221286773682
Test: [0/79]	Time 0.759 (0.759)	Loss 2.5329 (2.5329)	Prec@1 33.594 (33.594)
Test: [20/79]	Time 0.029 (0.064)	Loss 2.3884 (2.4745)	Prec@1 34.375 (31.213)
Test: [40/79]	Time 0.029 (0.047)	Loss 2.6240 (2.5203)	Prec@1 32.812 (30.926)
Test: [60/79]	Time 0.032 (0.042)	Loss 2.3213 (2.5295)	Prec@1 32.031 (30.789)
 * Prec@1 31.210
Epoch: [23][0/79]	Time 0.807 (0.807)	Loss 1.4974 (1.4974)	Prec@1 42.969 (42.969)
Epoch: [23][20/79]	Time 0.047 (0.082)	Loss 1.2400 (1.2707)	Prec@1 55.469 (54.539)
Epoch: [23][40/79]	Time 0.048 (0.065)	Loss 1.1392 (1.2111)	Prec@1 57.031 (57.031)
Epoch: [23][60/79]	Time 0.045 (0.059)	Loss 1.2113 (1.1833)	Prec@1 57.031 (57.902)
training time:  4.560464143753052
Test: [0/79]	Time 0.772 (0.772)	Loss 2.2327 (2.2327)	Prec@1 32.031 (32.031)
Test: [20/79]	Time 0.031 (0.067)	Loss 1.9359 (2.1211)	Prec@1 32.031 (31.176)
Test: [40/79]	Time 0.032 (0.050)	Loss 2.0318 (2.1066)	Prec@1 32.031 (32.069)
Test: [60/79]	Time 0.033 (0.044)	Loss 2.0840 (2.1097)	Prec@1 31.250 (31.801)
 * Prec@1 32.080
Epoch: [24][0/79]	Time 0.797 (0.797)	Loss 1.1171 (1.1171)	Prec@1 58.594 (58.594)
Epoch: [24][20/79]	Time 0.047 (0.082)	Loss 1.2361 (1.1663)	Prec@1 55.469 (59.598)
Epoch: [24][40/79]	Time 0.047 (0.065)	Loss 1.0767 (1.1446)	Prec@1 61.719 (59.737)
Epoch: [24][60/79]	Time 0.045 (0.059)	Loss 1.1778 (1.1197)	Prec@1 59.375 (60.515)
training time:  4.54100227355957
Test: [0/79]	Time 0.765 (0.765)	Loss 1.7221 (1.7221)	Prec@1 39.062 (39.062)
Test: [20/79]	Time 0.032 (0.066)	Loss 1.6184 (1.7275)	Prec@1 39.844 (40.960)
Test: [40/79]	Time 0.033 (0.049)	Loss 1.7095 (1.7082)	Prec@1 40.625 (41.482)
Test: [60/79]	Time 0.032 (0.044)	Loss 1.6753 (1.7074)	Prec@1 39.844 (41.317)
 * Prec@1 41.640
=> Saving checkpoint for epoch 24, with Prec@1 41.640000.
Epoch: [25][0/79]	Time 0.795 (0.795)	Loss 0.9139 (0.9139)	Prec@1 69.531 (69.531)
Epoch: [25][20/79]	Time 0.043 (0.080)	Loss 1.0138 (1.0332)	Prec@1 62.500 (62.351)
Epoch: [25][40/79]	Time 0.045 (0.064)	Loss 1.0353 (1.0523)	Prec@1 60.938 (62.290)
Epoch: [25][60/79]	Time 0.047 (0.058)	Loss 1.0726 (1.0493)	Prec@1 62.500 (62.167)
training time:  4.496784687042236
Test: [0/79]	Time 0.792 (0.792)	Loss 1.6169 (1.6169)	Prec@1 42.188 (42.188)
Test: [20/79]	Time 0.031 (0.066)	Loss 1.6065 (1.5778)	Prec@1 42.969 (43.973)
Test: [40/79]	Time 0.032 (0.049)	Loss 1.5247 (1.5699)	Prec@1 44.531 (44.474)
Test: [60/79]	Time 0.031 (0.043)	Loss 1.6884 (1.5928)	Prec@1 39.844 (43.648)
 * Prec@1 43.710
=> Saving checkpoint for epoch 25, with Prec@1 43.710000.
Epoch: [26][0/79]	Time 0.798 (0.798)	Loss 1.1332 (1.1332)	Prec@1 60.938 (60.938)
Epoch: [26][20/79]	Time 0.043 (0.080)	Loss 0.9982 (1.0003)	Prec@1 65.625 (65.253)
Epoch: [26][40/79]	Time 0.043 (0.063)	Loss 1.0751 (0.9955)	Prec@1 57.812 (64.710)
Epoch: [26][60/79]	Time 0.044 (0.057)	Loss 0.9504 (0.9975)	Prec@1 63.281 (64.434)
training time:  4.424794435501099
Test: [0/79]	Time 0.799 (0.799)	Loss 2.0315 (2.0315)	Prec@1 40.625 (40.625)
Test: [20/79]	Time 0.033 (0.069)	Loss 2.0211 (2.0597)	Prec@1 37.500 (38.281)
Test: [40/79]	Time 0.032 (0.051)	Loss 2.0643 (2.0948)	Prec@1 41.406 (38.681)
Test: [60/79]	Time 0.030 (0.045)	Loss 2.3552 (2.1319)	Prec@1 38.281 (38.422)
 * Prec@1 38.480
Epoch: [27][0/79]	Time 0.836 (0.836)	Loss 1.1926 (1.1926)	Prec@1 63.281 (63.281)
Epoch: [27][20/79]	Time 0.048 (0.084)	Loss 1.0543 (1.0716)	Prec@1 60.938 (61.235)
Epoch: [27][40/79]	Time 0.050 (0.069)	Loss 0.9703 (1.0200)	Prec@1 62.500 (63.262)
Epoch: [27][60/79]	Time 0.044 (0.062)	Loss 0.9332 (0.9879)	Prec@1 67.188 (64.626)
training time:  4.748998403549194
Test: [0/79]	Time 0.796 (0.796)	Loss 1.7012 (1.7012)	Prec@1 50.000 (50.000)
Test: [20/79]	Time 0.032 (0.068)	Loss 1.7551 (1.7930)	Prec@1 39.844 (43.192)
Test: [40/79]	Time 0.033 (0.051)	Loss 1.8114 (1.7922)	Prec@1 42.188 (43.502)
Test: [60/79]	Time 0.032 (0.045)	Loss 1.5262 (1.7993)	Prec@1 50.000 (42.982)
 * Prec@1 43.090
Epoch: [28][0/79]	Time 0.839 (0.839)	Loss 0.8354 (0.8354)	Prec@1 75.000 (75.000)
Epoch: [28][20/79]	Time 0.045 (0.084)	Loss 0.9517 (1.0315)	Prec@1 63.281 (63.356)
Epoch: [28][40/79]	Time 0.043 (0.066)	Loss 0.8726 (0.9871)	Prec@1 66.406 (64.882)
Epoch: [28][60/79]	Time 0.048 (0.059)	Loss 0.9057 (0.9658)	Prec@1 70.312 (65.804)
training time:  4.603262424468994
Test: [0/79]	Time 0.785 (0.785)	Loss 2.1903 (2.1903)	Prec@1 33.594 (33.594)
Test: [20/79]	Time 0.031 (0.068)	Loss 2.3741 (2.4835)	Prec@1 27.344 (29.390)
Test: [40/79]	Time 0.032 (0.050)	Loss 2.4482 (2.4857)	Prec@1 26.562 (29.554)
Test: [60/79]	Time 0.031 (0.044)	Loss 2.6885 (2.5062)	Prec@1 25.781 (29.431)
 * Prec@1 29.690
Epoch: [29][0/79]	Time 0.838 (0.838)	Loss 1.0783 (1.0783)	Prec@1 65.625 (65.625)
Epoch: [29][20/79]	Time 0.047 (0.084)	Loss 0.9192 (1.0694)	Prec@1 64.844 (63.207)
Epoch: [29][40/79]	Time 0.045 (0.066)	Loss 0.7306 (0.9868)	Prec@1 78.125 (65.777)
Epoch: [29][60/79]	Time 0.045 (0.059)	Loss 0.9279 (0.9543)	Prec@1 64.062 (66.483)
training time:  4.616944789886475
Test: [0/79]	Time 0.787 (0.787)	Loss 1.7299 (1.7299)	Prec@1 42.188 (42.188)
Test: [20/79]	Time 0.032 (0.068)	Loss 1.6923 (1.7936)	Prec@1 43.750 (41.853)
Test: [40/79]	Time 0.034 (0.050)	Loss 1.9603 (1.8168)	Prec@1 33.594 (41.749)
Test: [60/79]	Time 0.030 (0.044)	Loss 1.9791 (1.8409)	Prec@1 42.969 (41.726)
 * Prec@1 41.590
Epoch: [30][0/79]	Time 0.839 (0.839)	Loss 1.0776 (1.0776)	Prec@1 64.062 (64.062)
Epoch: [30][20/79]	Time 0.047 (0.084)	Loss 0.9198 (0.9475)	Prec@1 68.750 (65.885)
Epoch: [30][40/79]	Time 0.048 (0.066)	Loss 0.9820 (0.8892)	Prec@1 64.844 (68.693)
Epoch: [30][60/79]	Time 0.044 (0.059)	Loss 0.8269 (0.8867)	Prec@1 74.219 (69.096)
training time:  4.588017463684082
Test: [0/79]	Time 0.807 (0.807)	Loss 2.5158 (2.5158)	Prec@1 33.594 (33.594)
Test: [20/79]	Time 0.032 (0.068)	Loss 2.2464 (2.4325)	Prec@1 38.281 (34.747)
Test: [40/79]	Time 0.032 (0.050)	Loss 2.4311 (2.4044)	Prec@1 40.625 (36.204)
Test: [60/79]	Time 0.032 (0.045)	Loss 2.5912 (2.4113)	Prec@1 32.031 (35.873)
 * Prec@1 36.150
Epoch: [31][0/79]	Time 0.843 (0.843)	Loss 1.1521 (1.1521)	Prec@1 63.281 (63.281)
Epoch: [31][20/79]	Time 0.046 (0.085)	Loss 0.7552 (0.8773)	Prec@1 74.219 (69.866)
Epoch: [31][40/79]	Time 0.047 (0.066)	Loss 0.8086 (0.8688)	Prec@1 75.000 (69.417)
Epoch: [31][60/79]	Time 0.045 (0.060)	Loss 0.7258 (0.8498)	Prec@1 75.781 (70.120)
training time:  4.613709449768066
Test: [0/79]	Time 0.799 (0.799)	Loss 1.9524 (1.9524)	Prec@1 39.062 (39.062)
Test: [20/79]	Time 0.029 (0.066)	Loss 1.8207 (1.9772)	Prec@1 41.406 (39.918)
Test: [40/79]	Time 0.033 (0.049)	Loss 1.8463 (1.9611)	Prec@1 42.188 (40.549)
Test: [60/79]	Time 0.031 (0.043)	Loss 2.1657 (1.9641)	Prec@1 40.625 (40.689)
 * Prec@1 40.790
Epoch: [32][0/79]	Time 0.842 (0.842)	Loss 0.8327 (0.8327)	Prec@1 74.219 (74.219)
Epoch: [32][20/79]	Time 0.045 (0.084)	Loss 0.6892 (0.8069)	Prec@1 77.344 (71.801)
Epoch: [32][40/79]	Time 0.046 (0.066)	Loss 0.7467 (0.7893)	Prec@1 71.094 (71.913)
Epoch: [32][60/79]	Time 0.046 (0.059)	Loss 0.7878 (0.7828)	Prec@1 67.969 (72.182)
training time:  4.5893638134002686
Test: [0/79]	Time 0.792 (0.792)	Loss 1.7076 (1.7076)	Prec@1 50.000 (50.000)
Test: [20/79]	Time 0.033 (0.068)	Loss 1.5671 (1.9199)	Prec@1 50.781 (44.122)
Test: [40/79]	Time 0.032 (0.054)	Loss 2.0012 (1.8978)	Prec@1 41.406 (44.722)
Test: [60/79]	Time 0.031 (0.047)	Loss 2.3227 (1.9133)	Prec@1 37.500 (44.096)
 * Prec@1 44.060
=> Saving checkpoint for epoch 32, with Prec@1 44.060000.
Epoch: [33][0/79]	Time 0.826 (0.826)	Loss 0.8251 (0.8251)	Prec@1 71.094 (71.094)
Epoch: [33][20/79]	Time 0.044 (0.082)	Loss 0.9137 (0.8194)	Prec@1 67.969 (71.391)
Epoch: [33][40/79]	Time 0.046 (0.064)	Loss 0.9027 (0.7819)	Prec@1 67.188 (72.389)
Epoch: [33][60/79]	Time 0.047 (0.058)	Loss 0.6128 (0.7831)	Prec@1 77.344 (72.221)
training time:  4.505471229553223
Test: [0/79]	Time 0.792 (0.792)	Loss 1.7505 (1.7505)	Prec@1 50.000 (50.000)
Test: [20/79]	Time 0.033 (0.068)	Loss 1.4139 (1.7599)	Prec@1 48.438 (45.685)
Test: [40/79]	Time 0.032 (0.050)	Loss 1.8644 (1.7405)	Prec@1 42.969 (46.380)
Test: [60/79]	Time 0.031 (0.044)	Loss 1.8267 (1.7630)	Prec@1 44.531 (45.953)
 * Prec@1 46.260
=> Saving checkpoint for epoch 33, with Prec@1 46.260000.
Epoch: [34][0/79]	Time 0.833 (0.833)	Loss 0.9610 (0.9610)	Prec@1 63.281 (63.281)
Epoch: [34][20/79]	Time 0.046 (0.083)	Loss 0.7595 (0.7804)	Prec@1 71.094 (71.354)
Epoch: [34][40/79]	Time 0.046 (0.065)	Loss 0.7437 (0.7668)	Prec@1 72.656 (72.389)
Epoch: [34][60/79]	Time 0.045 (0.059)	Loss 0.7855 (0.7529)	Prec@1 71.094 (73.002)
training time:  4.582428455352783
Test: [0/79]	Time 0.804 (0.804)	Loss 1.8775 (1.8775)	Prec@1 42.969 (42.969)
Test: [20/79]	Time 0.031 (0.068)	Loss 1.8350 (1.9056)	Prec@1 42.188 (43.341)
Test: [40/79]	Time 0.031 (0.051)	Loss 1.9687 (1.9018)	Prec@1 41.406 (42.873)
Test: [60/79]	Time 0.031 (0.045)	Loss 1.9199 (1.9166)	Prec@1 40.625 (42.431)
 * Prec@1 42.540
Epoch: [35][0/79]	Time 0.828 (0.828)	Loss 1.0376 (1.0376)	Prec@1 64.844 (64.844)
Epoch: [35][20/79]	Time 0.048 (0.082)	Loss 0.5382 (0.7029)	Prec@1 83.594 (76.079)
Epoch: [35][40/79]	Time 0.048 (0.065)	Loss 0.6456 (0.6957)	Prec@1 78.906 (75.915)
Epoch: [35][60/79]	Time 0.047 (0.059)	Loss 0.6598 (0.6898)	Prec@1 77.344 (75.730)
training time:  4.5720460414886475
Test: [0/79]	Time 0.801 (0.801)	Loss 1.6294 (1.6294)	Prec@1 56.250 (56.250)
Test: [20/79]	Time 0.032 (0.069)	Loss 1.7102 (1.7653)	Prec@1 47.656 (48.810)
Test: [40/79]	Time 0.032 (0.051)	Loss 1.7446 (1.7577)	Prec@1 50.781 (49.219)
Test: [60/79]	Time 0.032 (0.045)	Loss 1.6146 (1.7467)	Prec@1 47.656 (49.078)
 * Prec@1 49.130
=> Saving checkpoint for epoch 35, with Prec@1 49.130000.
Epoch: [36][0/79]	Time 0.785 (0.785)	Loss 0.8958 (0.8958)	Prec@1 69.531 (69.531)
Epoch: [36][20/79]	Time 0.045 (0.082)	Loss 0.6324 (0.6971)	Prec@1 76.562 (75.632)
Epoch: [36][40/79]	Time 0.046 (0.064)	Loss 0.6696 (0.6934)	Prec@1 74.219 (75.648)
Epoch: [36][60/79]	Time 0.047 (0.058)	Loss 0.7177 (0.6851)	Prec@1 74.219 (76.012)
training time:  4.544174671173096
Test: [0/79]	Time 0.756 (0.756)	Loss 1.8787 (1.8787)	Prec@1 41.406 (41.406)
Test: [20/79]	Time 0.032 (0.067)	Loss 1.6221 (1.9673)	Prec@1 46.875 (41.629)
Test: [40/79]	Time 0.032 (0.050)	Loss 2.0889 (1.9573)	Prec@1 41.406 (42.454)
Test: [60/79]	Time 0.031 (0.044)	Loss 2.0803 (1.9727)	Prec@1 39.062 (42.456)
 * Prec@1 43.020
Epoch: [37][0/79]	Time 0.795 (0.795)	Loss 0.8751 (0.8751)	Prec@1 75.000 (75.000)
Epoch: [37][20/79]	Time 0.047 (0.082)	Loss 0.6938 (0.7144)	Prec@1 75.000 (74.740)
Epoch: [37][40/79]	Time 0.046 (0.064)	Loss 0.6467 (0.6796)	Prec@1 78.125 (76.162)
Epoch: [37][60/79]	Time 0.046 (0.058)	Loss 0.7504 (0.6704)	Prec@1 75.000 (76.281)
training time:  4.581251382827759
Test: [0/79]	Time 0.757 (0.757)	Loss 1.4070 (1.4070)	Prec@1 54.688 (54.688)
Test: [20/79]	Time 0.034 (0.066)	Loss 1.2842 (1.3735)	Prec@1 59.375 (55.283)
Test: [40/79]	Time 0.032 (0.050)	Loss 1.4892 (1.3401)	Prec@1 49.219 (56.364)
Test: [60/79]	Time 0.031 (0.044)	Loss 1.2130 (1.3630)	Prec@1 56.250 (55.584)
 * Prec@1 55.710
=> Saving checkpoint for epoch 37, with Prec@1 55.710000.
Epoch: [38][0/79]	Time 0.832 (0.832)	Loss 0.6001 (0.6001)	Prec@1 81.250 (81.250)
Epoch: [38][20/79]	Time 0.046 (0.084)	Loss 0.6119 (0.6304)	Prec@1 76.562 (77.641)
Epoch: [38][40/79]	Time 0.050 (0.069)	Loss 0.6200 (0.6140)	Prec@1 73.438 (78.182)
Epoch: [38][60/79]	Time 0.047 (0.062)	Loss 0.5505 (0.6276)	Prec@1 85.156 (77.946)
training time:  4.7569451332092285
Test: [0/79]	Time 0.790 (0.790)	Loss 1.2589 (1.2589)	Prec@1 60.938 (60.938)
Test: [20/79]	Time 0.032 (0.068)	Loss 1.1979 (1.2718)	Prec@1 57.031 (58.408)
Test: [40/79]	Time 0.031 (0.050)	Loss 1.4055 (1.2634)	Prec@1 45.312 (58.518)
Test: [60/79]	Time 0.031 (0.045)	Loss 1.2604 (1.2856)	Prec@1 56.250 (57.659)
 * Prec@1 57.500
=> Saving checkpoint for epoch 38, with Prec@1 57.500000.
Epoch: [39][0/79]	Time 0.846 (0.846)	Loss 0.5882 (0.5882)	Prec@1 78.906 (78.906)
Epoch: [39][20/79]	Time 0.046 (0.084)	Loss 0.5380 (0.6382)	Prec@1 83.594 (77.902)
Epoch: [39][40/79]	Time 0.045 (0.065)	Loss 0.5326 (0.6131)	Prec@1 80.469 (78.620)
Epoch: [39][60/79]	Time 0.046 (0.059)	Loss 0.5350 (0.6082)	Prec@1 78.906 (78.701)
training time:  4.582157135009766
Test: [0/79]	Time 0.755 (0.755)	Loss 1.6405 (1.6405)	Prec@1 52.344 (52.344)
Test: [20/79]	Time 0.032 (0.066)	Loss 1.2940 (1.5196)	Prec@1 57.031 (55.655)
Test: [40/79]	Time 0.032 (0.049)	Loss 1.6614 (1.5239)	Prec@1 55.469 (55.850)
Test: [60/79]	Time 0.030 (0.044)	Loss 1.7525 (1.5500)	Prec@1 53.906 (55.085)
 * Prec@1 54.860
Epoch: [40][0/79]	Time 0.799 (0.799)	Loss 0.4971 (0.4971)	Prec@1 84.375 (84.375)
Epoch: [40][20/79]	Time 0.044 (0.081)	Loss 0.6112 (0.5712)	Prec@1 80.469 (80.729)
Epoch: [40][40/79]	Time 0.046 (0.064)	Loss 0.5447 (0.5535)	Prec@1 80.469 (80.888)
Epoch: [40][60/79]	Time 0.047 (0.058)	Loss 0.5766 (0.5459)	Prec@1 79.688 (81.084)
training time:  4.525323867797852
Test: [0/79]	Time 0.792 (0.792)	Loss 1.5213 (1.5213)	Prec@1 58.594 (58.594)
Test: [20/79]	Time 0.032 (0.068)	Loss 1.2790 (1.4092)	Prec@1 58.594 (56.957)
Test: [40/79]	Time 0.032 (0.051)	Loss 1.5478 (1.4029)	Prec@1 53.125 (56.784)
Test: [60/79]	Time 0.031 (0.045)	Loss 1.6001 (1.4266)	Prec@1 52.344 (56.314)
 * Prec@1 56.610
Epoch: [41][0/79]	Time 0.842 (0.842)	Loss 0.5325 (0.5325)	Prec@1 81.250 (81.250)
Epoch: [41][20/79]	Time 0.046 (0.084)	Loss 0.3783 (0.5568)	Prec@1 83.594 (80.283)
Epoch: [41][40/79]	Time 0.046 (0.065)	Loss 0.5439 (0.5479)	Prec@1 78.125 (80.697)
Epoch: [41][60/79]	Time 0.044 (0.059)	Loss 0.7069 (0.5435)	Prec@1 75.000 (80.686)
training time:  4.574951648712158
Test: [0/79]	Time 0.812 (0.812)	Loss 2.4808 (2.4808)	Prec@1 42.188 (42.188)
Test: [20/79]	Time 0.031 (0.069)	Loss 2.2582 (2.3565)	Prec@1 50.000 (43.415)
Test: [40/79]	Time 0.033 (0.051)	Loss 2.3910 (2.3747)	Prec@1 49.219 (43.731)
Test: [60/79]	Time 0.030 (0.045)	Loss 2.3085 (2.3811)	Prec@1 36.719 (43.263)
 * Prec@1 43.420
Epoch: [42][0/79]	Time 0.832 (0.832)	Loss 0.5002 (0.5002)	Prec@1 82.031 (82.031)
Epoch: [42][20/79]	Time 0.045 (0.084)	Loss 0.6271 (0.6401)	Prec@1 77.344 (77.307)
Epoch: [42][40/79]	Time 0.047 (0.066)	Loss 0.4129 (0.5943)	Prec@1 85.938 (79.078)
Epoch: [42][60/79]	Time 0.045 (0.059)	Loss 0.5864 (0.5679)	Prec@1 78.906 (80.020)
training time:  4.606038331985474
Test: [0/79]	Time 0.806 (0.806)	Loss 1.3976 (1.3976)	Prec@1 53.125 (53.125)
Test: [20/79]	Time 0.032 (0.068)	Loss 1.2396 (1.3666)	Prec@1 60.156 (56.287)
Test: [40/79]	Time 0.032 (0.050)	Loss 1.4095 (1.3476)	Prec@1 51.562 (57.470)
Test: [60/79]	Time 0.031 (0.044)	Loss 1.6709 (1.3524)	Prec@1 50.781 (57.351)
 * Prec@1 57.260
Epoch: [43][0/79]	Time 0.839 (0.839)	Loss 0.3914 (0.3914)	Prec@1 87.500 (87.500)
Epoch: [43][20/79]	Time 0.047 (0.084)	Loss 0.5054 (0.4939)	Prec@1 82.031 (82.478)
Epoch: [43][40/79]	Time 0.043 (0.066)	Loss 0.6375 (0.4697)	Prec@1 77.344 (83.289)
Epoch: [43][60/79]	Time 0.045 (0.059)	Loss 0.3705 (0.4834)	Prec@1 89.062 (82.915)
training time:  4.556787014007568
Test: [0/79]	Time 0.797 (0.797)	Loss 1.5296 (1.5296)	Prec@1 54.688 (54.688)
Test: [20/79]	Time 0.159 (0.073)	Loss 1.3888 (1.4727)	Prec@1 53.906 (56.176)
Test: [40/79]	Time 0.032 (0.053)	Loss 1.7924 (1.4839)	Prec@1 49.219 (55.640)
Test: [60/79]	Time 0.032 (0.046)	Loss 1.4023 (1.4907)	Prec@1 53.906 (54.854)
 * Prec@1 55.300
Epoch: [44][0/79]	Time 0.834 (0.834)	Loss 0.4934 (0.4934)	Prec@1 81.250 (81.250)
Epoch: [44][20/79]	Time 0.046 (0.082)	Loss 0.3665 (0.5089)	Prec@1 89.062 (82.143)
Epoch: [44][40/79]	Time 0.046 (0.064)	Loss 0.3728 (0.5009)	Prec@1 88.281 (82.584)
Epoch: [44][60/79]	Time 0.045 (0.058)	Loss 0.5537 (0.4948)	Prec@1 79.688 (82.825)
training time:  4.514210224151611
Test: [0/79]	Time 0.810 (0.810)	Loss 1.8474 (1.8474)	Prec@1 56.250 (56.250)
Test: [20/79]	Time 0.032 (0.069)	Loss 1.6922 (1.6989)	Prec@1 54.688 (54.018)
Test: [40/79]	Time 0.032 (0.051)	Loss 1.7344 (1.6905)	Prec@1 50.781 (53.944)
Test: [60/79]	Time 0.031 (0.045)	Loss 1.5367 (1.7021)	Prec@1 54.688 (53.432)
 * Prec@1 53.500
Epoch: [45][0/79]	Time 0.841 (0.841)	Loss 0.4490 (0.4490)	Prec@1 85.938 (85.938)
Epoch: [45][20/79]	Time 0.046 (0.084)	Loss 0.5408 (0.5572)	Prec@1 82.812 (80.394)
Epoch: [45][40/79]	Time 0.047 (0.066)	Loss 0.4845 (0.5195)	Prec@1 82.812 (81.784)
Epoch: [45][60/79]	Time 0.042 (0.059)	Loss 0.4226 (0.5024)	Prec@1 83.594 (82.415)
training time:  4.5672619342803955
Test: [0/79]	Time 0.798 (0.798)	Loss 1.8341 (1.8341)	Prec@1 47.656 (47.656)
Test: [20/79]	Time 0.032 (0.068)	Loss 1.7573 (1.6347)	Prec@1 52.344 (54.688)
Test: [40/79]	Time 0.031 (0.051)	Loss 1.7142 (1.6777)	Prec@1 53.906 (54.478)
Test: [60/79]	Time 0.030 (0.045)	Loss 1.5390 (1.6764)	Prec@1 56.250 (54.188)
 * Prec@1 54.050
Epoch: [46][0/79]	Time 0.849 (0.849)	Loss 0.4713 (0.4713)	Prec@1 83.594 (83.594)
Epoch: [46][20/79]	Time 0.047 (0.085)	Loss 0.3727 (0.4562)	Prec@1 85.156 (84.115)
Epoch: [46][40/79]	Time 0.046 (0.066)	Loss 0.3792 (0.4361)	Prec@1 91.406 (85.023)
Epoch: [46][60/79]	Time 0.046 (0.059)	Loss 0.5044 (0.4373)	Prec@1 81.250 (85.143)
training time:  4.601907968521118
Test: [0/79]	Time 0.802 (0.802)	Loss 2.0640 (2.0640)	Prec@1 47.656 (47.656)
Test: [20/79]	Time 0.032 (0.067)	Loss 1.8857 (1.9902)	Prec@1 52.344 (49.107)
Test: [40/79]	Time 0.033 (0.050)	Loss 2.0510 (2.0008)	Prec@1 45.312 (47.980)
Test: [60/79]	Time 0.032 (0.044)	Loss 1.9703 (2.0148)	Prec@1 46.875 (47.272)
 * Prec@1 47.500
Epoch: [47][0/79]	Time 0.801 (0.801)	Loss 0.7393 (0.7393)	Prec@1 76.562 (76.562)
Epoch: [47][20/79]	Time 0.047 (0.082)	Loss 0.4963 (0.5286)	Prec@1 82.031 (81.882)
Epoch: [47][40/79]	Time 0.043 (0.064)	Loss 0.3209 (0.4914)	Prec@1 89.062 (82.698)
Epoch: [47][60/79]	Time 0.046 (0.058)	Loss 0.3896 (0.4705)	Prec@1 86.719 (83.414)
training time:  4.51027250289917
Test: [0/79]	Time 0.765 (0.765)	Loss 1.7846 (1.7846)	Prec@1 54.688 (54.688)
Test: [20/79]	Time 0.031 (0.067)	Loss 1.4995 (1.6737)	Prec@1 60.156 (55.692)
Test: [40/79]	Time 0.031 (0.050)	Loss 2.0493 (1.7042)	Prec@1 53.906 (55.774)
Test: [60/79]	Time 0.029 (0.044)	Loss 1.3657 (1.7260)	Prec@1 61.719 (55.251)
 * Prec@1 55.530
Epoch: [48][0/79]	Time 0.848 (0.848)	Loss 0.5074 (0.5074)	Prec@1 82.031 (82.031)
Epoch: [48][20/79]	Time 0.046 (0.085)	Loss 0.4285 (0.4725)	Prec@1 85.938 (83.445)
Epoch: [48][40/79]	Time 0.047 (0.066)	Loss 0.2933 (0.4463)	Prec@1 88.281 (84.223)
Epoch: [48][60/79]	Time 0.045 (0.059)	Loss 0.3769 (0.4365)	Prec@1 89.062 (84.273)
training time:  4.608285188674927
Test: [0/79]	Time 0.811 (0.811)	Loss 1.4033 (1.4033)	Prec@1 61.719 (61.719)
Test: [20/79]	Time 0.031 (0.069)	Loss 1.0584 (1.3664)	Prec@1 64.062 (59.598)
Test: [40/79]	Time 0.032 (0.051)	Loss 1.5842 (1.3822)	Prec@1 53.906 (59.566)
Test: [60/79]	Time 0.030 (0.045)	Loss 1.5437 (1.3973)	Prec@1 57.031 (58.735)
 * Prec@1 58.750
=> Saving checkpoint for epoch 48, with Prec@1 58.750000.
Epoch: [49][0/79]	Time 0.796 (0.796)	Loss 0.5369 (0.5369)	Prec@1 82.031 (82.031)
Epoch: [49][20/79]	Time 0.046 (0.088)	Loss 0.4212 (0.3755)	Prec@1 87.500 (85.863)
Epoch: [49][40/79]	Time 0.047 (0.068)	Loss 0.3294 (0.3706)	Prec@1 89.062 (86.452)
Epoch: [49][60/79]	Time 0.045 (0.060)	Loss 0.4963 (0.3741)	Prec@1 82.031 (86.578)
training time:  4.6820127964019775
Test: [0/79]	Time 0.767 (0.767)	Loss 1.3737 (1.3737)	Prec@1 57.812 (57.812)
Test: [20/79]	Time 0.029 (0.064)	Loss 1.4348 (1.4938)	Prec@1 60.156 (58.408)
Test: [40/79]	Time 0.031 (0.048)	Loss 1.5806 (1.5153)	Prec@1 53.125 (58.327)
Test: [60/79]	Time 0.030 (0.043)	Loss 1.7040 (1.5298)	Prec@1 60.938 (57.761)
 * Prec@1 57.960
Epoch: [50][0/79]	Time 0.807 (0.807)	Loss 0.2820 (0.2820)	Prec@1 89.844 (89.844)
Epoch: [50][20/79]	Time 0.045 (0.082)	Loss 0.3798 (0.3689)	Prec@1 86.719 (87.946)
Epoch: [50][40/79]	Time 0.046 (0.064)	Loss 0.3410 (0.3608)	Prec@1 87.500 (87.710)
Epoch: [50][60/79]	Time 0.045 (0.059)	Loss 0.2571 (0.3573)	Prec@1 91.406 (87.692)
training time:  4.552433490753174
Test: [0/79]	Time 0.760 (0.760)	Loss 1.4881 (1.4881)	Prec@1 57.031 (57.031)
Test: [20/79]	Time 0.033 (0.067)	Loss 1.4555 (1.7310)	Prec@1 53.125 (54.018)
Test: [40/79]	Time 0.033 (0.050)	Loss 2.1063 (1.7362)	Prec@1 51.562 (54.573)
Test: [60/79]	Time 0.031 (0.044)	Loss 1.8162 (1.7577)	Prec@1 55.469 (54.278)
 * Prec@1 54.510
Epoch: [51][0/79]	Time 0.807 (0.807)	Loss 0.5065 (0.5065)	Prec@1 83.594 (83.594)
Epoch: [51][20/79]	Time 0.046 (0.082)	Loss 0.3122 (0.3723)	Prec@1 89.844 (87.277)
Epoch: [51][40/79]	Time 0.047 (0.065)	Loss 0.2673 (0.3563)	Prec@1 92.969 (87.748)
Epoch: [51][60/79]	Time 0.047 (0.059)	Loss 0.3683 (0.3438)	Prec@1 83.594 (87.935)
training time:  4.55724573135376
Test: [0/79]	Time 0.764 (0.764)	Loss 1.2147 (1.2147)	Prec@1 68.750 (68.750)
Test: [20/79]	Time 0.030 (0.066)	Loss 1.1247 (1.4204)	Prec@1 63.281 (61.719)
Test: [40/79]	Time 0.029 (0.048)	Loss 1.6745 (1.4244)	Prec@1 59.375 (61.643)
Test: [60/79]	Time 0.032 (0.042)	Loss 1.6052 (1.4489)	Prec@1 62.500 (61.399)
 * Prec@1 61.670
=> Saving checkpoint for epoch 51, with Prec@1 61.670000.
Epoch: [52][0/79]	Time 0.792 (0.792)	Loss 0.3882 (0.3882)	Prec@1 86.719 (86.719)
Epoch: [52][20/79]	Time 0.046 (0.082)	Loss 0.4462 (0.3415)	Prec@1 85.938 (88.058)
Epoch: [52][40/79]	Time 0.045 (0.064)	Loss 0.3401 (0.3241)	Prec@1 89.844 (88.815)
Epoch: [52][60/79]	Time 0.045 (0.058)	Loss 0.2990 (0.3173)	Prec@1 89.062 (89.088)
training time:  4.527256965637207
Test: [0/79]	Time 0.793 (0.793)	Loss 1.5258 (1.5258)	Prec@1 57.031 (57.031)
Test: [20/79]	Time 0.031 (0.068)	Loss 1.4690 (1.5430)	Prec@1 57.812 (59.077)
Test: [40/79]	Time 0.032 (0.051)	Loss 1.7888 (1.5535)	Prec@1 57.812 (59.089)
Test: [60/79]	Time 0.031 (0.045)	Loss 1.3789 (1.5636)	Prec@1 61.719 (58.914)
 * Prec@1 59.290
Epoch: [53][0/79]	Time 0.831 (0.831)	Loss 0.2609 (0.2609)	Prec@1 92.188 (92.188)
Epoch: [53][20/79]	Time 0.046 (0.084)	Loss 0.3017 (0.3046)	Prec@1 89.844 (89.695)
Epoch: [53][40/79]	Time 0.046 (0.065)	Loss 0.2254 (0.2925)	Prec@1 90.625 (89.939)
Epoch: [53][60/79]	Time 0.046 (0.059)	Loss 0.5507 (0.2950)	Prec@1 82.031 (89.754)
training time:  4.584259748458862
Test: [0/79]	Time 0.803 (0.803)	Loss 1.2496 (1.2496)	Prec@1 63.281 (63.281)
Test: [20/79]	Time 0.030 (0.068)	Loss 1.1752 (1.4233)	Prec@1 62.500 (60.007)
Test: [40/79]	Time 0.030 (0.050)	Loss 1.6111 (1.4607)	Prec@1 60.156 (60.137)
Test: [60/79]	Time 0.032 (0.044)	Loss 1.5768 (1.4838)	Prec@1 57.031 (59.375)
 * Prec@1 59.780
Epoch: [54][0/79]	Time 0.804 (0.804)	Loss 0.3014 (0.3014)	Prec@1 89.062 (89.062)
Epoch: [54][20/79]	Time 0.046 (0.082)	Loss 0.4012 (0.2698)	Prec@1 85.938 (90.513)
Epoch: [54][40/79]	Time 0.046 (0.065)	Loss 0.3021 (0.2638)	Prec@1 89.844 (90.892)
Epoch: [54][60/79]	Time 0.045 (0.059)	Loss 0.2238 (0.2666)	Prec@1 91.406 (90.753)
training time:  4.548895597457886
Test: [0/79]	Time 0.754 (0.754)	Loss 1.4461 (1.4461)	Prec@1 64.844 (64.844)
Test: [20/79]	Time 0.031 (0.072)	Loss 1.5651 (1.5461)	Prec@1 62.500 (61.272)
Test: [40/79]	Time 0.029 (0.052)	Loss 1.8494 (1.5092)	Prec@1 59.375 (61.585)
Test: [60/79]	Time 0.028 (0.044)	Loss 1.5261 (1.5293)	Prec@1 60.938 (61.078)
 * Prec@1 61.420
Epoch: [55][0/79]	Time 0.789 (0.789)	Loss 0.2427 (0.2427)	Prec@1 90.625 (90.625)
Epoch: [55][20/79]	Time 0.046 (0.081)	Loss 0.2859 (0.2573)	Prec@1 89.844 (91.071)
Epoch: [55][40/79]	Time 0.045 (0.064)	Loss 0.1764 (0.2387)	Prec@1 92.188 (91.559)
Epoch: [55][60/79]	Time 0.048 (0.058)	Loss 0.2159 (0.2398)	Prec@1 88.281 (91.522)
training time:  4.546508312225342
Test: [0/79]	Time 0.792 (0.792)	Loss 1.9009 (1.9009)	Prec@1 53.125 (53.125)
Test: [20/79]	Time 0.032 (0.068)	Loss 1.6587 (1.9499)	Prec@1 60.938 (56.362)
Test: [40/79]	Time 0.032 (0.050)	Loss 2.2985 (1.8975)	Prec@1 53.906 (57.431)
Test: [60/79]	Time 0.032 (0.045)	Loss 1.9064 (1.8916)	Prec@1 52.344 (57.159)
 * Prec@1 57.420
Epoch: [56][0/79]	Time 0.837 (0.837)	Loss 0.3222 (0.3222)	Prec@1 91.406 (91.406)
Epoch: [56][20/79]	Time 0.044 (0.084)	Loss 0.2184 (0.3905)	Prec@1 92.969 (86.384)
Epoch: [56][40/79]	Time 0.047 (0.066)	Loss 0.2696 (0.3507)	Prec@1 90.625 (87.843)
Epoch: [56][60/79]	Time 0.047 (0.059)	Loss 0.2842 (0.3286)	Prec@1 89.062 (88.576)
training time:  4.606844425201416
Test: [0/79]	Time 0.808 (0.808)	Loss 1.5487 (1.5487)	Prec@1 60.156 (60.156)
Test: [20/79]	Time 0.031 (0.068)	Loss 1.5857 (1.5967)	Prec@1 57.812 (57.850)
Test: [40/79]	Time 0.028 (0.050)	Loss 1.9122 (1.6045)	Prec@1 57.031 (58.213)
Test: [60/79]	Time 0.032 (0.044)	Loss 1.9865 (1.6126)	Prec@1 57.031 (57.992)
 * Prec@1 58.480
Epoch: [57][0/79]	Time 0.805 (0.805)	Loss 0.2501 (0.2501)	Prec@1 92.188 (92.188)
Epoch: [57][20/79]	Time 0.048 (0.082)	Loss 0.2319 (0.2643)	Prec@1 92.188 (91.109)
Epoch: [57][40/79]	Time 0.047 (0.065)	Loss 0.2182 (0.2546)	Prec@1 91.406 (91.463)
Epoch: [57][60/79]	Time 0.047 (0.059)	Loss 0.2184 (0.2454)	Prec@1 92.188 (91.726)
training time:  4.559374809265137
Test: [0/79]	Time 0.760 (0.760)	Loss 1.2713 (1.2713)	Prec@1 64.062 (64.062)
Test: [20/79]	Time 0.032 (0.066)	Loss 1.3711 (1.4006)	Prec@1 61.719 (63.430)
Test: [40/79]	Time 0.032 (0.050)	Loss 1.5496 (1.4127)	Prec@1 59.375 (63.739)
Test: [60/79]	Time 0.032 (0.044)	Loss 1.6411 (1.4216)	Prec@1 60.156 (63.397)
 * Prec@1 63.660
=> Saving checkpoint for epoch 57, with Prec@1 63.660000.
Epoch: [58][0/79]	Time 0.792 (0.792)	Loss 0.1413 (0.1413)	Prec@1 96.094 (96.094)
Epoch: [58][20/79]	Time 0.045 (0.081)	Loss 0.1984 (0.2422)	Prec@1 93.750 (91.778)
Epoch: [58][40/79]	Time 0.043 (0.064)	Loss 0.1592 (0.2265)	Prec@1 95.312 (92.454)
Epoch: [58][60/79]	Time 0.047 (0.058)	Loss 0.1872 (0.2178)	Prec@1 92.969 (92.674)
training time:  4.504763603210449
Test: [0/79]	Time 0.756 (0.756)	Loss 1.2332 (1.2332)	Prec@1 69.531 (69.531)
Test: [20/79]	Time 0.032 (0.064)	Loss 1.3994 (1.4500)	Prec@1 62.500 (63.467)
Test: [40/79]	Time 0.031 (0.048)	Loss 1.8454 (1.4760)	Prec@1 60.938 (62.329)
Test: [60/79]	Time 0.031 (0.043)	Loss 1.6812 (1.5106)	Prec@1 55.469 (61.898)
 * Prec@1 62.740
Epoch: [59][0/79]	Time 0.802 (0.802)	Loss 0.1875 (0.1875)	Prec@1 92.188 (92.188)
Epoch: [59][20/79]	Time 0.047 (0.081)	Loss 0.3353 (0.2179)	Prec@1 89.844 (92.522)
Epoch: [59][40/79]	Time 0.046 (0.064)	Loss 0.2298 (0.1980)	Prec@1 92.188 (93.350)
Epoch: [59][60/79]	Time 0.045 (0.058)	Loss 0.1620 (0.1894)	Prec@1 95.312 (93.648)
training time:  4.506922483444214
Test: [0/79]	Time 0.763 (0.763)	Loss 1.4059 (1.4059)	Prec@1 64.062 (64.062)
Test: [20/79]	Time 0.031 (0.067)	Loss 1.3602 (1.5239)	Prec@1 61.719 (60.640)
Test: [40/79]	Time 0.032 (0.050)	Loss 1.8877 (1.5557)	Prec@1 57.812 (60.518)
Test: [60/79]	Time 0.032 (0.044)	Loss 1.7268 (1.5677)	Prec@1 60.156 (60.797)
 * Prec@1 60.930
Epoch: [60][0/79]	Time 0.788 (0.788)	Loss 0.3074 (0.3074)	Prec@1 89.062 (89.062)
Epoch: [60][20/79]	Time 0.046 (0.088)	Loss 0.2598 (0.3065)	Prec@1 89.844 (89.360)
Epoch: [60][40/79]	Time 0.046 (0.068)	Loss 0.2197 (0.2710)	Prec@1 93.750 (90.530)
Epoch: [60][60/79]	Time 0.047 (0.061)	Loss 0.1375 (0.2554)	Prec@1 95.312 (91.112)
training time:  4.661297559738159
Test: [0/79]	Time 0.750 (0.750)	Loss 1.5614 (1.5614)	Prec@1 62.500 (62.500)
Test: [20/79]	Time 0.032 (0.065)	Loss 1.4281 (1.6099)	Prec@1 60.156 (61.012)
Test: [40/79]	Time 0.032 (0.049)	Loss 1.6868 (1.6012)	Prec@1 57.812 (60.575)
Test: [60/79]	Time 0.031 (0.044)	Loss 1.9839 (1.6419)	Prec@1 57.031 (60.079)
 * Prec@1 60.330
Epoch: [61][0/79]	Time 0.841 (0.841)	Loss 0.2228 (0.2228)	Prec@1 89.844 (89.844)
Epoch: [61][20/79]	Time 0.048 (0.086)	Loss 0.1770 (0.3082)	Prec@1 95.312 (88.876)
Epoch: [61][40/79]	Time 0.045 (0.067)	Loss 0.2395 (0.2712)	Prec@1 91.406 (90.225)
Epoch: [61][60/79]	Time 0.047 (0.060)	Loss 0.2191 (0.2470)	Prec@1 94.531 (91.304)
training time:  4.617151737213135
Test: [0/79]	Time 0.822 (0.822)	Loss 1.1742 (1.1742)	Prec@1 69.531 (69.531)
Test: [20/79]	Time 0.029 (0.069)	Loss 1.1259 (1.3604)	Prec@1 65.625 (65.067)
Test: [40/79]	Time 0.032 (0.051)	Loss 1.4770 (1.3369)	Prec@1 60.938 (65.034)
Test: [60/79]	Time 0.031 (0.045)	Loss 1.6569 (1.3617)	Prec@1 55.469 (64.575)
 * Prec@1 64.770
=> Saving checkpoint for epoch 61, with Prec@1 64.770000.
Epoch: [62][0/79]	Time 0.785 (0.785)	Loss 0.2000 (0.2000)	Prec@1 93.750 (93.750)
Epoch: [62][20/79]	Time 0.041 (0.080)	Loss 0.1251 (0.1725)	Prec@1 95.312 (94.271)
Epoch: [62][40/79]	Time 0.043 (0.063)	Loss 0.1796 (0.1592)	Prec@1 94.531 (94.665)
Epoch: [62][60/79]	Time 0.046 (0.058)	Loss 0.1790 (0.1492)	Prec@1 94.531 (95.069)
training time:  4.486541748046875
Test: [0/79]	Time 0.795 (0.795)	Loss 1.3165 (1.3165)	Prec@1 66.406 (66.406)
Test: [20/79]	Time 0.029 (0.068)	Loss 1.3868 (1.3738)	Prec@1 61.719 (63.914)
Test: [40/79]	Time 0.030 (0.050)	Loss 1.3878 (1.3529)	Prec@1 63.281 (65.187)
Test: [60/79]	Time 0.030 (0.044)	Loss 1.3409 (1.3584)	Prec@1 67.188 (64.933)
 * Prec@1 65.480
=> Saving checkpoint for epoch 62, with Prec@1 65.480000.
Epoch: [63][0/79]	Time 0.837 (0.837)	Loss 0.1739 (0.1739)	Prec@1 95.312 (95.312)
Epoch: [63][20/79]	Time 0.043 (0.083)	Loss 0.1037 (0.2530)	Prec@1 96.875 (91.815)
Epoch: [63][40/79]	Time 0.047 (0.065)	Loss 0.1770 (0.2130)	Prec@1 93.750 (92.873)
Epoch: [63][60/79]	Time 0.045 (0.059)	Loss 0.2434 (0.2027)	Prec@1 90.625 (93.148)
training time:  4.551488161087036
Test: [0/79]	Time 0.794 (0.794)	Loss 1.3917 (1.3917)	Prec@1 64.062 (64.062)
Test: [20/79]	Time 0.031 (0.067)	Loss 1.3681 (1.5338)	Prec@1 61.719 (62.016)
Test: [40/79]	Time 0.032 (0.050)	Loss 1.8585 (1.5517)	Prec@1 60.156 (61.471)
Test: [60/79]	Time 0.031 (0.044)	Loss 1.7211 (1.5620)	Prec@1 59.375 (61.245)
 * Prec@1 61.770
Epoch: [64][0/79]	Time 0.811 (0.811)	Loss 0.1052 (0.1052)	Prec@1 93.750 (93.750)
Epoch: [64][20/79]	Time 0.047 (0.082)	Loss 0.1692 (0.2003)	Prec@1 93.750 (92.932)
Epoch: [64][40/79]	Time 0.048 (0.065)	Loss 0.1454 (0.1835)	Prec@1 92.969 (93.483)
Epoch: [64][60/79]	Time 0.044 (0.059)	Loss 0.0824 (0.1693)	Prec@1 97.656 (94.121)
training time:  4.540440320968628
Test: [0/79]	Time 0.756 (0.756)	Loss 1.2660 (1.2660)	Prec@1 67.969 (67.969)
Test: [20/79]	Time 0.032 (0.066)	Loss 1.2746 (1.3417)	Prec@1 67.188 (65.625)
Test: [40/79]	Time 0.032 (0.049)	Loss 1.5599 (1.3538)	Prec@1 64.844 (65.511)
Test: [60/79]	Time 0.030 (0.044)	Loss 1.5005 (1.3785)	Prec@1 57.031 (64.664)
 * Prec@1 64.790
Epoch: [65][0/79]	Time 0.787 (0.787)	Loss 0.1208 (0.1208)	Prec@1 95.312 (95.312)
Epoch: [65][20/79]	Time 0.045 (0.082)	Loss 0.1313 (0.1645)	Prec@1 95.312 (94.420)
Epoch: [65][40/79]	Time 0.047 (0.065)	Loss 0.1328 (0.1376)	Prec@1 96.094 (95.579)
Epoch: [65][60/79]	Time 0.045 (0.058)	Loss 0.1131 (0.1290)	Prec@1 96.875 (95.761)
training time:  4.537078857421875
Test: [0/79]	Time 0.763 (0.763)	Loss 2.0135 (2.0135)	Prec@1 53.906 (53.906)
Test: [20/79]	Time 0.032 (0.074)	Loss 1.4060 (1.9607)	Prec@1 63.281 (58.557)
Test: [40/79]	Time 0.031 (0.053)	Loss 2.0110 (1.9656)	Prec@1 54.688 (58.498)
Test: [60/79]	Time 0.033 (0.047)	Loss 2.2332 (1.9958)	Prec@1 56.250 (57.966)
 * Prec@1 57.860
Epoch: [66][0/79]	Time 0.841 (0.841)	Loss 0.1374 (0.1374)	Prec@1 95.312 (95.312)
Epoch: [66][20/79]	Time 0.047 (0.084)	Loss 0.1863 (0.2156)	Prec@1 93.750 (92.485)
Epoch: [66][40/79]	Time 0.048 (0.066)	Loss 0.1514 (0.1846)	Prec@1 95.312 (93.636)
Epoch: [66][60/79]	Time 0.046 (0.059)	Loss 0.1720 (0.1676)	Prec@1 94.531 (94.326)
training time:  4.5929529666900635
Test: [0/79]	Time 0.801 (0.801)	Loss 1.3703 (1.3703)	Prec@1 67.188 (67.188)
Test: [20/79]	Time 0.032 (0.069)	Loss 1.2075 (1.4370)	Prec@1 63.281 (64.732)
Test: [40/79]	Time 0.032 (0.050)	Loss 1.5683 (1.4401)	Prec@1 64.062 (64.806)
Test: [60/79]	Time 0.032 (0.045)	Loss 1.6549 (1.4549)	Prec@1 60.938 (64.114)
 * Prec@1 64.440
Epoch: [67][0/79]	Time 0.843 (0.843)	Loss 0.0844 (0.0844)	Prec@1 96.875 (96.875)
Epoch: [67][20/79]	Time 0.044 (0.082)	Loss 0.1230 (0.1356)	Prec@1 96.094 (95.536)
Epoch: [67][40/79]	Time 0.046 (0.063)	Loss 0.0957 (0.1230)	Prec@1 96.875 (95.884)
Epoch: [67][60/79]	Time 0.047 (0.058)	Loss 0.0726 (0.1204)	Prec@1 99.219 (95.914)
training time:  4.5431671142578125
Test: [0/79]	Time 0.808 (0.808)	Loss 1.1763 (1.1763)	Prec@1 67.969 (67.969)
Test: [20/79]	Time 0.032 (0.069)	Loss 1.3427 (1.4547)	Prec@1 65.625 (64.509)
Test: [40/79]	Time 0.032 (0.051)	Loss 1.7210 (1.4879)	Prec@1 61.719 (63.872)
Test: [60/79]	Time 0.030 (0.045)	Loss 1.6568 (1.5208)	Prec@1 65.625 (63.217)
 * Prec@1 63.590
Epoch: [68][0/79]	Time 0.783 (0.783)	Loss 0.1628 (0.1628)	Prec@1 94.531 (94.531)
Epoch: [68][20/79]	Time 0.045 (0.081)	Loss 0.3270 (0.2319)	Prec@1 91.406 (92.225)
Epoch: [68][40/79]	Time 0.046 (0.064)	Loss 0.1442 (0.1944)	Prec@1 94.531 (93.388)
Epoch: [68][60/79]	Time 0.045 (0.057)	Loss 0.1181 (0.1714)	Prec@1 95.312 (94.134)
training time:  4.441202163696289
Test: [0/79]	Time 0.768 (0.768)	Loss 1.3782 (1.3782)	Prec@1 62.500 (62.500)
Test: [20/79]	Time 0.029 (0.066)	Loss 1.6860 (1.6846)	Prec@1 56.250 (60.640)
Test: [40/79]	Time 0.032 (0.049)	Loss 1.9592 (1.7228)	Prec@1 57.812 (60.328)
Test: [60/79]	Time 0.030 (0.044)	Loss 2.0702 (1.7430)	Prec@1 58.594 (60.079)
 * Prec@1 60.610
Epoch: [69][0/79]	Time 0.821 (0.821)	Loss 0.0483 (0.0483)	Prec@1 99.219 (99.219)
Epoch: [69][20/79]	Time 0.047 (0.083)	Loss 0.2245 (0.1945)	Prec@1 91.406 (93.638)
Epoch: [69][40/79]	Time 0.046 (0.065)	Loss 0.0929 (0.1568)	Prec@1 97.656 (94.931)
Epoch: [69][60/79]	Time 0.046 (0.059)	Loss 0.1342 (0.1388)	Prec@1 95.312 (95.505)
training time:  4.554523468017578
Test: [0/79]	Time 0.770 (0.770)	Loss 1.2048 (1.2048)	Prec@1 70.312 (70.312)
Test: [20/79]	Time 0.032 (0.067)	Loss 1.1282 (1.3117)	Prec@1 64.844 (67.857)
Test: [40/79]	Time 0.028 (0.050)	Loss 1.5732 (1.3081)	Prec@1 64.062 (67.511)
Test: [60/79]	Time 0.032 (0.044)	Loss 1.5092 (1.3155)	Prec@1 63.281 (67.469)
 * Prec@1 67.690
=> Saving checkpoint for epoch 69, with Prec@1 67.690000.
Epoch: [70][0/79]	Time 0.804 (0.804)	Loss 0.1019 (0.1019)	Prec@1 96.875 (96.875)
Epoch: [70][20/79]	Time 0.047 (0.081)	Loss 0.1086 (0.0945)	Prec@1 96.875 (97.321)
Epoch: [70][40/79]	Time 0.047 (0.064)	Loss 0.1102 (0.0924)	Prec@1 96.094 (97.199)
Epoch: [70][60/79]	Time 0.042 (0.057)	Loss 0.0581 (0.0866)	Prec@1 97.656 (97.387)
training time:  4.483428955078125
Test: [0/79]	Time 0.761 (0.761)	Loss 1.2332 (1.2332)	Prec@1 71.875 (71.875)
Test: [20/79]	Time 0.031 (0.066)	Loss 1.1234 (1.3270)	Prec@1 67.969 (68.527)
Test: [40/79]	Time 0.032 (0.049)	Loss 1.6182 (1.3100)	Prec@1 62.500 (67.816)
Test: [60/79]	Time 0.031 (0.044)	Loss 1.4610 (1.3152)	Prec@1 63.281 (67.559)
 * Prec@1 68.040
=> Saving checkpoint for epoch 70, with Prec@1 68.040000.
Epoch: [71][0/79]	Time 0.798 (0.798)	Loss 0.0604 (0.0604)	Prec@1 96.875 (96.875)
Epoch: [71][20/79]	Time 0.044 (0.088)	Loss 0.0303 (0.0636)	Prec@1 100.000 (98.214)
Epoch: [71][40/79]	Time 0.048 (0.067)	Loss 0.0330 (0.0583)	Prec@1 99.219 (98.418)
Epoch: [71][60/79]	Time 0.044 (0.060)	Loss 0.0334 (0.0539)	Prec@1 99.219 (98.578)
training time:  4.6572675704956055
Test: [0/79]	Time 0.780 (0.780)	Loss 1.0499 (1.0499)	Prec@1 73.438 (73.438)
Test: [20/79]	Time 0.032 (0.066)	Loss 1.1407 (1.2533)	Prec@1 64.062 (67.746)
Test: [40/79]	Time 0.028 (0.049)	Loss 1.4879 (1.2726)	Prec@1 64.062 (67.931)
Test: [60/79]	Time 0.032 (0.043)	Loss 1.5818 (1.2798)	Prec@1 64.062 (67.918)
 * Prec@1 68.170
=> Saving checkpoint for epoch 71, with Prec@1 68.170000.
Epoch: [72][0/79]	Time 0.794 (0.794)	Loss 0.0177 (0.0177)	Prec@1 100.000 (100.000)
Epoch: [72][20/79]	Time 0.046 (0.083)	Loss 0.0462 (0.0529)	Prec@1 99.219 (98.624)
Epoch: [72][40/79]	Time 0.047 (0.065)	Loss 0.0782 (0.0537)	Prec@1 97.656 (98.590)
Epoch: [72][60/79]	Time 0.047 (0.059)	Loss 0.0265 (0.0507)	Prec@1 99.219 (98.630)
training time:  4.5587804317474365
Test: [0/79]	Time 0.760 (0.760)	Loss 1.1869 (1.1869)	Prec@1 68.750 (68.750)
Test: [20/79]	Time 0.032 (0.066)	Loss 1.4371 (1.4177)	Prec@1 63.281 (67.150)
Test: [40/79]	Time 0.031 (0.049)	Loss 1.6679 (1.4245)	Prec@1 66.406 (66.845)
Test: [60/79]	Time 0.030 (0.043)	Loss 1.7238 (1.4346)	Prec@1 61.719 (66.611)
 * Prec@1 66.890
Epoch: [73][0/79]	Time 0.798 (0.798)	Loss 0.0371 (0.0371)	Prec@1 98.438 (98.438)
Epoch: [73][20/79]	Time 0.046 (0.082)	Loss 0.0739 (0.0645)	Prec@1 96.875 (97.805)
Epoch: [73][40/79]	Time 0.046 (0.065)	Loss 0.0189 (0.0567)	Prec@1 100.000 (98.304)
Epoch: [73][60/79]	Time 0.047 (0.059)	Loss 0.0769 (0.0552)	Prec@1 96.094 (98.309)
training time:  4.588658332824707
Test: [0/79]	Time 0.812 (0.812)	Loss 1.1445 (1.1445)	Prec@1 70.312 (70.312)
Test: [20/79]	Time 0.032 (0.069)	Loss 1.2245 (1.4418)	Prec@1 68.750 (65.960)
Test: [40/79]	Time 0.032 (0.051)	Loss 1.6964 (1.4661)	Prec@1 67.188 (66.197)
Test: [60/79]	Time 0.030 (0.045)	Loss 1.8011 (1.4802)	Prec@1 64.844 (66.099)
 * Prec@1 66.410
Epoch: [74][0/79]	Time 0.846 (0.846)	Loss 0.0523 (0.0523)	Prec@1 98.438 (98.438)
Epoch: [74][20/79]	Time 0.046 (0.085)	Loss 0.0874 (0.1066)	Prec@1 96.094 (97.024)
Epoch: [74][40/79]	Time 0.047 (0.066)	Loss 0.0426 (0.0848)	Prec@1 99.219 (97.485)
Epoch: [74][60/79]	Time 0.045 (0.060)	Loss 0.0256 (0.0760)	Prec@1 100.000 (97.746)
training time:  4.630062818527222
Test: [0/79]	Time 0.800 (0.800)	Loss 1.4012 (1.4012)	Prec@1 64.844 (64.844)
Test: [20/79]	Time 0.032 (0.068)	Loss 1.4193 (1.4885)	Prec@1 66.406 (65.737)
Test: [40/79]	Time 0.032 (0.050)	Loss 1.7073 (1.4606)	Prec@1 66.406 (66.273)
Test: [60/79]	Time 0.031 (0.044)	Loss 1.6413 (1.4555)	Prec@1 62.500 (66.189)
 * Prec@1 66.290
Epoch: [75][0/79]	Time 0.845 (0.845)	Loss 0.0405 (0.0405)	Prec@1 100.000 (100.000)
Epoch: [75][20/79]	Time 0.048 (0.084)	Loss 0.0873 (0.1257)	Prec@1 98.438 (95.982)
Epoch: [75][40/79]	Time 0.047 (0.066)	Loss 0.0516 (0.1045)	Prec@1 98.438 (96.704)
Epoch: [75][60/79]	Time 0.046 (0.059)	Loss 0.0378 (0.0919)	Prec@1 99.219 (97.234)
training time:  4.630311965942383
Test: [0/79]	Time 0.801 (0.801)	Loss 1.3153 (1.3153)	Prec@1 67.969 (67.969)
Test: [20/79]	Time 0.031 (0.069)	Loss 1.1888 (1.3805)	Prec@1 73.438 (67.299)
Test: [40/79]	Time 0.032 (0.050)	Loss 1.7691 (1.4059)	Prec@1 65.625 (67.492)
Test: [60/79]	Time 0.031 (0.044)	Loss 1.6726 (1.4090)	Prec@1 66.406 (67.175)
 * Prec@1 67.230
Epoch: [76][0/79]	Time 0.848 (0.848)	Loss 0.0296 (0.0296)	Prec@1 100.000 (100.000)
Epoch: [76][20/79]	Time 0.047 (0.084)	Loss 0.1086 (0.0760)	Prec@1 95.312 (97.470)
Epoch: [76][40/79]	Time 0.048 (0.066)	Loss 0.0903 (0.0590)	Prec@1 96.875 (98.304)
Epoch: [76][60/79]	Time 0.042 (0.059)	Loss 0.0223 (0.0565)	Prec@1 100.000 (98.335)
training time:  4.578520059585571
Test: [0/79]	Time 0.792 (0.792)	Loss 1.1900 (1.1900)	Prec@1 69.531 (69.531)
Test: [20/79]	Time 0.032 (0.075)	Loss 1.1872 (1.3649)	Prec@1 72.656 (68.713)
Test: [40/79]	Time 0.032 (0.054)	Loss 1.6060 (1.3632)	Prec@1 64.844 (68.598)
Test: [60/79]	Time 0.032 (0.047)	Loss 1.7335 (1.3649)	Prec@1 66.406 (68.366)
 * Prec@1 68.680
=> Saving checkpoint for epoch 76, with Prec@1 68.680000.
Epoch: [77][0/79]	Time 0.792 (0.792)	Loss 0.0250 (0.0250)	Prec@1 100.000 (100.000)
Epoch: [77][20/79]	Time 0.046 (0.082)	Loss 0.0260 (0.0479)	Prec@1 99.219 (98.624)
Epoch: [77][40/79]	Time 0.047 (0.064)	Loss 0.0649 (0.0443)	Prec@1 97.656 (98.780)
Epoch: [77][60/79]	Time 0.046 (0.059)	Loss 0.0265 (0.0413)	Prec@1 100.000 (98.963)
training time:  4.520054340362549
Test: [0/79]	Time 0.757 (0.757)	Loss 1.1512 (1.1512)	Prec@1 71.094 (71.094)
Test: [20/79]	Time 0.032 (0.067)	Loss 1.1294 (1.4048)	Prec@1 71.094 (68.862)
Test: [40/79]	Time 0.032 (0.050)	Loss 1.5564 (1.3924)	Prec@1 66.406 (68.788)
Test: [60/79]	Time 0.031 (0.044)	Loss 1.7972 (1.4109)	Prec@1 63.281 (68.443)
 * Prec@1 68.880
=> Saving checkpoint for epoch 77, with Prec@1 68.880000.
Epoch: [78][0/79]	Time 0.789 (0.789)	Loss 0.0213 (0.0213)	Prec@1 100.000 (100.000)
Epoch: [78][20/79]	Time 0.047 (0.080)	Loss 0.0348 (0.0371)	Prec@1 98.438 (98.958)
Epoch: [78][40/79]	Time 0.042 (0.063)	Loss 0.0208 (0.0366)	Prec@1 100.000 (98.990)
Epoch: [78][60/79]	Time 0.042 (0.057)	Loss 0.0244 (0.0346)	Prec@1 99.219 (99.078)
training time:  4.443525791168213
Test: [0/79]	Time 0.752 (0.752)	Loss 1.2796 (1.2796)	Prec@1 68.750 (68.750)
Test: [20/79]	Time 0.032 (0.066)	Loss 1.3216 (1.4861)	Prec@1 70.312 (66.518)
Test: [40/79]	Time 0.032 (0.050)	Loss 1.8212 (1.4953)	Prec@1 64.844 (66.692)
Test: [60/79]	Time 0.031 (0.044)	Loss 1.6802 (1.5062)	Prec@1 64.844 (66.432)
 * Prec@1 66.740
Epoch: [79][0/79]	Time 0.799 (0.799)	Loss 0.0529 (0.0529)	Prec@1 98.438 (98.438)
Epoch: [79][20/79]	Time 0.044 (0.082)	Loss 0.0558 (0.0819)	Prec@1 98.438 (97.656)
Epoch: [79][40/79]	Time 0.046 (0.065)	Loss 0.0491 (0.0665)	Prec@1 98.438 (98.114)
Epoch: [79][60/79]	Time 0.045 (0.059)	Loss 0.0227 (0.0585)	Prec@1 100.000 (98.425)
training time:  4.570154666900635
Test: [0/79]	Time 0.780 (0.780)	Loss 1.2102 (1.2102)	Prec@1 68.750 (68.750)
Test: [20/79]	Time 0.031 (0.067)	Loss 1.0928 (1.3046)	Prec@1 71.875 (69.234)
Test: [40/79]	Time 0.032 (0.050)	Loss 1.4869 (1.3006)	Prec@1 67.188 (69.512)
Test: [60/79]	Time 0.029 (0.044)	Loss 1.5034 (1.3092)	Prec@1 67.969 (69.326)
 * Prec@1 69.400
=> Saving checkpoint for epoch 79, with Prec@1 69.400000.
Epoch: [80][0/79]	Time 0.798 (0.798)	Loss 0.0251 (0.0251)	Prec@1 100.000 (100.000)
Epoch: [80][20/79]	Time 0.048 (0.082)	Loss 0.0267 (0.0452)	Prec@1 99.219 (98.735)
Epoch: [80][40/79]	Time 0.049 (0.065)	Loss 0.0532 (0.0395)	Prec@1 97.656 (98.895)
Epoch: [80][60/79]	Time 0.048 (0.059)	Loss 0.0350 (0.0364)	Prec@1 99.219 (99.039)
training time:  4.569734811782837
Test: [0/79]	Time 0.747 (0.747)	Loss 1.1664 (1.1664)	Prec@1 73.438 (73.438)
Test: [20/79]	Time 0.033 (0.066)	Loss 1.0995 (1.3208)	Prec@1 70.312 (69.903)
Test: [40/79]	Time 0.032 (0.050)	Loss 1.5009 (1.3113)	Prec@1 69.531 (70.179)
Test: [60/79]	Time 0.033 (0.044)	Loss 1.5374 (1.3257)	Prec@1 64.844 (69.531)
 * Prec@1 69.620
=> Saving checkpoint for epoch 80, with Prec@1 69.620000.
Epoch: [81][0/79]	Time 0.805 (0.805)	Loss 0.0361 (0.0361)	Prec@1 99.219 (99.219)
Epoch: [81][20/79]	Time 0.048 (0.083)	Loss 0.0299 (0.0286)	Prec@1 99.219 (99.368)
Epoch: [81][40/79]	Time 0.045 (0.065)	Loss 0.0141 (0.0254)	Prec@1 100.000 (99.543)
Epoch: [81][60/79]	Time 0.048 (0.059)	Loss 0.0464 (0.0253)	Prec@1 98.438 (99.539)
training time:  4.586823225021362
Test: [0/79]	Time 0.759 (0.759)	Loss 1.1493 (1.1493)	Prec@1 70.312 (70.312)
Test: [20/79]	Time 0.030 (0.065)	Loss 1.1198 (1.3044)	Prec@1 70.312 (69.717)
Test: [40/79]	Time 0.032 (0.048)	Loss 1.4700 (1.3095)	Prec@1 68.750 (69.341)
Test: [60/79]	Time 0.030 (0.043)	Loss 1.5970 (1.3211)	Prec@1 65.625 (68.801)
 * Prec@1 69.070
Epoch: [82][0/79]	Time 0.850 (0.850)	Loss 0.0553 (0.0553)	Prec@1 99.219 (99.219)
Epoch: [82][20/79]	Time 0.042 (0.089)	Loss 0.0194 (0.0286)	Prec@1 100.000 (99.256)
Epoch: [82][40/79]	Time 0.045 (0.067)	Loss 0.0290 (0.0302)	Prec@1 98.438 (99.295)
Epoch: [82][60/79]	Time 0.046 (0.060)	Loss 0.0148 (0.0287)	Prec@1 100.000 (99.321)
training time:  4.639126777648926
Test: [0/79]	Time 0.813 (0.813)	Loss 1.2163 (1.2163)	Prec@1 73.438 (73.438)
Test: [20/79]	Time 0.032 (0.068)	Loss 1.1180 (1.2796)	Prec@1 69.531 (70.573)
Test: [40/79]	Time 0.031 (0.051)	Loss 1.5404 (1.2758)	Prec@1 71.094 (69.989)
Test: [60/79]	Time 0.032 (0.045)	Loss 1.4650 (1.2865)	Prec@1 66.406 (69.621)
 * Prec@1 69.670
=> Saving checkpoint for epoch 82, with Prec@1 69.670000.
Epoch: [83][0/79]	Time 0.794 (0.794)	Loss 0.0095 (0.0095)	Prec@1 100.000 (100.000)
Epoch: [83][20/79]	Time 0.046 (0.082)	Loss 0.0170 (0.0199)	Prec@1 100.000 (99.479)
Epoch: [83][40/79]	Time 0.046 (0.065)	Loss 0.0137 (0.0192)	Prec@1 100.000 (99.581)
Epoch: [83][60/79]	Time 0.047 (0.059)	Loss 0.0176 (0.0196)	Prec@1 100.000 (99.577)
training time:  4.556874990463257
Test: [0/79]	Time 0.757 (0.757)	Loss 1.2415 (1.2415)	Prec@1 72.656 (72.656)
Test: [20/79]	Time 0.032 (0.066)	Loss 1.1243 (1.3265)	Prec@1 70.312 (69.568)
Test: [40/79]	Time 0.033 (0.050)	Loss 1.5512 (1.3280)	Prec@1 69.531 (69.188)
Test: [60/79]	Time 0.031 (0.044)	Loss 1.5489 (1.3396)	Prec@1 65.625 (68.955)
 * Prec@1 69.200
Epoch: [84][0/79]	Time 0.802 (0.802)	Loss 0.0230 (0.0230)	Prec@1 99.219 (99.219)
Epoch: [84][20/79]	Time 0.046 (0.082)	Loss 0.0171 (0.0154)	Prec@1 100.000 (99.851)
Epoch: [84][40/79]	Time 0.045 (0.064)	Loss 0.0249 (0.0195)	Prec@1 99.219 (99.543)
Epoch: [84][60/79]	Time 0.046 (0.058)	Loss 0.0288 (0.0188)	Prec@1 99.219 (99.616)
training time:  4.533487558364868
Test: [0/79]	Time 0.788 (0.788)	Loss 1.1465 (1.1465)	Prec@1 71.875 (71.875)
Test: [20/79]	Time 0.032 (0.068)	Loss 1.0599 (1.2965)	Prec@1 75.000 (70.461)
Test: [40/79]	Time 0.033 (0.050)	Loss 1.5508 (1.2997)	Prec@1 71.875 (70.198)
Test: [60/79]	Time 0.031 (0.045)	Loss 1.5655 (1.3111)	Prec@1 64.844 (69.762)
 * Prec@1 69.870
=> Saving checkpoint for epoch 84, with Prec@1 69.870000.
Epoch: [85][0/79]	Time 0.841 (0.841)	Loss 0.0139 (0.0139)	Prec@1 100.000 (100.000)
Epoch: [85][20/79]	Time 0.044 (0.084)	Loss 0.0738 (0.0256)	Prec@1 98.438 (99.330)
Epoch: [85][40/79]	Time 0.046 (0.066)	Loss 0.0259 (0.0230)	Prec@1 99.219 (99.447)
Epoch: [85][60/79]	Time 0.046 (0.059)	Loss 0.0170 (0.0213)	Prec@1 100.000 (99.539)
training time:  4.596777439117432
Test: [0/79]	Time 0.793 (0.793)	Loss 1.2179 (1.2179)	Prec@1 72.656 (72.656)
Test: [20/79]	Time 0.032 (0.068)	Loss 1.1151 (1.3287)	Prec@1 71.875 (70.312)
Test: [40/79]	Time 0.032 (0.050)	Loss 1.5923 (1.3312)	Prec@1 69.531 (69.684)
Test: [60/79]	Time 0.031 (0.045)	Loss 1.6476 (1.3458)	Prec@1 61.719 (69.045)
 * Prec@1 69.180
Epoch: [86][0/79]	Time 0.847 (0.847)	Loss 0.0110 (0.0110)	Prec@1 100.000 (100.000)
Epoch: [86][20/79]	Time 0.047 (0.085)	Loss 0.0110 (0.0216)	Prec@1 100.000 (99.554)
Epoch: [86][40/79]	Time 0.046 (0.066)	Loss 0.0139 (0.0201)	Prec@1 100.000 (99.581)
Epoch: [86][60/79]	Time 0.046 (0.060)	Loss 0.0609 (0.0198)	Prec@1 96.875 (99.590)
training time:  4.6082987785339355
Test: [0/79]	Time 0.806 (0.806)	Loss 1.1093 (1.1093)	Prec@1 74.219 (74.219)
Test: [20/79]	Time 0.031 (0.069)	Loss 1.0797 (1.2946)	Prec@1 72.656 (70.536)
Test: [40/79]	Time 0.032 (0.051)	Loss 1.4974 (1.2935)	Prec@1 68.750 (70.065)
Test: [60/79]	Time 0.031 (0.045)	Loss 1.5309 (1.3046)	Prec@1 64.062 (69.775)
 * Prec@1 69.890
=> Saving checkpoint for epoch 86, with Prec@1 69.890000.
Epoch: [87][0/79]	Time 0.836 (0.836)	Loss 0.0156 (0.0156)	Prec@1 100.000 (100.000)
Epoch: [87][20/79]	Time 0.046 (0.083)	Loss 0.0222 (0.0153)	Prec@1 99.219 (99.702)
Epoch: [87][40/79]	Time 0.045 (0.065)	Loss 0.0165 (0.0151)	Prec@1 100.000 (99.752)
Epoch: [87][60/79]	Time 0.045 (0.059)	Loss 0.0104 (0.0149)	Prec@1 100.000 (99.744)
training time:  4.5747880935668945
Test: [0/79]	Time 0.795 (0.795)	Loss 1.1127 (1.1127)	Prec@1 74.219 (74.219)
Test: [20/79]	Time 0.032 (0.075)	Loss 1.0788 (1.2722)	Prec@1 73.438 (70.685)
Test: [40/79]	Time 0.032 (0.054)	Loss 1.4398 (1.2716)	Prec@1 71.875 (70.560)
Test: [60/79]	Time 0.032 (0.047)	Loss 1.4715 (1.2839)	Prec@1 64.062 (70.133)
 * Prec@1 70.240
=> Saving checkpoint for epoch 87, with Prec@1 70.240000.
Epoch: [88][0/79]	Time 0.834 (0.834)	Loss 0.0168 (0.0168)	Prec@1 100.000 (100.000)
Epoch: [88][20/79]	Time 0.050 (0.082)	Loss 0.0047 (0.0155)	Prec@1 100.000 (99.814)
Epoch: [88][40/79]	Time 0.044 (0.063)	Loss 0.0168 (0.0148)	Prec@1 100.000 (99.771)
Epoch: [88][60/79]	Time 0.042 (0.058)	Loss 0.0276 (0.0148)	Prec@1 99.219 (99.744)
training time:  4.4615137577056885
Test: [0/79]	Time 0.798 (0.798)	Loss 1.1483 (1.1483)	Prec@1 73.438 (73.438)
Test: [20/79]	Time 0.033 (0.068)	Loss 1.1013 (1.3005)	Prec@1 71.094 (70.201)
Test: [40/79]	Time 0.032 (0.051)	Loss 1.4371 (1.2959)	Prec@1 68.750 (70.141)
Test: [60/79]	Time 0.031 (0.045)	Loss 1.4975 (1.3122)	Prec@1 64.062 (69.826)
 * Prec@1 69.990
Epoch: [89][0/79]	Time 0.840 (0.840)	Loss 0.0168 (0.0168)	Prec@1 99.219 (99.219)
Epoch: [89][20/79]	Time 0.046 (0.084)	Loss 0.0174 (0.0158)	Prec@1 99.219 (99.702)
Epoch: [89][40/79]	Time 0.047 (0.066)	Loss 0.0152 (0.0174)	Prec@1 100.000 (99.714)
Epoch: [89][60/79]	Time 0.048 (0.060)	Loss 0.0127 (0.0167)	Prec@1 100.000 (99.693)
training time:  4.676878213882446
Test: [0/79]	Time 0.799 (0.799)	Loss 1.0754 (1.0754)	Prec@1 74.219 (74.219)
Test: [20/79]	Time 0.031 (0.068)	Loss 1.0650 (1.2470)	Prec@1 73.438 (70.833)
Test: [40/79]	Time 0.032 (0.051)	Loss 1.4802 (1.2440)	Prec@1 68.750 (70.675)
Test: [60/79]	Time 0.030 (0.045)	Loss 1.4770 (1.2522)	Prec@1 63.281 (70.453)
 * Prec@1 70.530
=> Saving checkpoint for epoch 89, with Prec@1 70.530000.
Epoch: [90][0/79]	Time 0.794 (0.794)	Loss 0.0084 (0.0084)	Prec@1 100.000 (100.000)
Epoch: [90][20/79]	Time 0.044 (0.082)	Loss 0.0092 (0.0115)	Prec@1 100.000 (99.963)
Epoch: [90][40/79]	Time 0.044 (0.065)	Loss 0.0140 (0.0138)	Prec@1 100.000 (99.790)
Epoch: [90][60/79]	Time 0.046 (0.059)	Loss 0.0062 (0.0135)	Prec@1 100.000 (99.821)
training time:  4.565984010696411
Test: [0/79]	Time 0.758 (0.758)	Loss 1.1260 (1.1260)	Prec@1 74.219 (74.219)
Test: [20/79]	Time 0.033 (0.067)	Loss 1.0696 (1.2710)	Prec@1 71.094 (70.685)
Test: [40/79]	Time 0.032 (0.050)	Loss 1.5324 (1.2760)	Prec@1 68.750 (70.312)
Test: [60/79]	Time 0.031 (0.044)	Loss 1.5902 (1.2936)	Prec@1 62.500 (69.890)
 * Prec@1 70.040
Epoch: [91][0/79]	Time 0.807 (0.807)	Loss 0.0171 (0.0171)	Prec@1 99.219 (99.219)
Epoch: [91][20/79]	Time 0.041 (0.081)	Loss 0.0417 (0.0129)	Prec@1 99.219 (99.814)
Epoch: [91][40/79]	Time 0.046 (0.064)	Loss 0.0096 (0.0123)	Prec@1 100.000 (99.848)
Epoch: [91][60/79]	Time 0.043 (0.058)	Loss 0.0256 (0.0129)	Prec@1 99.219 (99.821)
training time:  4.536445617675781
Test: [0/79]	Time 0.801 (0.801)	Loss 1.0940 (1.0940)	Prec@1 74.219 (74.219)
Test: [20/79]	Time 0.032 (0.068)	Loss 1.1450 (1.2905)	Prec@1 70.312 (70.387)
Test: [40/79]	Time 0.031 (0.051)	Loss 1.5099 (1.2907)	Prec@1 69.531 (70.103)
Test: [60/79]	Time 0.031 (0.045)	Loss 1.6300 (1.3104)	Prec@1 63.281 (69.570)
 * Prec@1 69.770
Epoch: [92][0/79]	Time 0.836 (0.836)	Loss 0.0077 (0.0077)	Prec@1 100.000 (100.000)
Epoch: [92][20/79]	Time 0.047 (0.084)	Loss 0.0129 (0.0130)	Prec@1 100.000 (99.851)
Epoch: [92][40/79]	Time 0.045 (0.066)	Loss 0.0259 (0.0109)	Prec@1 99.219 (99.905)
Epoch: [92][60/79]	Time 0.046 (0.059)	Loss 0.0057 (0.0112)	Prec@1 100.000 (99.898)
training time:  4.594113349914551
Test: [0/79]	Time 0.762 (0.762)	Loss 1.1069 (1.1069)	Prec@1 74.219 (74.219)
Test: [20/79]	Time 0.032 (0.065)	Loss 1.0966 (1.2962)	Prec@1 71.875 (70.164)
Test: [40/79]	Time 0.033 (0.049)	Loss 1.5281 (1.2942)	Prec@1 70.312 (70.255)
Test: [60/79]	Time 0.031 (0.043)	Loss 1.5770 (1.3080)	Prec@1 63.281 (69.723)
 * Prec@1 69.780
Epoch: [93][0/79]	Time 0.925 (0.925)	Loss 0.0069 (0.0069)	Prec@1 100.000 (100.000)
Epoch: [93][20/79]	Time 0.042 (0.087)	Loss 0.0231 (0.0133)	Prec@1 99.219 (99.740)
Epoch: [93][40/79]	Time 0.043 (0.066)	Loss 0.0080 (0.0119)	Prec@1 100.000 (99.809)
Epoch: [93][60/79]	Time 0.042 (0.059)	Loss 0.0075 (0.0119)	Prec@1 100.000 (99.821)
training time:  4.609846830368042
Test: [0/79]	Time 0.762 (0.762)	Loss 1.0938 (1.0938)	Prec@1 74.219 (74.219)
Test: [20/79]	Time 0.032 (0.066)	Loss 1.1454 (1.3224)	Prec@1 71.875 (70.312)
Test: [40/79]	Time 0.032 (0.050)	Loss 1.5582 (1.3181)	Prec@1 68.750 (70.027)
Test: [60/79]	Time 0.031 (0.044)	Loss 1.6111 (1.3322)	Prec@1 60.938 (69.672)
 * Prec@1 69.710
Epoch: [94][0/79]	Time 0.804 (0.804)	Loss 0.0141 (0.0141)	Prec@1 100.000 (100.000)
Epoch: [94][20/79]	Time 0.045 (0.082)	Loss 0.0137 (0.0131)	Prec@1 100.000 (99.740)
Epoch: [94][40/79]	Time 0.046 (0.064)	Loss 0.0408 (0.0130)	Prec@1 99.219 (99.790)
Epoch: [94][60/79]	Time 0.046 (0.058)	Loss 0.0239 (0.0124)	Prec@1 99.219 (99.795)
training time:  4.474717140197754
Test: [0/79]	Time 0.764 (0.764)	Loss 1.0776 (1.0776)	Prec@1 75.781 (75.781)
Test: [20/79]	Time 0.029 (0.064)	Loss 1.1611 (1.3025)	Prec@1 67.969 (70.536)
Test: [40/79]	Time 0.031 (0.049)	Loss 1.5090 (1.3051)	Prec@1 66.406 (70.179)
Test: [60/79]	Time 0.030 (0.043)	Loss 1.6270 (1.3206)	Prec@1 62.500 (69.582)
 * Prec@1 69.680
Epoch: [95][0/79]	Time 0.793 (0.793)	Loss 0.0215 (0.0215)	Prec@1 99.219 (99.219)
Epoch: [95][20/79]	Time 0.043 (0.079)	Loss 0.0110 (0.0114)	Prec@1 100.000 (99.777)
Epoch: [95][40/79]	Time 0.046 (0.063)	Loss 0.0185 (0.0124)	Prec@1 99.219 (99.771)
Epoch: [95][60/79]	Time 0.047 (0.057)	Loss 0.0193 (0.0119)	Prec@1 99.219 (99.808)
training time:  4.47109317779541
Test: [0/79]	Time 0.797 (0.797)	Loss 1.0967 (1.0967)	Prec@1 71.094 (71.094)
Test: [20/79]	Time 0.031 (0.068)	Loss 1.1240 (1.2824)	Prec@1 71.875 (70.424)
Test: [40/79]	Time 0.032 (0.051)	Loss 1.6101 (1.2862)	Prec@1 70.312 (70.236)
Test: [60/79]	Time 0.031 (0.045)	Loss 1.5715 (1.2967)	Prec@1 62.500 (69.915)
 * Prec@1 70.030
Epoch: [96][0/79]	Time 0.839 (0.839)	Loss 0.0116 (0.0116)	Prec@1 100.000 (100.000)
Epoch: [96][20/79]	Time 0.046 (0.084)	Loss 0.0195 (0.0114)	Prec@1 99.219 (99.777)
Epoch: [96][40/79]	Time 0.042 (0.065)	Loss 0.0061 (0.0110)	Prec@1 100.000 (99.867)
Epoch: [96][60/79]	Time 0.043 (0.059)	Loss 0.0113 (0.0109)	Prec@1 100.000 (99.859)
training time:  4.558666706085205
Test: [0/79]	Time 0.802 (0.802)	Loss 1.1228 (1.1228)	Prec@1 72.656 (72.656)
Test: [20/79]	Time 0.031 (0.069)	Loss 1.0940 (1.2832)	Prec@1 73.438 (71.057)
Test: [40/79]	Time 0.032 (0.051)	Loss 1.5306 (1.2840)	Prec@1 68.750 (70.465)
Test: [60/79]	Time 0.031 (0.045)	Loss 1.5943 (1.2992)	Prec@1 61.719 (69.941)
 * Prec@1 69.910
Epoch: [97][0/79]	Time 0.827 (0.827)	Loss 0.0073 (0.0073)	Prec@1 100.000 (100.000)
Epoch: [97][20/79]	Time 0.047 (0.083)	Loss 0.0071 (0.0095)	Prec@1 100.000 (99.926)
Epoch: [97][40/79]	Time 0.047 (0.065)	Loss 0.0247 (0.0099)	Prec@1 100.000 (99.867)
Epoch: [97][60/79]	Time 0.045 (0.059)	Loss 0.0040 (0.0094)	Prec@1 100.000 (99.898)
training time:  4.577986240386963
Test: [0/79]	Time 0.790 (0.790)	Loss 1.0969 (1.0969)	Prec@1 73.438 (73.438)
Test: [20/79]	Time 0.028 (0.067)	Loss 1.1599 (1.2876)	Prec@1 70.312 (70.685)
Test: [40/79]	Time 0.028 (0.049)	Loss 1.5259 (1.2891)	Prec@1 71.094 (70.351)
Test: [60/79]	Time 0.032 (0.043)	Loss 1.6294 (1.3022)	Prec@1 60.156 (69.685)
 * Prec@1 69.830
Epoch: [98][0/79]	Time 0.833 (0.833)	Loss 0.0098 (0.0098)	Prec@1 100.000 (100.000)
Epoch: [98][20/79]	Time 0.042 (0.083)	Loss 0.0132 (0.0128)	Prec@1 100.000 (99.702)
Epoch: [98][40/79]	Time 0.049 (0.066)	Loss 0.0194 (0.0135)	Prec@1 99.219 (99.771)
Epoch: [98][60/79]	Time 0.045 (0.060)	Loss 0.0198 (0.0133)	Prec@1 99.219 (99.744)
training time:  4.623612880706787
Test: [0/79]	Time 0.761 (0.761)	Loss 1.1011 (1.1011)	Prec@1 74.219 (74.219)
Test: [20/79]	Time 0.032 (0.073)	Loss 1.1259 (1.3032)	Prec@1 70.312 (70.312)
Test: [40/79]	Time 0.031 (0.053)	Loss 1.5132 (1.3036)	Prec@1 70.312 (70.217)
Test: [60/79]	Time 0.031 (0.046)	Loss 1.5797 (1.3143)	Prec@1 62.500 (69.621)
 * Prec@1 69.690
Epoch: [99][0/79]	Time 0.825 (0.825)	Loss 0.0153 (0.0153)	Prec@1 100.000 (100.000)
Epoch: [99][20/79]	Time 0.045 (0.083)	Loss 0.0048 (0.0115)	Prec@1 100.000 (99.814)
Epoch: [99][40/79]	Time 0.046 (0.065)	Loss 0.0060 (0.0107)	Prec@1 100.000 (99.867)
Epoch: [99][60/79]	Time 0.045 (0.059)	Loss 0.0157 (0.0115)	Prec@1 100.000 (99.834)
training time:  4.572977066040039
Test: [0/79]	Time 0.820 (0.820)	Loss 1.0626 (1.0626)	Prec@1 72.656 (72.656)
Test: [20/79]	Time 0.033 (0.070)	Loss 1.1326 (1.2914)	Prec@1 71.094 (70.164)
Test: [40/79]	Time 0.032 (0.051)	Loss 1.4843 (1.2893)	Prec@1 71.094 (70.141)
Test: [60/79]	Time 0.031 (0.045)	Loss 1.5893 (1.2992)	Prec@1 64.062 (69.647)
 * Prec@1 69.850
training time:  806.513429403305
| Best accuracy:  70.53
================== Exp 1 ==================
dataset: CIFAR10, model: ResNet18, selection: GraNd, num_ex: 10, epochs: 100, fraction: 0.2, seed: 73952, lr: 0.1, save_path: ./result, resume: , device: cuda, checkpoint_name: CIFAR10_ResNet18_GraNd_exp0_epoch100_2024-03-18 15:32:32.606939_0.2_
Files already downloaded and verified
Files already downloaded and verified
called grand, with 10 ensemble and 1 epochs
=> Early Training Epoch #0
| Epoch [  0/  1] Iter[  1/391]		Loss: 2.3862
| Epoch [  0/  1] Iter[ 21/391]		Loss: 2.3717
| Epoch [  0/  1] Iter[ 41/391]		Loss: 2.2695
| Epoch [  0/  1] Iter[ 61/391]		Loss: 2.1487
| Epoch [  0/  1] Iter[ 81/391]		Loss: 1.9989
| Epoch [  0/  1] Iter[101/391]		Loss: 2.0706
| Epoch [  0/  1] Iter[121/391]		Loss: 1.8308
| Epoch [  0/  1] Iter[141/391]		Loss: 1.9563
| Epoch [  0/  1] Iter[161/391]		Loss: 1.9120
| Epoch [  0/  1] Iter[181/391]		Loss: 1.8383
| Epoch [  0/  1] Iter[201/391]		Loss: 1.7164
| Epoch [  0/  1] Iter[221/391]		Loss: 1.6404
| Epoch [  0/  1] Iter[241/391]		Loss: 1.4960
| Epoch [  0/  1] Iter[261/391]		Loss: 1.6509
| Epoch [  0/  1] Iter[281/391]		Loss: 1.6166
| Epoch [  0/  1] Iter[301/391]		Loss: 1.4106
| Epoch [  0/  1] Iter[321/391]		Loss: 1.5383
| Epoch [  0/  1] Iter[341/391]		Loss: 1.4182
| Epoch [  0/  1] Iter[361/391]		Loss: 1.3899
| Epoch [  0/  1] Iter[381/391]		Loss: 1.4022
=> Early Training Epoch #0
| Epoch [  0/  1] Iter[  1/391]		Loss: 2.5003
| Epoch [  0/  1] Iter[ 21/391]		Loss: 3.6557
| Epoch [  0/  1] Iter[ 41/391]		Loss: 2.3912
| Epoch [  0/  1] Iter[ 61/391]		Loss: 2.0460
| Epoch [  0/  1] Iter[ 81/391]		Loss: 2.0503
| Epoch [  0/  1] Iter[101/391]		Loss: 1.9826
| Epoch [  0/  1] Iter[121/391]		Loss: 1.9539
| Epoch [  0/  1] Iter[141/391]		Loss: 1.9745
| Epoch [  0/  1] Iter[161/391]		Loss: 1.8561
| Epoch [  0/  1] Iter[181/391]		Loss: 1.9131
| Epoch [  0/  1] Iter[201/391]		Loss: 1.8096
| Epoch [  0/  1] Iter[221/391]		Loss: 1.7651
| Epoch [  0/  1] Iter[241/391]		Loss: 1.7666
| Epoch [  0/  1] Iter[261/391]		Loss: 1.6713
| Epoch [  0/  1] Iter[281/391]		Loss: 1.7443
| Epoch [  0/  1] Iter[301/391]		Loss: 1.6685
| Epoch [  0/  1] Iter[321/391]		Loss: 1.6294
| Epoch [  0/  1] Iter[341/391]		Loss: 1.3622
| Epoch [  0/  1] Iter[361/391]		Loss: 1.6502
| Epoch [  0/  1] Iter[381/391]		Loss: 1.5491
=> Early Training Epoch #0
| Epoch [  0/  1] Iter[  1/391]		Loss: 2.3938
| Epoch [  0/  1] Iter[ 21/391]		Loss: 2.4090
| Epoch [  0/  1] Iter[ 41/391]		Loss: 2.0319
| Epoch [  0/  1] Iter[ 61/391]		Loss: 1.8047
| Epoch [  0/  1] Iter[ 81/391]		Loss: 1.9487
| Epoch [  0/  1] Iter[101/391]		Loss: 1.9321
| Epoch [  0/  1] Iter[121/391]		Loss: 1.6567
| Epoch [  0/  1] Iter[141/391]		Loss: 1.8993
| Epoch [  0/  1] Iter[161/391]		Loss: 1.6090
| Epoch [  0/  1] Iter[181/391]		Loss: 1.5772
| Epoch [  0/  1] Iter[201/391]		Loss: 1.4599
| Epoch [  0/  1] Iter[221/391]		Loss: 1.5249
Traceback (most recent call last):
  File "/home/yancheng/code/DeepCore/main.py", line 351, in <module>
    main()
  File "/home/yancheng/code/DeepCore/main.py", line 174, in main
    subset = method.select()
  File "/home/yancheng/code/DeepCore/deepcore/methods/grand.py", line 64, in select
    self.run()
  File "/home/yancheng/code/DeepCore/deepcore/methods/earlytrain.py", line 140, in run
    self.train(epoch, list_of_train_idx)
  File "/home/yancheng/code/DeepCore/deepcore/methods/earlytrain.py", line 92, in train
    loss.backward()
  File "/home/yancheng/.local/lib/python3.10/site-packages/torch/_tensor.py", line 492, in backward
    torch.autograd.backward(
  File "/home/yancheng/.local/lib/python3.10/site-packages/torch/autograd/__init__.py", line 251, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
KeyboardInterrupt
Traceback (most recent call last):
  File "/home/yancheng/code/DeepCore/main.py", line 351, in <module>
    main()
  File "/home/yancheng/code/DeepCore/main.py", line 174, in main
    subset = method.select()
  File "/home/yancheng/code/DeepCore/deepcore/methods/grand.py", line 64, in select
    self.run()
  File "/home/yancheng/code/DeepCore/deepcore/methods/earlytrain.py", line 140, in run
    self.train(epoch, list_of_train_idx)
  File "/home/yancheng/code/DeepCore/deepcore/methods/earlytrain.py", line 92, in train
    loss.backward()
  File "/home/yancheng/.local/lib/python3.10/site-packages/torch/_tensor.py", line 492, in backward
    torch.autograd.backward(
  File "/home/yancheng/.local/lib/python3.10/site-packages/torch/autograd/__init__.py", line 251, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
KeyboardInterrupt