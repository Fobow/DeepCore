================== Exp 0 ==================
dataset: CIFAR10, model: ResNet18, selection: Full, num_ex: 1, epochs: 100, fraction: 0.2, seed: 61244, lr: 0.1, save_path: ./result, resume: , device: cuda, checkpoint_name: CIFAR10_ResNet18_Full_exp0_epoch100_2024-03-18 18:57:43.019468_0.2_
Files already downloaded and verified
Files already downloaded and verified
=> selecting time:  6.079673767089844e-05
=> number of seletcted samples:  50000
=> Saving checkpoint for epoch 0, with Prec@1 0.000000.
Epoch: [0][0/391]	Time 4.694 (4.694)	Loss 2.4124 (2.4124)	Prec@1 7.812 (7.812)
Epoch: [0][20/391]	Time 0.042 (0.267)	Loss 2.2609 (4.3382)	Prec@1 17.969 (13.393)
Epoch: [0][40/391]	Time 0.043 (0.158)	Loss 2.0722 (3.3090)	Prec@1 16.406 (15.854)
Epoch: [0][60/391]	Time 0.045 (0.121)	Loss 2.2111 (2.9179)	Prec@1 12.500 (16.944)
Epoch: [0][80/391]	Time 0.049 (0.103)	Loss 1.9561 (2.7019)	Prec@1 26.562 (18.316)
Epoch: [0][100/391]	Time 0.046 (0.092)	Loss 1.9845 (2.5628)	Prec@1 25.000 (19.701)
Epoch: [0][120/391]	Time 0.048 (0.084)	Loss 1.9160 (2.4666)	Prec@1 32.812 (20.687)
Epoch: [0][140/391]	Time 0.045 (0.079)	Loss 2.0811 (2.3953)	Prec@1 23.438 (21.786)
Epoch: [0][160/391]	Time 0.049 (0.075)	Loss 1.8053 (2.3302)	Prec@1 32.031 (22.748)
Epoch: [0][180/391]	Time 0.047 (0.072)	Loss 1.8321 (2.2775)	Prec@1 25.000 (23.718)
Epoch: [0][200/391]	Time 0.047 (0.069)	Loss 1.8293 (2.2288)	Prec@1 33.594 (24.736)
Epoch: [0][220/391]	Time 0.044 (0.067)	Loss 1.8016 (2.1915)	Prec@1 33.594 (25.357)
Epoch: [0][240/391]	Time 0.048 (0.065)	Loss 1.7413 (2.1560)	Prec@1 35.156 (26.118)
Epoch: [0][260/391]	Time 0.047 (0.064)	Loss 1.6962 (2.1243)	Prec@1 37.500 (26.781)
Epoch: [0][280/391]	Time 0.041 (0.063)	Loss 1.6795 (2.0958)	Prec@1 36.719 (27.488)
Epoch: [0][300/391]	Time 0.047 (0.062)	Loss 1.5898 (2.0670)	Prec@1 39.844 (28.260)
Epoch: [0][320/391]	Time 0.047 (0.061)	Loss 1.6295 (2.0435)	Prec@1 35.156 (28.780)
Epoch: [0][340/391]	Time 0.047 (0.060)	Loss 1.6416 (2.0180)	Prec@1 35.938 (29.477)
Epoch: [0][360/391]	Time 0.050 (0.059)	Loss 1.6048 (1.9981)	Prec@1 45.312 (30.027)
Epoch: [0][380/391]	Time 0.048 (0.059)	Loss 1.4851 (1.9773)	Prec@1 46.875 (30.534)
training time:  22.97460675239563
Test: [0/79]	Time 0.703 (0.703)	Loss 1.5008 (1.5008)	Prec@1 47.656 (47.656)
Test: [20/79]	Time 0.029 (0.066)	Loss 1.5732 (1.5466)	Prec@1 42.969 (43.527)
Test: [40/79]	Time 0.037 (0.051)	Loss 1.5962 (1.5376)	Prec@1 46.875 (43.750)
Test: [60/79]	Time 0.033 (0.045)	Loss 1.4657 (1.5399)	Prec@1 46.875 (43.161)
 * Prec@1 42.790
=> Saving checkpoint for epoch 0, with Prec@1 42.790000.
Epoch: [1][0/391]	Time 0.675 (0.675)	Loss 1.6352 (1.6352)	Prec@1 43.750 (43.750)
Epoch: [1][20/391]	Time 0.047 (0.076)	Loss 1.5345 (1.6010)	Prec@1 39.844 (40.253)
Epoch: [1][40/391]	Time 0.047 (0.062)	Loss 1.5087 (1.5811)	Prec@1 46.875 (40.739)
Epoch: [1][60/391]	Time 0.044 (0.057)	Loss 1.4522 (1.5643)	Prec@1 47.656 (41.291)
Epoch: [1][80/391]	Time 0.046 (0.054)	Loss 1.5938 (1.5582)	Prec@1 41.406 (42.226)
Epoch: [1][100/391]	Time 0.049 (0.053)	Loss 1.5288 (1.5533)	Prec@1 48.438 (42.729)
Epoch: [1][120/391]	Time 0.046 (0.052)	Loss 1.6186 (1.5423)	Prec@1 37.500 (43.111)
Epoch: [1][140/391]	Time 0.046 (0.051)	Loss 1.3327 (1.5345)	Prec@1 52.344 (43.434)
Epoch: [1][160/391]	Time 0.047 (0.050)	Loss 1.6254 (1.5325)	Prec@1 36.719 (43.624)
Epoch: [1][180/391]	Time 0.047 (0.050)	Loss 1.3709 (1.5246)	Prec@1 47.656 (44.009)
Epoch: [1][200/391]	Time 0.041 (0.050)	Loss 1.4213 (1.5174)	Prec@1 48.438 (44.329)
Epoch: [1][220/391]	Time 0.042 (0.050)	Loss 1.4438 (1.5104)	Prec@1 46.094 (44.598)
Epoch: [1][240/391]	Time 0.047 (0.049)	Loss 1.2461 (1.5020)	Prec@1 55.469 (44.836)
Epoch: [1][260/391]	Time 0.048 (0.049)	Loss 1.4675 (1.4967)	Prec@1 50.781 (45.088)
Epoch: [1][280/391]	Time 0.044 (0.049)	Loss 1.2643 (1.4899)	Prec@1 55.469 (45.285)
Epoch: [1][300/391]	Time 0.047 (0.049)	Loss 1.4887 (1.4783)	Prec@1 46.875 (45.775)
Epoch: [1][320/391]	Time 0.044 (0.049)	Loss 1.2751 (1.4679)	Prec@1 53.125 (46.225)
Epoch: [1][340/391]	Time 0.042 (0.049)	Loss 1.2904 (1.4593)	Prec@1 53.906 (46.552)
Epoch: [1][360/391]	Time 0.042 (0.048)	Loss 1.4020 (1.4508)	Prec@1 44.531 (46.830)
Epoch: [1][380/391]	Time 0.043 (0.048)	Loss 1.2891 (1.4443)	Prec@1 53.906 (47.117)
training time:  18.945523738861084
Test: [0/79]	Time 0.703 (0.703)	Loss 1.2225 (1.2225)	Prec@1 58.594 (58.594)
Test: [20/79]	Time 0.032 (0.064)	Loss 1.3259 (1.3063)	Prec@1 53.906 (53.460)
Test: [40/79]	Time 0.032 (0.048)	Loss 1.2536 (1.2976)	Prec@1 50.781 (53.735)
Test: [60/79]	Time 0.031 (0.043)	Loss 1.2312 (1.2995)	Prec@1 57.812 (53.663)
 * Prec@1 53.160
=> Saving checkpoint for epoch 1, with Prec@1 53.160000.
Epoch: [2][0/391]	Time 0.733 (0.733)	Loss 1.3361 (1.3361)	Prec@1 50.000 (50.000)
Epoch: [2][20/391]	Time 0.047 (0.079)	Loss 1.1311 (1.3037)	Prec@1 59.375 (53.311)
Epoch: [2][40/391]	Time 0.049 (0.063)	Loss 1.2399 (1.2927)	Prec@1 57.031 (54.135)
Epoch: [2][60/391]	Time 0.044 (0.058)	Loss 1.5007 (1.2765)	Prec@1 48.438 (54.431)
Epoch: [2][80/391]	Time 0.044 (0.055)	Loss 1.2861 (1.2690)	Prec@1 53.125 (54.610)
Epoch: [2][100/391]	Time 0.047 (0.053)	Loss 1.2580 (1.2673)	Prec@1 52.344 (54.610)
Epoch: [2][120/391]	Time 0.044 (0.052)	Loss 1.1788 (1.2591)	Prec@1 55.469 (54.823)
Epoch: [2][140/391]	Time 0.047 (0.051)	Loss 1.3277 (1.2524)	Prec@1 54.688 (55.003)
Epoch: [2][160/391]	Time 0.046 (0.051)	Loss 0.9375 (1.2428)	Prec@1 67.188 (55.236)
Epoch: [2][180/391]	Time 0.047 (0.050)	Loss 1.1014 (1.2340)	Prec@1 60.156 (55.672)
Epoch: [2][200/391]	Time 0.044 (0.050)	Loss 1.4745 (1.2246)	Prec@1 45.312 (56.098)
Epoch: [2][220/391]	Time 0.047 (0.051)	Loss 1.0326 (1.2176)	Prec@1 64.062 (56.331)
Epoch: [2][240/391]	Time 0.043 (0.050)	Loss 1.0973 (1.2127)	Prec@1 63.281 (56.496)
Epoch: [2][260/391]	Time 0.047 (0.050)	Loss 1.0878 (1.2063)	Prec@1 64.844 (56.831)
Epoch: [2][280/391]	Time 0.047 (0.050)	Loss 1.0114 (1.2032)	Prec@1 62.500 (56.898)
Epoch: [2][300/391]	Time 0.047 (0.049)	Loss 1.0994 (1.1965)	Prec@1 60.938 (57.177)
Epoch: [2][320/391]	Time 0.047 (0.049)	Loss 1.1509 (1.1906)	Prec@1 61.719 (57.401)
Epoch: [2][340/391]	Time 0.047 (0.049)	Loss 1.2700 (1.1865)	Prec@1 58.594 (57.577)
Epoch: [2][360/391]	Time 0.047 (0.049)	Loss 1.0125 (1.1823)	Prec@1 60.938 (57.693)
Epoch: [2][380/391]	Time 0.046 (0.049)	Loss 1.0155 (1.1750)	Prec@1 66.406 (57.993)
training time:  19.151800394058228
Test: [0/79]	Time 0.676 (0.676)	Loss 0.9602 (0.9602)	Prec@1 68.750 (68.750)
Test: [20/79]	Time 0.031 (0.063)	Loss 1.0235 (1.0367)	Prec@1 64.062 (63.207)
Test: [40/79]	Time 0.031 (0.048)	Loss 1.0637 (1.0413)	Prec@1 60.156 (63.548)
Test: [60/79]	Time 0.031 (0.043)	Loss 0.9403 (1.0501)	Prec@1 64.844 (63.345)
 * Prec@1 63.080
=> Saving checkpoint for epoch 2, with Prec@1 63.080000.
Epoch: [3][0/391]	Time 0.711 (0.711)	Loss 1.0254 (1.0254)	Prec@1 62.500 (62.500)
Epoch: [3][20/391]	Time 0.047 (0.078)	Loss 0.9924 (1.0504)	Prec@1 61.719 (62.574)
Epoch: [3][40/391]	Time 0.046 (0.063)	Loss 0.8569 (1.0327)	Prec@1 68.750 (63.491)
Epoch: [3][60/391]	Time 0.048 (0.057)	Loss 0.9112 (1.0350)	Prec@1 64.062 (63.397)
Epoch: [3][80/391]	Time 0.046 (0.055)	Loss 1.1582 (1.0356)	Prec@1 60.156 (63.156)
Epoch: [3][100/391]	Time 0.046 (0.053)	Loss 1.1283 (1.0356)	Prec@1 57.031 (63.026)
Epoch: [3][120/391]	Time 0.046 (0.052)	Loss 0.9826 (1.0306)	Prec@1 61.719 (63.262)
Epoch: [3][140/391]	Time 0.048 (0.051)	Loss 1.0038 (1.0286)	Prec@1 64.844 (63.281)
Epoch: [3][160/391]	Time 0.046 (0.051)	Loss 1.1316 (1.0265)	Prec@1 58.594 (63.320)
Epoch: [3][180/391]	Time 0.047 (0.050)	Loss 0.9667 (1.0219)	Prec@1 62.500 (63.549)
Epoch: [3][200/391]	Time 0.045 (0.050)	Loss 1.0194 (1.0181)	Prec@1 60.156 (63.584)
Epoch: [3][220/391]	Time 0.046 (0.049)	Loss 0.9327 (1.0142)	Prec@1 72.656 (63.812)
Epoch: [3][240/391]	Time 0.045 (0.049)	Loss 0.9813 (1.0142)	Prec@1 61.719 (63.813)
Epoch: [3][260/391]	Time 0.045 (0.049)	Loss 0.8788 (1.0109)	Prec@1 64.062 (63.868)
Epoch: [3][280/391]	Time 0.046 (0.049)	Loss 0.9964 (1.0069)	Prec@1 67.188 (64.021)
Epoch: [3][300/391]	Time 0.046 (0.049)	Loss 1.0853 (1.0049)	Prec@1 64.844 (64.099)
Epoch: [3][320/391]	Time 0.046 (0.049)	Loss 0.7962 (0.9999)	Prec@1 71.094 (64.340)
Epoch: [3][340/391]	Time 0.049 (0.049)	Loss 0.7760 (0.9955)	Prec@1 75.781 (64.541)
Epoch: [3][360/391]	Time 0.047 (0.048)	Loss 0.9776 (0.9953)	Prec@1 67.188 (64.601)
Epoch: [3][380/391]	Time 0.046 (0.048)	Loss 0.8853 (0.9905)	Prec@1 68.750 (64.801)
training time:  18.978386640548706
Test: [0/79]	Time 0.683 (0.683)	Loss 0.8384 (0.8384)	Prec@1 71.094 (71.094)
Test: [20/79]	Time 0.032 (0.062)	Loss 0.9037 (1.0029)	Prec@1 67.188 (64.955)
Test: [40/79]	Time 0.032 (0.048)	Loss 0.9546 (0.9904)	Prec@1 66.406 (65.320)
Test: [60/79]	Time 0.031 (0.043)	Loss 0.9963 (0.9823)	Prec@1 66.406 (65.548)
 * Prec@1 65.360
=> Saving checkpoint for epoch 3, with Prec@1 65.360000.
Epoch: [4][0/391]	Time 0.717 (0.717)	Loss 0.9042 (0.9042)	Prec@1 64.062 (64.062)
Epoch: [4][20/391]	Time 0.047 (0.079)	Loss 0.8495 (0.9136)	Prec@1 71.875 (67.262)
Epoch: [4][40/391]	Time 0.047 (0.063)	Loss 0.9354 (0.9128)	Prec@1 64.062 (67.588)
Epoch: [4][60/391]	Time 0.047 (0.057)	Loss 0.7389 (0.8997)	Prec@1 74.219 (68.058)
Epoch: [4][80/391]	Time 0.047 (0.055)	Loss 0.8733 (0.9071)	Prec@1 67.188 (67.718)
Epoch: [4][100/391]	Time 0.047 (0.053)	Loss 0.9919 (0.9046)	Prec@1 64.062 (68.015)
Epoch: [4][120/391]	Time 0.047 (0.052)	Loss 1.0995 (0.8987)	Prec@1 67.188 (68.343)
Epoch: [4][140/391]	Time 0.048 (0.051)	Loss 0.9966 (0.8957)	Prec@1 65.625 (68.551)
Epoch: [4][160/391]	Time 0.046 (0.051)	Loss 0.9698 (0.8954)	Prec@1 68.750 (68.614)
Epoch: [4][180/391]	Time 0.046 (0.051)	Loss 0.8170 (0.8926)	Prec@1 69.531 (68.595)
Epoch: [4][200/391]	Time 0.046 (0.050)	Loss 0.8768 (0.8886)	Prec@1 67.969 (68.711)
Epoch: [4][220/391]	Time 0.049 (0.050)	Loss 0.7751 (0.8862)	Prec@1 72.656 (68.792)
Epoch: [4][240/391]	Time 0.046 (0.050)	Loss 0.7582 (0.8802)	Prec@1 69.531 (68.980)
Epoch: [4][260/391]	Time 0.049 (0.049)	Loss 0.8349 (0.8802)	Prec@1 71.094 (68.960)
Epoch: [4][280/391]	Time 0.046 (0.049)	Loss 0.8319 (0.8777)	Prec@1 71.875 (69.056)
Epoch: [4][300/391]	Time 0.046 (0.049)	Loss 0.8064 (0.8756)	Prec@1 67.969 (69.173)
Epoch: [4][320/391]	Time 0.045 (0.049)	Loss 0.8047 (0.8740)	Prec@1 71.875 (69.259)
Epoch: [4][340/391]	Time 0.046 (0.049)	Loss 0.6799 (0.8731)	Prec@1 76.562 (69.275)
Epoch: [4][360/391]	Time 0.046 (0.048)	Loss 0.8603 (0.8726)	Prec@1 66.406 (69.226)
Epoch: [4][380/391]	Time 0.046 (0.048)	Loss 0.8827 (0.8706)	Prec@1 73.438 (69.316)
training time:  19.01938223838806
Test: [0/79]	Time 0.713 (0.713)	Loss 0.8944 (0.8944)	Prec@1 66.406 (66.406)
Test: [20/79]	Time 0.030 (0.064)	Loss 0.8541 (0.9016)	Prec@1 69.531 (69.159)
Test: [40/79]	Time 0.032 (0.048)	Loss 0.7948 (0.8891)	Prec@1 73.438 (69.646)
Test: [60/79]	Time 0.032 (0.042)	Loss 0.7858 (0.8904)	Prec@1 74.219 (69.467)
 * Prec@1 68.990
=> Saving checkpoint for epoch 4, with Prec@1 68.990000.
Epoch: [5][0/391]	Time 0.709 (0.709)	Loss 0.8222 (0.8222)	Prec@1 70.312 (70.312)
Epoch: [5][20/391]	Time 0.046 (0.076)	Loss 0.7609 (0.8005)	Prec@1 70.312 (72.545)
Epoch: [5][40/391]	Time 0.048 (0.062)	Loss 0.7718 (0.7763)	Prec@1 69.531 (73.095)
Epoch: [5][60/391]	Time 0.047 (0.057)	Loss 0.8318 (0.7749)	Prec@1 71.094 (72.989)
Epoch: [5][80/391]	Time 0.047 (0.055)	Loss 0.8332 (0.7853)	Prec@1 73.438 (72.512)
Epoch: [5][100/391]	Time 0.046 (0.053)	Loss 0.9074 (0.7873)	Prec@1 69.531 (72.633)
Epoch: [5][120/391]	Time 0.046 (0.052)	Loss 0.7229 (0.7856)	Prec@1 73.438 (72.572)
Epoch: [5][140/391]	Time 0.047 (0.051)	Loss 0.9435 (0.7818)	Prec@1 71.875 (72.678)
Epoch: [5][160/391]	Time 0.047 (0.051)	Loss 0.7005 (0.7805)	Prec@1 76.562 (72.768)
Epoch: [5][180/391]	Time 0.045 (0.050)	Loss 0.7807 (0.7806)	Prec@1 72.656 (72.764)
Epoch: [5][200/391]	Time 0.046 (0.050)	Loss 0.8805 (0.7764)	Prec@1 65.625 (72.901)
Epoch: [5][220/391]	Time 0.046 (0.049)	Loss 0.6941 (0.7745)	Prec@1 75.781 (72.907)
Epoch: [5][240/391]	Time 0.045 (0.049)	Loss 0.6175 (0.7720)	Prec@1 79.688 (72.964)
Epoch: [5][260/391]	Time 0.046 (0.049)	Loss 0.6160 (0.7671)	Prec@1 75.781 (73.171)
Epoch: [5][280/391]	Time 0.046 (0.049)	Loss 0.7121 (0.7659)	Prec@1 73.438 (73.193)
Epoch: [5][300/391]	Time 0.047 (0.049)	Loss 0.8381 (0.7652)	Prec@1 67.969 (73.204)
Epoch: [5][320/391]	Time 0.044 (0.049)	Loss 0.7578 (0.7648)	Prec@1 70.312 (73.187)
Epoch: [5][340/391]	Time 0.046 (0.048)	Loss 0.6246 (0.7623)	Prec@1 78.906 (73.302)
Epoch: [5][360/391]	Time 0.046 (0.048)	Loss 0.7069 (0.7582)	Prec@1 75.000 (73.427)
Epoch: [5][380/391]	Time 0.046 (0.048)	Loss 0.6024 (0.7551)	Prec@1 79.688 (73.528)
training time:  18.863583087921143
Test: [0/79]	Time 0.685 (0.685)	Loss 0.7296 (0.7296)	Prec@1 73.438 (73.438)
Test: [20/79]	Time 0.032 (0.063)	Loss 0.6046 (0.7521)	Prec@1 77.344 (73.177)
Test: [40/79]	Time 0.032 (0.048)	Loss 0.7502 (0.7532)	Prec@1 71.875 (73.190)
Test: [60/79]	Time 0.031 (0.043)	Loss 0.7229 (0.7554)	Prec@1 75.781 (73.040)
 * Prec@1 72.700
=> Saving checkpoint for epoch 5, with Prec@1 72.700000.
Epoch: [6][0/391]	Time 0.720 (0.720)	Loss 0.7079 (0.7079)	Prec@1 74.219 (74.219)
Epoch: [6][20/391]	Time 0.046 (0.078)	Loss 0.6113 (0.6711)	Prec@1 79.688 (76.972)
Epoch: [6][40/391]	Time 0.047 (0.062)	Loss 0.7170 (0.6688)	Prec@1 71.875 (76.620)
Epoch: [6][60/391]	Time 0.046 (0.057)	Loss 0.6055 (0.6643)	Prec@1 76.562 (76.819)
Epoch: [6][80/391]	Time 0.047 (0.054)	Loss 0.7589 (0.6718)	Prec@1 75.781 (76.485)
Epoch: [6][100/391]	Time 0.047 (0.053)	Loss 0.7403 (0.6775)	Prec@1 75.000 (76.385)
Epoch: [6][120/391]	Time 0.046 (0.051)	Loss 0.7072 (0.6792)	Prec@1 76.562 (76.278)
Epoch: [6][140/391]	Time 0.048 (0.051)	Loss 0.7565 (0.6763)	Prec@1 76.562 (76.285)
Epoch: [6][160/391]	Time 0.045 (0.050)	Loss 0.6246 (0.6771)	Prec@1 82.031 (76.325)
Epoch: [6][180/391]	Time 0.046 (0.050)	Loss 0.6407 (0.6759)	Prec@1 78.125 (76.390)
Epoch: [6][200/391]	Time 0.045 (0.050)	Loss 0.9138 (0.6762)	Prec@1 68.750 (76.496)
Epoch: [6][220/391]	Time 0.045 (0.049)	Loss 0.6376 (0.6765)	Prec@1 78.125 (76.492)
Epoch: [6][240/391]	Time 0.044 (0.049)	Loss 0.9031 (0.6750)	Prec@1 65.625 (76.569)
Epoch: [6][260/391]	Time 0.046 (0.049)	Loss 0.6583 (0.6748)	Prec@1 75.000 (76.548)
Epoch: [6][280/391]	Time 0.046 (0.049)	Loss 0.4162 (0.6720)	Prec@1 85.156 (76.599)
Epoch: [6][300/391]	Time 0.046 (0.048)	Loss 0.5673 (0.6715)	Prec@1 83.594 (76.640)
Epoch: [6][320/391]	Time 0.045 (0.048)	Loss 0.5541 (0.6675)	Prec@1 78.125 (76.762)
Epoch: [6][340/391]	Time 0.046 (0.048)	Loss 0.7436 (0.6681)	Prec@1 71.094 (76.730)
Epoch: [6][360/391]	Time 0.047 (0.048)	Loss 0.6878 (0.6660)	Prec@1 76.562 (76.805)
Epoch: [6][380/391]	Time 0.046 (0.048)	Loss 0.5801 (0.6639)	Prec@1 79.688 (76.905)
training time:  18.878127098083496
Test: [0/79]	Time 0.704 (0.704)	Loss 0.9191 (0.9191)	Prec@1 70.312 (70.312)
Test: [20/79]	Time 0.032 (0.064)	Loss 0.7190 (0.7531)	Prec@1 73.438 (73.624)
Test: [40/79]	Time 0.032 (0.048)	Loss 0.7131 (0.7402)	Prec@1 75.781 (74.219)
Test: [60/79]	Time 0.032 (0.043)	Loss 0.7063 (0.7389)	Prec@1 75.781 (74.257)
 * Prec@1 74.070
=> Saving checkpoint for epoch 6, with Prec@1 74.070000.
Epoch: [7][0/391]	Time 0.749 (0.749)	Loss 0.6317 (0.6317)	Prec@1 75.781 (75.781)
Epoch: [7][20/391]	Time 0.049 (0.079)	Loss 0.5886 (0.6090)	Prec@1 78.906 (79.055)
Epoch: [7][40/391]	Time 0.045 (0.063)	Loss 0.6385 (0.5997)	Prec@1 77.344 (79.325)
Epoch: [7][60/391]	Time 0.042 (0.057)	Loss 0.6349 (0.5999)	Prec@1 78.125 (79.188)
Epoch: [7][80/391]	Time 0.042 (0.054)	Loss 0.6071 (0.6053)	Prec@1 80.469 (78.887)
Epoch: [7][100/391]	Time 0.044 (0.052)	Loss 0.6241 (0.6071)	Prec@1 80.469 (78.945)
Epoch: [7][120/391]	Time 0.043 (0.051)	Loss 0.7075 (0.6072)	Prec@1 78.125 (78.964)
Epoch: [7][140/391]	Time 0.043 (0.050)	Loss 0.8450 (0.6069)	Prec@1 68.750 (78.906)
Epoch: [7][160/391]	Time 0.043 (0.050)	Loss 0.6244 (0.6057)	Prec@1 78.906 (78.989)
Epoch: [7][180/391]	Time 0.045 (0.049)	Loss 0.5286 (0.6015)	Prec@1 80.469 (79.200)
Epoch: [7][200/391]	Time 0.046 (0.049)	Loss 0.5371 (0.6028)	Prec@1 81.250 (79.167)
Epoch: [7][220/391]	Time 0.043 (0.049)	Loss 0.5106 (0.6032)	Prec@1 83.594 (79.118)
Epoch: [7][240/391]	Time 0.045 (0.048)	Loss 0.5951 (0.6016)	Prec@1 79.688 (79.211)
Epoch: [7][260/391]	Time 0.042 (0.048)	Loss 0.5503 (0.6010)	Prec@1 82.812 (79.176)
Epoch: [7][280/391]	Time 0.042 (0.048)	Loss 0.4806 (0.6005)	Prec@1 85.156 (79.209)
Epoch: [7][300/391]	Time 0.042 (0.048)	Loss 0.5840 (0.5980)	Prec@1 82.812 (79.275)
Epoch: [7][320/391]	Time 0.044 (0.047)	Loss 0.4773 (0.5955)	Prec@1 81.250 (79.342)
Epoch: [7][340/391]	Time 0.042 (0.047)	Loss 0.5347 (0.5942)	Prec@1 80.469 (79.394)
Epoch: [7][360/391]	Time 0.043 (0.047)	Loss 0.6612 (0.5953)	Prec@1 78.906 (79.369)
Epoch: [7][380/391]	Time 0.046 (0.047)	Loss 0.4563 (0.5940)	Prec@1 84.375 (79.388)
training time:  18.50515913963318
Test: [0/79]	Time 0.730 (0.730)	Loss 0.4960 (0.4960)	Prec@1 85.156 (85.156)
Test: [20/79]	Time 0.032 (0.065)	Loss 0.5339 (0.6529)	Prec@1 83.594 (78.088)
Test: [40/79]	Time 0.032 (0.049)	Loss 0.6447 (0.6472)	Prec@1 77.344 (78.201)
Test: [60/79]	Time 0.032 (0.043)	Loss 0.7979 (0.6470)	Prec@1 77.344 (78.151)
 * Prec@1 78.350
=> Saving checkpoint for epoch 7, with Prec@1 78.350000.
Epoch: [8][0/391]	Time 0.707 (0.707)	Loss 0.7436 (0.7436)	Prec@1 75.781 (75.781)
Epoch: [8][20/391]	Time 0.046 (0.078)	Loss 0.6883 (0.6078)	Prec@1 77.344 (78.683)
Epoch: [8][40/391]	Time 0.047 (0.062)	Loss 0.5944 (0.5864)	Prec@1 80.469 (79.802)
Epoch: [8][60/391]	Time 0.047 (0.057)	Loss 0.5613 (0.5719)	Prec@1 80.469 (80.213)
Epoch: [8][80/391]	Time 0.048 (0.055)	Loss 0.5805 (0.5675)	Prec@1 77.344 (80.421)
Epoch: [8][100/391]	Time 0.047 (0.053)	Loss 0.6001 (0.5679)	Prec@1 78.906 (80.360)
Epoch: [8][120/391]	Time 0.047 (0.052)	Loss 0.4320 (0.5640)	Prec@1 88.281 (80.527)
Epoch: [8][140/391]	Time 0.046 (0.051)	Loss 0.5626 (0.5600)	Prec@1 79.688 (80.690)
Epoch: [8][160/391]	Time 0.047 (0.050)	Loss 0.5942 (0.5588)	Prec@1 80.469 (80.745)
Epoch: [8][180/391]	Time 0.181 (0.051)	Loss 0.5922 (0.5590)	Prec@1 79.688 (80.719)
Epoch: [8][200/391]	Time 0.047 (0.050)	Loss 0.5088 (0.5568)	Prec@1 83.594 (80.822)
Epoch: [8][220/391]	Time 0.049 (0.050)	Loss 0.6550 (0.5554)	Prec@1 77.344 (80.829)
Epoch: [8][240/391]	Time 0.043 (0.050)	Loss 0.4478 (0.5572)	Prec@1 83.594 (80.738)
Epoch: [8][260/391]	Time 0.047 (0.050)	Loss 0.4661 (0.5560)	Prec@1 82.031 (80.810)
Epoch: [8][280/391]	Time 0.048 (0.049)	Loss 0.4279 (0.5552)	Prec@1 82.812 (80.808)
Epoch: [8][300/391]	Time 0.047 (0.049)	Loss 0.5296 (0.5545)	Prec@1 78.125 (80.817)
Epoch: [8][320/391]	Time 0.045 (0.049)	Loss 0.6544 (0.5538)	Prec@1 73.438 (80.819)
Epoch: [8][340/391]	Time 0.047 (0.049)	Loss 0.4765 (0.5527)	Prec@1 88.281 (80.893)
Epoch: [8][360/391]	Time 0.045 (0.049)	Loss 0.5324 (0.5522)	Prec@1 81.250 (80.910)
Epoch: [8][380/391]	Time 0.045 (0.049)	Loss 0.7423 (0.5524)	Prec@1 75.781 (80.920)
training time:  19.117384910583496
Test: [0/79]	Time 0.676 (0.676)	Loss 0.7088 (0.7088)	Prec@1 76.562 (76.562)
Test: [20/79]	Time 0.032 (0.061)	Loss 0.5432 (0.6223)	Prec@1 80.469 (78.906)
Test: [40/79]	Time 0.028 (0.046)	Loss 0.5966 (0.6114)	Prec@1 77.344 (78.906)
Test: [60/79]	Time 0.032 (0.042)	Loss 0.5328 (0.6109)	Prec@1 80.469 (78.881)
 * Prec@1 78.870
=> Saving checkpoint for epoch 8, with Prec@1 78.870000.
Epoch: [9][0/391]	Time 0.774 (0.774)	Loss 0.4478 (0.4478)	Prec@1 79.688 (79.688)
Epoch: [9][20/391]	Time 0.046 (0.081)	Loss 0.5460 (0.5165)	Prec@1 84.375 (82.143)
Epoch: [9][40/391]	Time 0.046 (0.064)	Loss 0.5362 (0.5186)	Prec@1 82.031 (82.336)
Epoch: [9][60/391]	Time 0.046 (0.058)	Loss 0.6300 (0.5129)	Prec@1 76.562 (82.518)
Epoch: [9][80/391]	Time 0.047 (0.055)	Loss 0.6548 (0.5168)	Prec@1 77.344 (82.485)
Epoch: [9][100/391]	Time 0.046 (0.054)	Loss 0.4678 (0.5158)	Prec@1 81.250 (82.472)
Epoch: [9][120/391]	Time 0.047 (0.052)	Loss 0.5029 (0.5168)	Prec@1 79.688 (82.451)
Epoch: [9][140/391]	Time 0.047 (0.052)	Loss 0.4574 (0.5129)	Prec@1 82.812 (82.436)
Epoch: [9][160/391]	Time 0.046 (0.051)	Loss 0.4937 (0.5157)	Prec@1 82.812 (82.405)
Epoch: [9][180/391]	Time 0.048 (0.050)	Loss 0.4846 (0.5150)	Prec@1 84.375 (82.273)
Epoch: [9][200/391]	Time 0.048 (0.050)	Loss 0.5254 (0.5142)	Prec@1 83.594 (82.261)
Epoch: [9][220/391]	Time 0.047 (0.050)	Loss 0.6451 (0.5187)	Prec@1 82.031 (82.137)
Epoch: [9][240/391]	Time 0.047 (0.050)	Loss 0.4342 (0.5186)	Prec@1 85.156 (82.132)
Epoch: [9][260/391]	Time 0.046 (0.049)	Loss 0.5485 (0.5169)	Prec@1 82.031 (82.196)
Epoch: [9][280/391]	Time 0.047 (0.049)	Loss 0.5205 (0.5155)	Prec@1 80.469 (82.256)
Epoch: [9][300/391]	Time 0.046 (0.049)	Loss 0.6094 (0.5151)	Prec@1 78.906 (82.265)
Epoch: [9][320/391]	Time 0.047 (0.049)	Loss 0.6078 (0.5153)	Prec@1 78.906 (82.216)
Epoch: [9][340/391]	Time 0.046 (0.049)	Loss 0.4757 (0.5162)	Prec@1 86.719 (82.219)
Epoch: [9][360/391]	Time 0.050 (0.049)	Loss 0.4583 (0.5171)	Prec@1 86.719 (82.133)
Epoch: [9][380/391]	Time 0.047 (0.048)	Loss 0.6046 (0.5185)	Prec@1 80.469 (82.099)
training time:  19.090665102005005
Test: [0/79]	Time 0.728 (0.728)	Loss 0.4593 (0.4593)	Prec@1 82.812 (82.812)
Test: [20/79]	Time 0.032 (0.065)	Loss 0.4824 (0.5274)	Prec@1 82.812 (82.552)
Test: [40/79]	Time 0.031 (0.049)	Loss 0.6095 (0.5286)	Prec@1 80.469 (81.898)
Test: [60/79]	Time 0.030 (0.043)	Loss 0.5079 (0.5276)	Prec@1 76.562 (82.018)
 * Prec@1 81.970
=> Saving checkpoint for epoch 9, with Prec@1 81.970000.
Epoch: [10][0/391]	Time 0.719 (0.719)	Loss 0.5629 (0.5629)	Prec@1 82.031 (82.031)
Epoch: [10][20/391]	Time 0.049 (0.079)	Loss 0.4434 (0.4768)	Prec@1 85.938 (84.375)
Epoch: [10][40/391]	Time 0.046 (0.064)	Loss 0.4534 (0.5033)	Prec@1 84.375 (83.518)
Epoch: [10][60/391]	Time 0.042 (0.058)	Loss 0.6005 (0.4983)	Prec@1 81.250 (83.427)
Epoch: [10][80/391]	Time 0.046 (0.055)	Loss 0.4355 (0.5031)	Prec@1 84.375 (83.314)
Epoch: [10][100/391]	Time 0.042 (0.054)	Loss 0.4910 (0.5001)	Prec@1 82.812 (83.184)
Epoch: [10][120/391]	Time 0.045 (0.052)	Loss 0.5735 (0.4984)	Prec@1 79.688 (83.084)
Epoch: [10][140/391]	Time 0.047 (0.051)	Loss 0.5731 (0.4985)	Prec@1 83.594 (83.195)
Epoch: [10][160/391]	Time 0.046 (0.051)	Loss 0.5372 (0.4985)	Prec@1 83.594 (83.147)
Epoch: [10][180/391]	Time 0.191 (0.051)	Loss 0.3712 (0.4956)	Prec@1 85.938 (83.197)
Epoch: [10][200/391]	Time 0.045 (0.051)	Loss 0.4900 (0.4951)	Prec@1 80.469 (83.170)
Epoch: [10][220/391]	Time 0.047 (0.050)	Loss 0.4083 (0.4947)	Prec@1 89.062 (83.198)
Epoch: [10][240/391]	Time 0.044 (0.050)	Loss 0.4158 (0.4938)	Prec@1 85.156 (83.182)
Epoch: [10][260/391]	Time 0.047 (0.050)	Loss 0.5714 (0.4934)	Prec@1 82.031 (83.214)
Epoch: [10][280/391]	Time 0.047 (0.049)	Loss 0.4978 (0.4944)	Prec@1 84.375 (83.149)
Epoch: [10][300/391]	Time 0.045 (0.049)	Loss 0.3820 (0.4933)	Prec@1 88.281 (83.197)
Epoch: [10][320/391]	Time 0.046 (0.049)	Loss 0.6506 (0.4938)	Prec@1 79.688 (83.143)
Epoch: [10][340/391]	Time 0.044 (0.049)	Loss 0.4963 (0.4938)	Prec@1 83.594 (83.131)
Epoch: [10][360/391]	Time 0.046 (0.049)	Loss 0.6983 (0.4943)	Prec@1 77.344 (83.150)
Epoch: [10][380/391]	Time 0.047 (0.049)	Loss 0.5370 (0.4955)	Prec@1 83.594 (83.100)
training time:  19.149598360061646
Test: [0/79]	Time 0.731 (0.731)	Loss 0.4467 (0.4467)	Prec@1 84.375 (84.375)
Test: [20/79]	Time 0.032 (0.065)	Loss 0.4630 (0.4980)	Prec@1 81.250 (83.147)
Test: [40/79]	Time 0.032 (0.049)	Loss 0.4711 (0.4992)	Prec@1 82.812 (82.832)
Test: [60/79]	Time 0.032 (0.044)	Loss 0.4679 (0.4989)	Prec@1 82.031 (82.889)
 * Prec@1 82.500
=> Saving checkpoint for epoch 10, with Prec@1 82.500000.
Epoch: [11][0/391]	Time 0.724 (0.724)	Loss 0.4643 (0.4643)	Prec@1 83.594 (83.594)
Epoch: [11][20/391]	Time 0.047 (0.079)	Loss 0.6101 (0.4612)	Prec@1 79.688 (83.408)
Epoch: [11][40/391]	Time 0.047 (0.063)	Loss 0.4371 (0.4658)	Prec@1 81.250 (83.422)
Epoch: [11][60/391]	Time 0.047 (0.058)	Loss 0.3819 (0.4629)	Prec@1 85.938 (84.093)
Epoch: [11][80/391]	Time 0.048 (0.055)	Loss 0.5894 (0.4657)	Prec@1 80.469 (83.864)
Epoch: [11][100/391]	Time 0.048 (0.053)	Loss 0.3015 (0.4720)	Prec@1 88.281 (83.578)
Epoch: [11][120/391]	Time 0.047 (0.052)	Loss 0.4596 (0.4703)	Prec@1 88.281 (83.678)
Epoch: [11][140/391]	Time 0.044 (0.051)	Loss 0.4925 (0.4706)	Prec@1 80.469 (83.644)
Epoch: [11][160/391]	Time 0.045 (0.051)	Loss 0.6158 (0.4756)	Prec@1 78.125 (83.550)
Epoch: [11][180/391]	Time 0.046 (0.050)	Loss 0.5484 (0.4733)	Prec@1 82.812 (83.671)
Epoch: [11][200/391]	Time 0.043 (0.050)	Loss 0.5572 (0.4746)	Prec@1 78.125 (83.586)
Epoch: [11][220/391]	Time 0.041 (0.049)	Loss 0.4627 (0.4737)	Prec@1 87.500 (83.622)
Epoch: [11][240/391]	Time 0.043 (0.049)	Loss 0.6251 (0.4734)	Prec@1 79.688 (83.678)
Epoch: [11][260/391]	Time 0.042 (0.049)	Loss 0.4753 (0.4747)	Prec@1 84.375 (83.642)
Epoch: [11][280/391]	Time 0.043 (0.048)	Loss 0.4107 (0.4758)	Prec@1 85.156 (83.605)
Epoch: [11][300/391]	Time 0.043 (0.048)	Loss 0.4743 (0.4760)	Prec@1 83.594 (83.586)
Epoch: [11][320/391]	Time 0.044 (0.048)	Loss 0.4933 (0.4739)	Prec@1 82.812 (83.674)
Epoch: [11][340/391]	Time 0.043 (0.047)	Loss 0.5469 (0.4756)	Prec@1 82.812 (83.566)
Epoch: [11][360/391]	Time 0.044 (0.047)	Loss 0.4367 (0.4751)	Prec@1 88.281 (83.587)
Epoch: [11][380/391]	Time 0.043 (0.047)	Loss 0.5698 (0.4736)	Prec@1 80.469 (83.647)
training time:  18.476203680038452
Test: [0/79]	Time 0.683 (0.683)	Loss 0.5204 (0.5204)	Prec@1 82.031 (82.031)
Test: [20/79]	Time 0.032 (0.063)	Loss 0.5167 (0.5236)	Prec@1 81.250 (81.771)
Test: [40/79]	Time 0.032 (0.048)	Loss 0.5374 (0.5144)	Prec@1 82.812 (82.146)
Test: [60/79]	Time 0.031 (0.043)	Loss 0.5574 (0.5150)	Prec@1 80.469 (82.044)
 * Prec@1 82.100
Epoch: [12][0/391]	Time 0.777 (0.777)	Loss 0.5346 (0.5346)	Prec@1 83.594 (83.594)
Epoch: [12][20/391]	Time 0.049 (0.079)	Loss 0.3783 (0.4696)	Prec@1 88.281 (84.003)
Epoch: [12][40/391]	Time 0.044 (0.062)	Loss 0.4849 (0.4472)	Prec@1 82.031 (84.718)
Epoch: [12][60/391]	Time 0.046 (0.056)	Loss 0.5223 (0.4475)	Prec@1 85.156 (84.759)
Epoch: [12][80/391]	Time 0.048 (0.053)	Loss 0.4285 (0.4484)	Prec@1 82.031 (84.626)
Epoch: [12][100/391]	Time 0.044 (0.051)	Loss 0.4484 (0.4534)	Prec@1 82.812 (84.445)
Epoch: [12][120/391]	Time 0.048 (0.050)	Loss 0.4366 (0.4504)	Prec@1 86.719 (84.620)
Epoch: [12][140/391]	Time 0.047 (0.050)	Loss 0.5204 (0.4507)	Prec@1 78.906 (84.497)
Epoch: [12][160/391]	Time 0.050 (0.049)	Loss 0.6536 (0.4501)	Prec@1 79.688 (84.501)
Epoch: [12][180/391]	Time 0.050 (0.050)	Loss 0.4905 (0.4553)	Prec@1 80.469 (84.315)
Epoch: [12][200/391]	Time 0.047 (0.049)	Loss 0.4044 (0.4538)	Prec@1 85.156 (84.352)
Epoch: [12][220/391]	Time 0.047 (0.049)	Loss 0.5196 (0.4517)	Prec@1 84.375 (84.368)
Epoch: [12][240/391]	Time 0.044 (0.049)	Loss 0.5303 (0.4516)	Prec@1 79.688 (84.356)
Epoch: [12][260/391]	Time 0.041 (0.049)	Loss 0.3600 (0.4554)	Prec@1 85.938 (84.273)
Epoch: [12][280/391]	Time 0.047 (0.049)	Loss 0.4030 (0.4544)	Prec@1 85.156 (84.333)
Epoch: [12][300/391]	Time 0.044 (0.048)	Loss 0.5216 (0.4555)	Prec@1 78.125 (84.258)
Epoch: [12][320/391]	Time 0.043 (0.048)	Loss 0.4311 (0.4551)	Prec@1 86.719 (84.280)
Epoch: [12][340/391]	Time 0.043 (0.048)	Loss 0.4609 (0.4548)	Prec@1 82.031 (84.293)
Epoch: [12][360/391]	Time 0.045 (0.048)	Loss 0.3906 (0.4534)	Prec@1 86.719 (84.358)
Epoch: [12][380/391]	Time 0.045 (0.048)	Loss 0.4948 (0.4562)	Prec@1 82.812 (84.272)
training time:  18.91791844367981
Test: [0/79]	Time 0.733 (0.733)	Loss 0.4878 (0.4878)	Prec@1 82.812 (82.812)
Test: [20/79]	Time 0.032 (0.063)	Loss 0.5537 (0.5789)	Prec@1 83.594 (80.097)
Test: [40/79]	Time 0.029 (0.047)	Loss 0.5361 (0.5619)	Prec@1 83.594 (80.393)
Test: [60/79]	Time 0.032 (0.042)	Loss 0.5725 (0.5750)	Prec@1 81.250 (79.867)
 * Prec@1 79.900
Epoch: [13][0/391]	Time 0.769 (0.769)	Loss 0.4404 (0.4404)	Prec@1 82.031 (82.031)
Epoch: [13][20/391]	Time 0.045 (0.081)	Loss 0.3479 (0.4341)	Prec@1 89.844 (84.859)
Epoch: [13][40/391]	Time 0.046 (0.064)	Loss 0.4796 (0.4277)	Prec@1 83.594 (84.928)
Epoch: [13][60/391]	Time 0.048 (0.058)	Loss 0.4368 (0.4282)	Prec@1 84.375 (85.182)
Epoch: [13][80/391]	Time 0.045 (0.055)	Loss 0.4310 (0.4295)	Prec@1 85.938 (85.041)
Epoch: [13][100/391]	Time 0.044 (0.053)	Loss 0.4759 (0.4344)	Prec@1 84.375 (84.723)
Epoch: [13][120/391]	Time 0.046 (0.052)	Loss 0.3610 (0.4321)	Prec@1 88.281 (84.872)
Epoch: [13][140/391]	Time 0.045 (0.051)	Loss 0.4375 (0.4340)	Prec@1 82.812 (84.802)
Epoch: [13][160/391]	Time 0.045 (0.050)	Loss 0.4077 (0.4351)	Prec@1 86.719 (84.753)
Epoch: [13][180/391]	Time 0.045 (0.050)	Loss 0.4279 (0.4371)	Prec@1 85.938 (84.720)
Epoch: [13][200/391]	Time 0.045 (0.049)	Loss 0.3365 (0.4364)	Prec@1 87.500 (84.690)
Epoch: [13][220/391]	Time 0.046 (0.049)	Loss 0.4316 (0.4378)	Prec@1 83.594 (84.683)
Epoch: [13][240/391]	Time 0.047 (0.049)	Loss 0.4518 (0.4404)	Prec@1 83.594 (84.563)
Epoch: [13][260/391]	Time 0.045 (0.049)	Loss 0.2894 (0.4401)	Prec@1 89.062 (84.641)
Epoch: [13][280/391]	Time 0.045 (0.049)	Loss 0.5062 (0.4420)	Prec@1 82.031 (84.611)
Epoch: [13][300/391]	Time 0.047 (0.048)	Loss 0.3912 (0.4409)	Prec@1 86.719 (84.650)
Epoch: [13][320/391]	Time 0.048 (0.048)	Loss 0.6061 (0.4411)	Prec@1 74.219 (84.633)
Epoch: [13][340/391]	Time 0.047 (0.048)	Loss 0.5297 (0.4422)	Prec@1 80.469 (84.579)
Epoch: [13][360/391]	Time 0.046 (0.048)	Loss 0.4558 (0.4427)	Prec@1 83.594 (84.563)
Epoch: [13][380/391]	Time 0.045 (0.048)	Loss 0.3103 (0.4436)	Prec@1 88.281 (84.588)
training time:  18.889320611953735
Test: [0/79]	Time 0.722 (0.722)	Loss 0.6616 (0.6616)	Prec@1 78.906 (78.906)
Test: [20/79]	Time 0.032 (0.065)	Loss 0.6799 (0.5790)	Prec@1 79.688 (81.064)
Test: [40/79]	Time 0.032 (0.049)	Loss 0.6201 (0.5584)	Prec@1 78.125 (81.669)
Test: [60/79]	Time 0.032 (0.043)	Loss 0.5831 (0.5550)	Prec@1 80.469 (81.365)
 * Prec@1 81.330
Epoch: [14][0/391]	Time 0.765 (0.765)	Loss 0.5085 (0.5085)	Prec@1 80.469 (80.469)
Epoch: [14][20/391]	Time 0.046 (0.079)	Loss 0.3326 (0.4083)	Prec@1 89.062 (86.124)
Epoch: [14][40/391]	Time 0.042 (0.063)	Loss 0.4621 (0.4172)	Prec@1 83.594 (85.747)
Epoch: [14][60/391]	Time 0.046 (0.057)	Loss 0.6390 (0.4196)	Prec@1 76.562 (85.566)
Epoch: [14][80/391]	Time 0.043 (0.054)	Loss 0.5005 (0.4157)	Prec@1 85.156 (85.764)
Epoch: [14][100/391]	Time 0.043 (0.052)	Loss 0.3206 (0.4163)	Prec@1 87.500 (85.667)
Epoch: [14][120/391]	Time 0.046 (0.051)	Loss 0.4186 (0.4150)	Prec@1 87.500 (85.757)
Epoch: [14][140/391]	Time 0.046 (0.050)	Loss 0.4937 (0.4213)	Prec@1 84.375 (85.572)
Epoch: [14][160/391]	Time 0.046 (0.049)	Loss 0.4493 (0.4208)	Prec@1 88.281 (85.603)
Epoch: [14][180/391]	Time 0.047 (0.050)	Loss 0.3487 (0.4185)	Prec@1 85.156 (85.674)
Epoch: [14][200/391]	Time 0.046 (0.049)	Loss 0.4444 (0.4185)	Prec@1 84.375 (85.595)
Epoch: [14][220/391]	Time 0.047 (0.049)	Loss 0.4945 (0.4211)	Prec@1 80.469 (85.517)
Epoch: [14][240/391]	Time 0.046 (0.049)	Loss 0.5443 (0.4238)	Prec@1 82.812 (85.432)
Epoch: [14][260/391]	Time 0.046 (0.049)	Loss 0.4352 (0.4263)	Prec@1 83.594 (85.333)
Epoch: [14][280/391]	Time 0.046 (0.049)	Loss 0.4736 (0.4273)	Prec@1 84.375 (85.320)
Epoch: [14][300/391]	Time 0.045 (0.048)	Loss 0.4895 (0.4244)	Prec@1 82.812 (85.434)
Epoch: [14][320/391]	Time 0.046 (0.048)	Loss 0.4875 (0.4250)	Prec@1 82.031 (85.448)
Epoch: [14][340/391]	Time 0.046 (0.048)	Loss 0.4356 (0.4251)	Prec@1 85.938 (85.424)
Epoch: [14][360/391]	Time 0.046 (0.048)	Loss 0.4179 (0.4252)	Prec@1 86.719 (85.414)
Epoch: [14][380/391]	Time 0.045 (0.048)	Loss 0.4234 (0.4278)	Prec@1 84.375 (85.339)
training time:  18.900967836380005
Test: [0/79]	Time 0.736 (0.736)	Loss 0.5802 (0.5802)	Prec@1 83.594 (83.594)
Test: [20/79]	Time 0.031 (0.066)	Loss 0.6379 (0.5932)	Prec@1 75.781 (80.432)
Test: [40/79]	Time 0.031 (0.049)	Loss 0.6644 (0.5822)	Prec@1 78.906 (80.831)
Test: [60/79]	Time 0.032 (0.044)	Loss 0.4051 (0.5783)	Prec@1 90.625 (80.674)
 * Prec@1 80.950
Epoch: [15][0/391]	Time 0.730 (0.730)	Loss 0.3928 (0.3928)	Prec@1 84.375 (84.375)
Epoch: [15][20/391]	Time 0.046 (0.079)	Loss 0.3654 (0.4252)	Prec@1 85.156 (85.751)
Epoch: [15][40/391]	Time 0.046 (0.063)	Loss 0.4182 (0.4045)	Prec@1 84.375 (86.090)
Epoch: [15][60/391]	Time 0.046 (0.058)	Loss 0.4182 (0.3985)	Prec@1 85.156 (86.360)
Epoch: [15][80/391]	Time 0.047 (0.055)	Loss 0.3350 (0.4006)	Prec@1 88.281 (86.101)
Epoch: [15][100/391]	Time 0.046 (0.053)	Loss 0.5416 (0.4083)	Prec@1 82.031 (85.968)
Epoch: [15][120/391]	Time 0.047 (0.052)	Loss 0.3297 (0.4100)	Prec@1 86.719 (85.802)
Epoch: [15][140/391]	Time 0.045 (0.052)	Loss 0.3996 (0.4103)	Prec@1 84.375 (85.805)
Epoch: [15][160/391]	Time 0.047 (0.051)	Loss 0.4243 (0.4132)	Prec@1 86.719 (85.705)
Epoch: [15][180/391]	Time 0.046 (0.050)	Loss 0.3507 (0.4171)	Prec@1 88.281 (85.519)
Epoch: [15][200/391]	Time 0.048 (0.050)	Loss 0.3847 (0.4183)	Prec@1 88.281 (85.479)
Epoch: [15][220/391]	Time 0.046 (0.050)	Loss 0.4617 (0.4192)	Prec@1 85.938 (85.464)
Epoch: [15][240/391]	Time 0.046 (0.049)	Loss 0.3821 (0.4190)	Prec@1 84.375 (85.474)
Epoch: [15][260/391]	Time 0.046 (0.049)	Loss 0.3644 (0.4178)	Prec@1 89.062 (85.489)
Epoch: [15][280/391]	Time 0.046 (0.049)	Loss 0.4351 (0.4174)	Prec@1 85.938 (85.543)
Epoch: [15][300/391]	Time 0.047 (0.049)	Loss 0.3892 (0.4170)	Prec@1 82.812 (85.579)
Epoch: [15][320/391]	Time 0.047 (0.049)	Loss 0.3041 (0.4164)	Prec@1 91.406 (85.631)
Epoch: [15][340/391]	Time 0.048 (0.049)	Loss 0.6918 (0.4186)	Prec@1 79.688 (85.562)
Epoch: [15][360/391]	Time 0.045 (0.048)	Loss 0.3964 (0.4181)	Prec@1 85.156 (85.567)
Epoch: [15][380/391]	Time 0.045 (0.048)	Loss 0.4915 (0.4170)	Prec@1 85.156 (85.597)
training time:  19.065521478652954
Test: [0/79]	Time 0.683 (0.683)	Loss 0.4750 (0.4750)	Prec@1 85.156 (85.156)
Test: [20/79]	Time 0.031 (0.061)	Loss 0.6988 (0.5822)	Prec@1 78.906 (80.804)
Test: [40/79]	Time 0.029 (0.046)	Loss 0.5336 (0.5672)	Prec@1 80.469 (81.136)
Test: [60/79]	Time 0.031 (0.041)	Loss 0.5306 (0.5615)	Prec@1 80.469 (81.135)
 * Prec@1 80.810
Epoch: [16][0/391]	Time 0.728 (0.728)	Loss 0.3032 (0.3032)	Prec@1 90.625 (90.625)
Epoch: [16][20/391]	Time 0.046 (0.078)	Loss 0.4511 (0.4066)	Prec@1 82.812 (86.049)
Epoch: [16][40/391]	Time 0.046 (0.063)	Loss 0.3188 (0.4038)	Prec@1 88.281 (86.280)
Epoch: [16][60/391]	Time 0.046 (0.057)	Loss 0.4117 (0.4172)	Prec@1 83.594 (85.925)
Epoch: [16][80/391]	Time 0.046 (0.054)	Loss 0.4642 (0.4147)	Prec@1 85.156 (85.976)
Epoch: [16][100/391]	Time 0.046 (0.053)	Loss 0.3037 (0.4074)	Prec@1 89.844 (86.154)
Epoch: [16][120/391]	Time 0.047 (0.052)	Loss 0.4301 (0.4076)	Prec@1 87.500 (86.138)
Epoch: [16][140/391]	Time 0.049 (0.051)	Loss 0.4081 (0.4083)	Prec@1 87.500 (86.120)
Epoch: [16][160/391]	Time 0.045 (0.051)	Loss 0.4788 (0.4086)	Prec@1 83.594 (86.180)
Epoch: [16][180/391]	Time 0.047 (0.051)	Loss 0.4672 (0.4090)	Prec@1 84.375 (86.076)
Epoch: [16][200/391]	Time 0.046 (0.050)	Loss 0.4548 (0.4082)	Prec@1 85.938 (86.050)
Epoch: [16][220/391]	Time 0.046 (0.050)	Loss 0.4181 (0.4084)	Prec@1 86.719 (86.026)
Epoch: [16][240/391]	Time 0.041 (0.050)	Loss 0.4050 (0.4088)	Prec@1 86.719 (85.989)
Epoch: [16][260/391]	Time 0.045 (0.049)	Loss 0.3943 (0.4098)	Prec@1 83.594 (85.985)
Epoch: [16][280/391]	Time 0.046 (0.049)	Loss 0.4496 (0.4098)	Prec@1 84.375 (86.018)
Epoch: [16][300/391]	Time 0.045 (0.049)	Loss 0.4092 (0.4072)	Prec@1 85.938 (86.106)
Epoch: [16][320/391]	Time 0.044 (0.049)	Loss 0.5365 (0.4055)	Prec@1 80.469 (86.132)
Epoch: [16][340/391]	Time 0.045 (0.048)	Loss 0.3115 (0.4070)	Prec@1 89.844 (86.052)
Epoch: [16][360/391]	Time 0.047 (0.048)	Loss 0.3525 (0.4070)	Prec@1 88.281 (86.031)
Epoch: [16][380/391]	Time 0.044 (0.048)	Loss 0.4971 (0.4066)	Prec@1 82.031 (86.028)
training time:  18.979820013046265
Test: [0/79]	Time 0.735 (0.735)	Loss 0.7094 (0.7094)	Prec@1 77.344 (77.344)
Test: [20/79]	Time 0.032 (0.064)	Loss 0.6321 (0.6682)	Prec@1 80.469 (77.790)
Test: [40/79]	Time 0.032 (0.048)	Loss 0.7237 (0.6707)	Prec@1 72.656 (77.896)
Test: [60/79]	Time 0.031 (0.042)	Loss 0.6519 (0.6672)	Prec@1 77.344 (77.728)
 * Prec@1 77.650
Epoch: [17][0/391]	Time 0.771 (0.771)	Loss 0.4737 (0.4737)	Prec@1 83.594 (83.594)
Epoch: [17][20/391]	Time 0.047 (0.081)	Loss 0.3199 (0.3816)	Prec@1 88.281 (86.756)
Epoch: [17][40/391]	Time 0.046 (0.064)	Loss 0.2811 (0.3889)	Prec@1 89.844 (86.585)
Epoch: [17][60/391]	Time 0.046 (0.058)	Loss 0.4464 (0.3840)	Prec@1 85.156 (86.603)
Epoch: [17][80/391]	Time 0.047 (0.055)	Loss 0.4092 (0.3817)	Prec@1 86.719 (86.796)
Epoch: [17][100/391]	Time 0.048 (0.054)	Loss 0.3514 (0.3816)	Prec@1 86.719 (86.757)
Epoch: [17][120/391]	Time 0.045 (0.052)	Loss 0.3568 (0.3855)	Prec@1 85.156 (86.622)
Epoch: [17][140/391]	Time 0.047 (0.052)	Loss 0.3739 (0.3876)	Prec@1 88.281 (86.586)
Epoch: [17][160/391]	Time 0.046 (0.051)	Loss 0.2919 (0.3892)	Prec@1 88.281 (86.544)
Epoch: [17][180/391]	Time 0.045 (0.050)	Loss 0.4736 (0.3939)	Prec@1 83.594 (86.378)
Epoch: [17][200/391]	Time 0.046 (0.050)	Loss 0.3663 (0.3970)	Prec@1 87.500 (86.365)
Epoch: [17][220/391]	Time 0.047 (0.049)	Loss 0.4573 (0.3993)	Prec@1 84.375 (86.326)
Epoch: [17][240/391]	Time 0.047 (0.049)	Loss 0.3914 (0.3984)	Prec@1 85.938 (86.375)
Epoch: [17][260/391]	Time 0.047 (0.049)	Loss 0.5758 (0.3998)	Prec@1 82.812 (86.318)
Epoch: [17][280/391]	Time 0.046 (0.048)	Loss 0.3230 (0.4003)	Prec@1 92.188 (86.341)
Epoch: [17][300/391]	Time 0.046 (0.048)	Loss 0.3645 (0.3995)	Prec@1 84.375 (86.366)
Epoch: [17][320/391]	Time 0.047 (0.048)	Loss 0.3645 (0.3985)	Prec@1 86.719 (86.388)
Epoch: [17][340/391]	Time 0.047 (0.048)	Loss 0.3929 (0.3987)	Prec@1 85.938 (86.403)
Epoch: [17][360/391]	Time 0.046 (0.048)	Loss 0.3065 (0.3984)	Prec@1 90.625 (86.388)
Epoch: [17][380/391]	Time 0.046 (0.048)	Loss 0.3734 (0.3981)	Prec@1 89.844 (86.370)
training time:  18.876790761947632
Test: [0/79]	Time 0.755 (0.755)	Loss 0.5236 (0.5236)	Prec@1 85.156 (85.156)
Test: [20/79]	Time 0.032 (0.066)	Loss 0.3887 (0.5083)	Prec@1 83.594 (82.589)
Test: [40/79]	Time 0.032 (0.050)	Loss 0.4935 (0.4944)	Prec@1 82.812 (82.946)
Test: [60/79]	Time 0.031 (0.044)	Loss 0.3512 (0.4999)	Prec@1 89.062 (82.992)
 * Prec@1 82.870
=> Saving checkpoint for epoch 17, with Prec@1 82.870000.
Epoch: [18][0/391]	Time 0.764 (0.764)	Loss 0.3291 (0.3291)	Prec@1 89.062 (89.062)
Epoch: [18][20/391]	Time 0.046 (0.081)	Loss 0.3525 (0.3580)	Prec@1 89.062 (88.095)
Epoch: [18][40/391]	Time 0.046 (0.064)	Loss 0.3629 (0.3644)	Prec@1 84.375 (87.786)
Epoch: [18][60/391]	Time 0.047 (0.058)	Loss 0.4665 (0.3802)	Prec@1 84.375 (86.975)
Epoch: [18][80/391]	Time 0.045 (0.055)	Loss 0.4601 (0.3891)	Prec@1 85.156 (86.738)
Epoch: [18][100/391]	Time 0.046 (0.053)	Loss 0.3307 (0.3854)	Prec@1 89.844 (86.812)
Epoch: [18][120/391]	Time 0.046 (0.052)	Loss 0.3613 (0.3825)	Prec@1 87.500 (86.971)
Epoch: [18][140/391]	Time 0.047 (0.051)	Loss 0.3784 (0.3878)	Prec@1 87.500 (86.807)
Epoch: [18][160/391]	Time 0.048 (0.051)	Loss 0.5617 (0.3877)	Prec@1 78.906 (86.874)
Epoch: [18][180/391]	Time 0.047 (0.051)	Loss 0.4883 (0.3879)	Prec@1 85.156 (86.809)
Epoch: [18][200/391]	Time 0.048 (0.051)	Loss 0.3933 (0.3897)	Prec@1 85.156 (86.785)
Epoch: [18][220/391]	Time 0.046 (0.050)	Loss 0.3984 (0.3923)	Prec@1 84.375 (86.645)
Epoch: [18][240/391]	Time 0.048 (0.050)	Loss 0.3463 (0.3932)	Prec@1 89.062 (86.596)
Epoch: [18][260/391]	Time 0.046 (0.050)	Loss 0.3013 (0.3916)	Prec@1 90.625 (86.623)
Epoch: [18][280/391]	Time 0.046 (0.049)	Loss 0.4611 (0.3901)	Prec@1 81.250 (86.716)
Epoch: [18][300/391]	Time 0.048 (0.049)	Loss 0.4973 (0.3910)	Prec@1 84.375 (86.651)
Epoch: [18][320/391]	Time 0.047 (0.049)	Loss 0.5528 (0.3912)	Prec@1 85.156 (86.624)
Epoch: [18][340/391]	Time 0.046 (0.049)	Loss 0.4296 (0.3918)	Prec@1 85.156 (86.620)
Epoch: [18][360/391]	Time 0.046 (0.049)	Loss 0.4322 (0.3915)	Prec@1 85.156 (86.587)
Epoch: [18][380/391]	Time 0.045 (0.049)	Loss 0.4377 (0.3902)	Prec@1 85.938 (86.647)
training time:  19.144365549087524
Test: [0/79]	Time 0.732 (0.732)	Loss 0.5184 (0.5184)	Prec@1 82.031 (82.031)
Test: [20/79]	Time 0.032 (0.064)	Loss 0.4506 (0.5066)	Prec@1 82.031 (83.110)
Test: [40/79]	Time 0.032 (0.048)	Loss 0.5352 (0.5060)	Prec@1 80.469 (83.213)
Test: [60/79]	Time 0.031 (0.043)	Loss 0.4049 (0.5033)	Prec@1 88.281 (83.427)
 * Prec@1 83.450
=> Saving checkpoint for epoch 18, with Prec@1 83.450000.
Epoch: [19][0/391]	Time 0.755 (0.755)	Loss 0.3958 (0.3958)	Prec@1 85.156 (85.156)
Epoch: [19][20/391]	Time 0.047 (0.080)	Loss 0.3537 (0.3472)	Prec@1 88.281 (88.393)
Epoch: [19][40/391]	Time 0.044 (0.063)	Loss 0.3477 (0.3584)	Prec@1 85.156 (88.262)
Epoch: [19][60/391]	Time 0.047 (0.057)	Loss 0.2745 (0.3613)	Prec@1 89.844 (88.128)
Epoch: [19][80/391]	Time 0.045 (0.055)	Loss 0.3489 (0.3651)	Prec@1 86.719 (87.722)
Epoch: [19][100/391]	Time 0.046 (0.053)	Loss 0.3762 (0.3707)	Prec@1 85.156 (87.531)
Epoch: [19][120/391]	Time 0.048 (0.052)	Loss 0.3271 (0.3711)	Prec@1 86.719 (87.526)
Epoch: [19][140/391]	Time 0.048 (0.051)	Loss 0.4485 (0.3739)	Prec@1 85.156 (87.422)
Epoch: [19][160/391]	Time 0.047 (0.051)	Loss 0.2768 (0.3768)	Prec@1 90.625 (87.384)
Epoch: [19][180/391]	Time 0.047 (0.050)	Loss 0.4238 (0.3791)	Prec@1 86.719 (87.194)
Epoch: [19][200/391]	Time 0.046 (0.050)	Loss 0.4165 (0.3786)	Prec@1 86.719 (87.166)
Epoch: [19][220/391]	Time 0.046 (0.049)	Loss 0.2821 (0.3758)	Prec@1 90.625 (87.277)
Epoch: [19][240/391]	Time 0.046 (0.049)	Loss 0.4620 (0.3750)	Prec@1 83.594 (87.331)
Epoch: [19][260/391]	Time 0.046 (0.049)	Loss 0.3700 (0.3780)	Prec@1 88.281 (87.234)
Epoch: [19][280/391]	Time 0.048 (0.049)	Loss 0.4025 (0.3787)	Prec@1 86.719 (87.152)
Epoch: [19][300/391]	Time 0.046 (0.049)	Loss 0.4818 (0.3796)	Prec@1 82.812 (87.105)
Epoch: [19][320/391]	Time 0.047 (0.048)	Loss 0.3761 (0.3793)	Prec@1 89.062 (87.113)
Epoch: [19][340/391]	Time 0.047 (0.048)	Loss 0.3966 (0.3808)	Prec@1 89.844 (87.099)
Epoch: [19][360/391]	Time 0.048 (0.048)	Loss 0.2950 (0.3808)	Prec@1 92.188 (87.126)
Epoch: [19][380/391]	Time 0.044 (0.048)	Loss 0.4397 (0.3817)	Prec@1 86.719 (87.090)
training time:  18.93507146835327
Test: [0/79]	Time 0.732 (0.732)	Loss 0.3983 (0.3983)	Prec@1 85.156 (85.156)
Test: [20/79]	Time 0.031 (0.065)	Loss 0.5590 (0.5162)	Prec@1 82.812 (82.775)
Test: [40/79]	Time 0.031 (0.049)	Loss 0.4668 (0.5077)	Prec@1 85.938 (82.565)
Test: [60/79]	Time 0.031 (0.044)	Loss 0.5270 (0.4986)	Prec@1 82.031 (82.812)
 * Prec@1 82.740
Epoch: [20][0/391]	Time 0.764 (0.764)	Loss 0.2691 (0.2691)	Prec@1 90.625 (90.625)
Epoch: [20][20/391]	Time 0.047 (0.081)	Loss 0.2709 (0.3410)	Prec@1 89.062 (88.170)
Epoch: [20][40/391]	Time 0.046 (0.064)	Loss 0.3907 (0.3519)	Prec@1 90.625 (87.805)
Epoch: [20][60/391]	Time 0.047 (0.058)	Loss 0.3585 (0.3511)	Prec@1 87.500 (87.705)
Epoch: [20][80/391]	Time 0.046 (0.055)	Loss 0.4503 (0.3593)	Prec@1 89.062 (87.625)
Epoch: [20][100/391]	Time 0.046 (0.054)	Loss 0.2476 (0.3586)	Prec@1 92.188 (87.678)
Epoch: [20][120/391]	Time 0.047 (0.053)	Loss 0.3751 (0.3585)	Prec@1 85.156 (87.726)
Epoch: [20][140/391]	Time 0.047 (0.052)	Loss 0.3188 (0.3637)	Prec@1 89.844 (87.644)
Epoch: [20][160/391]	Time 0.047 (0.051)	Loss 0.3819 (0.3625)	Prec@1 85.938 (87.709)
Epoch: [20][180/391]	Time 0.190 (0.051)	Loss 0.2817 (0.3678)	Prec@1 88.281 (87.379)
Epoch: [20][200/391]	Time 0.047 (0.051)	Loss 0.3881 (0.3684)	Prec@1 87.500 (87.403)
Epoch: [20][220/391]	Time 0.045 (0.051)	Loss 0.3072 (0.3667)	Prec@1 89.062 (87.451)
Epoch: [20][240/391]	Time 0.046 (0.050)	Loss 0.3421 (0.3677)	Prec@1 87.500 (87.370)
Epoch: [20][260/391]	Time 0.047 (0.050)	Loss 0.3659 (0.3680)	Prec@1 88.281 (87.338)
Epoch: [20][280/391]	Time 0.046 (0.050)	Loss 0.4729 (0.3691)	Prec@1 84.375 (87.358)
Epoch: [20][300/391]	Time 0.045 (0.049)	Loss 0.5025 (0.3693)	Prec@1 83.594 (87.396)
Epoch: [20][320/391]	Time 0.045 (0.049)	Loss 0.5272 (0.3716)	Prec@1 80.469 (87.283)
Epoch: [20][340/391]	Time 0.048 (0.049)	Loss 0.3925 (0.3733)	Prec@1 89.062 (87.255)
Epoch: [20][360/391]	Time 0.046 (0.049)	Loss 0.2977 (0.3757)	Prec@1 90.625 (87.158)
Epoch: [20][380/391]	Time 0.046 (0.049)	Loss 0.2221 (0.3750)	Prec@1 92.188 (87.174)
training time:  19.197801113128662
Test: [0/79]	Time 0.736 (0.736)	Loss 0.3481 (0.3481)	Prec@1 92.188 (92.188)
Test: [20/79]	Time 0.033 (0.065)	Loss 0.4029 (0.4263)	Prec@1 86.719 (85.342)
Test: [40/79]	Time 0.032 (0.049)	Loss 0.4903 (0.4125)	Prec@1 82.812 (85.995)
Test: [60/79]	Time 0.031 (0.044)	Loss 0.3869 (0.4147)	Prec@1 89.844 (85.797)
 * Prec@1 85.770
=> Saving checkpoint for epoch 20, with Prec@1 85.770000.
Epoch: [21][0/391]	Time 0.763 (0.763)	Loss 0.3259 (0.3259)	Prec@1 86.719 (86.719)
Epoch: [21][20/391]	Time 0.049 (0.081)	Loss 0.3848 (0.3239)	Prec@1 85.938 (88.876)
Epoch: [21][40/391]	Time 0.045 (0.064)	Loss 0.2790 (0.3382)	Prec@1 91.406 (88.605)
Epoch: [21][60/391]	Time 0.045 (0.058)	Loss 0.3506 (0.3404)	Prec@1 88.281 (88.371)
Epoch: [21][80/391]	Time 0.046 (0.055)	Loss 0.3426 (0.3454)	Prec@1 92.188 (88.233)
Epoch: [21][100/391]	Time 0.046 (0.054)	Loss 0.3669 (0.3481)	Prec@1 86.719 (88.235)
Epoch: [21][120/391]	Time 0.050 (0.052)	Loss 0.5747 (0.3512)	Prec@1 82.031 (88.120)
Epoch: [21][140/391]	Time 0.048 (0.051)	Loss 0.3907 (0.3523)	Prec@1 85.938 (88.015)
Epoch: [21][160/391]	Time 0.046 (0.051)	Loss 0.3586 (0.3544)	Prec@1 87.500 (87.917)
Epoch: [21][180/391]	Time 0.043 (0.050)	Loss 0.5204 (0.3588)	Prec@1 83.594 (87.724)
Epoch: [21][200/391]	Time 0.043 (0.050)	Loss 0.3925 (0.3608)	Prec@1 87.500 (87.671)
Epoch: [21][220/391]	Time 0.044 (0.050)	Loss 0.4563 (0.3621)	Prec@1 85.156 (87.603)
Epoch: [21][240/391]	Time 0.048 (0.049)	Loss 0.2977 (0.3632)	Prec@1 91.406 (87.607)
Epoch: [21][260/391]	Time 0.048 (0.049)	Loss 0.3507 (0.3641)	Prec@1 90.625 (87.623)
Epoch: [21][280/391]	Time 0.047 (0.049)	Loss 0.4289 (0.3634)	Prec@1 83.594 (87.600)
Epoch: [21][300/391]	Time 0.046 (0.049)	Loss 0.4275 (0.3653)	Prec@1 85.156 (87.516)
Epoch: [21][320/391]	Time 0.046 (0.048)	Loss 0.3346 (0.3684)	Prec@1 89.844 (87.398)
Epoch: [21][340/391]	Time 0.049 (0.048)	Loss 0.4120 (0.3699)	Prec@1 85.938 (87.353)
Epoch: [21][360/391]	Time 0.044 (0.048)	Loss 0.3573 (0.3709)	Prec@1 89.844 (87.292)
Epoch: [21][380/391]	Time 0.047 (0.048)	Loss 0.3358 (0.3719)	Prec@1 89.844 (87.270)
training time:  18.94044852256775
Test: [0/79]	Time 0.739 (0.739)	Loss 0.4617 (0.4617)	Prec@1 85.156 (85.156)
Test: [20/79]	Time 0.032 (0.065)	Loss 0.4567 (0.5268)	Prec@1 85.938 (80.952)
Test: [40/79]	Time 0.032 (0.049)	Loss 0.4939 (0.5256)	Prec@1 82.031 (81.364)
Test: [60/79]	Time 0.032 (0.044)	Loss 0.3789 (0.5248)	Prec@1 87.500 (81.660)
 * Prec@1 81.560
Epoch: [22][0/391]	Time 0.768 (0.768)	Loss 0.3878 (0.3878)	Prec@1 86.719 (86.719)
Epoch: [22][20/391]	Time 0.046 (0.080)	Loss 0.2734 (0.3308)	Prec@1 89.844 (88.839)
Epoch: [22][40/391]	Time 0.045 (0.064)	Loss 0.2611 (0.3267)	Prec@1 89.844 (89.120)
Epoch: [22][60/391]	Time 0.045 (0.058)	Loss 0.4985 (0.3397)	Prec@1 82.812 (88.576)
Epoch: [22][80/391]	Time 0.047 (0.055)	Loss 0.2772 (0.3372)	Prec@1 89.062 (88.561)
Epoch: [22][100/391]	Time 0.047 (0.053)	Loss 0.4184 (0.3426)	Prec@1 88.281 (88.374)
Epoch: [22][120/391]	Time 0.047 (0.052)	Loss 0.3331 (0.3477)	Prec@1 84.375 (88.107)
Epoch: [22][140/391]	Time 0.047 (0.051)	Loss 0.3628 (0.3435)	Prec@1 88.281 (88.209)
Epoch: [22][160/391]	Time 0.045 (0.051)	Loss 0.3385 (0.3452)	Prec@1 87.500 (88.170)
Epoch: [22][180/391]	Time 0.046 (0.051)	Loss 0.2722 (0.3469)	Prec@1 92.188 (88.195)
Epoch: [22][200/391]	Time 0.048 (0.050)	Loss 0.4024 (0.3492)	Prec@1 86.719 (88.110)
Epoch: [22][220/391]	Time 0.048 (0.050)	Loss 0.2991 (0.3474)	Prec@1 89.062 (88.182)
Epoch: [22][240/391]	Time 0.046 (0.050)	Loss 0.4670 (0.3500)	Prec@1 82.812 (88.025)
Epoch: [22][260/391]	Time 0.047 (0.049)	Loss 0.3368 (0.3525)	Prec@1 89.062 (87.868)
Epoch: [22][280/391]	Time 0.048 (0.049)	Loss 0.2880 (0.3531)	Prec@1 89.844 (87.892)
Epoch: [22][300/391]	Time 0.048 (0.049)	Loss 0.4000 (0.3566)	Prec@1 85.938 (87.788)
Epoch: [22][320/391]	Time 0.047 (0.049)	Loss 0.3119 (0.3576)	Prec@1 89.062 (87.743)
Epoch: [22][340/391]	Time 0.045 (0.049)	Loss 0.3869 (0.3589)	Prec@1 83.594 (87.672)
Epoch: [22][360/391]	Time 0.048 (0.049)	Loss 0.4606 (0.3603)	Prec@1 80.469 (87.632)
Epoch: [22][380/391]	Time 0.045 (0.049)	Loss 0.4322 (0.3606)	Prec@1 85.156 (87.633)
training time:  19.10125994682312
Test: [0/79]	Time 0.728 (0.728)	Loss 0.5542 (0.5542)	Prec@1 79.688 (79.688)
Test: [20/79]	Time 0.032 (0.064)	Loss 0.6468 (0.5987)	Prec@1 78.906 (80.729)
Test: [40/79]	Time 0.029 (0.048)	Loss 0.7170 (0.5877)	Prec@1 77.344 (81.479)
Test: [60/79]	Time 0.031 (0.043)	Loss 0.5000 (0.5740)	Prec@1 82.812 (81.814)
 * Prec@1 81.880
Epoch: [23][0/391]	Time 0.762 (0.762)	Loss 0.2786 (0.2786)	Prec@1 90.625 (90.625)
