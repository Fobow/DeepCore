================== Exp 0 ==================
dataset: CIFAR10, model: ResNet18, selection: GraNd, num_ex: 1, epochs: 100, fraction: 0.2, seed: 51881, lr: 0.1, save_path: ./result, resume: , device: cuda, checkpoint_name: CIFAR10_ResNet18_GraNd_exp0_epoch100_2024-03-18 15:34:13.735782_0.2_
Files already downloaded and verified
Files already downloaded and verified
called grand, with 10 ensemble and 10 epochs
=> Early Training Epoch #0
| Epoch [  0/ 10] Iter[  1/391]		Loss: 2.4070
| Epoch [  0/ 10] Iter[ 21/391]		Loss: 2.3322
| Epoch [  0/ 10] Iter[ 41/391]		Loss: 2.1047
| Epoch [  0/ 10] Iter[ 61/391]		Loss: 1.9469
| Epoch [  0/ 10] Iter[ 81/391]		Loss: 1.8871
| Epoch [  0/ 10] Iter[101/391]		Loss: 1.9069
| Epoch [  0/ 10] Iter[121/391]		Loss: 1.8195
| Epoch [  0/ 10] Iter[141/391]		Loss: 1.6845
| Epoch [  0/ 10] Iter[161/391]		Loss: 1.8008
| Epoch [  0/ 10] Iter[181/391]		Loss: 1.6218
| Epoch [  0/ 10] Iter[201/391]		Loss: 1.7439
| Epoch [  0/ 10] Iter[221/391]		Loss: 1.5938
| Epoch [  0/ 10] Iter[241/391]		Loss: 1.6827
| Epoch [  0/ 10] Iter[261/391]		Loss: 1.5656
| Epoch [  0/ 10] Iter[281/391]		Loss: 1.5376
| Epoch [  0/ 10] Iter[301/391]		Loss: 1.4913
| Epoch [  0/ 10] Iter[321/391]		Loss: 1.4629
| Epoch [  0/ 10] Iter[341/391]		Loss: 1.4615
| Epoch [  0/ 10] Iter[361/391]		Loss: 1.4037
| Epoch [  0/ 10] Iter[381/391]		Loss: 1.5904
=> Early Training Epoch #1
| Epoch [  1/ 10] Iter[  1/391]		Loss: 1.5069
| Epoch [  1/ 10] Iter[ 21/391]		Loss: 1.3844
| Epoch [  1/ 10] Iter[ 41/391]		Loss: 1.4702
| Epoch [  1/ 10] Iter[ 61/391]		Loss: 1.3942
| Epoch [  1/ 10] Iter[ 81/391]		Loss: 1.3488
| Epoch [  1/ 10] Iter[101/391]		Loss: 1.3686
| Epoch [  1/ 10] Iter[121/391]		Loss: 1.4180
| Epoch [  1/ 10] Iter[141/391]		Loss: 1.3393
| Epoch [  1/ 10] Iter[161/391]		Loss: 1.3098
| Epoch [  1/ 10] Iter[181/391]		Loss: 1.2618
| Epoch [  1/ 10] Iter[201/391]		Loss: 1.3216
| Epoch [  1/ 10] Iter[221/391]		Loss: 1.1693
| Epoch [  1/ 10] Iter[241/391]		Loss: 1.1543
| Epoch [  1/ 10] Iter[261/391]		Loss: 1.1509
| Epoch [  1/ 10] Iter[281/391]		Loss: 1.3376
| Epoch [  1/ 10] Iter[301/391]		Loss: 1.1549
| Epoch [  1/ 10] Iter[321/391]		Loss: 1.1257
| Epoch [  1/ 10] Iter[341/391]		Loss: 1.2175
| Epoch [  1/ 10] Iter[361/391]		Loss: 1.0845
| Epoch [  1/ 10] Iter[381/391]		Loss: 1.1278
=> Early Training Epoch #2
| Epoch [  2/ 10] Iter[  1/391]		Loss: 1.0189
| Epoch [  2/ 10] Iter[ 21/391]		Loss: 1.0316
| Epoch [  2/ 10] Iter[ 41/391]		Loss: 1.1596
| Epoch [  2/ 10] Iter[ 61/391]		Loss: 0.9909
| Epoch [  2/ 10] Iter[ 81/391]		Loss: 1.1130
| Epoch [  2/ 10] Iter[101/391]		Loss: 0.9193
| Epoch [  2/ 10] Iter[121/391]		Loss: 1.0177
| Epoch [  2/ 10] Iter[141/391]		Loss: 1.0511
| Epoch [  2/ 10] Iter[161/391]		Loss: 0.9815
| Epoch [  2/ 10] Iter[181/391]		Loss: 0.9063
| Epoch [  2/ 10] Iter[201/391]		Loss: 0.9215
| Epoch [  2/ 10] Iter[221/391]		Loss: 0.8881
| Epoch [  2/ 10] Iter[241/391]		Loss: 1.1124
| Epoch [  2/ 10] Iter[261/391]		Loss: 1.1646
| Epoch [  2/ 10] Iter[281/391]		Loss: 1.0370
| Epoch [  2/ 10] Iter[301/391]		Loss: 0.9602
| Epoch [  2/ 10] Iter[321/391]		Loss: 0.6674
| Epoch [  2/ 10] Iter[341/391]		Loss: 1.0191
| Epoch [  2/ 10] Iter[361/391]		Loss: 0.9139
| Epoch [  2/ 10] Iter[381/391]		Loss: 0.8511
=> Early Training Epoch #3
| Epoch [  3/ 10] Iter[  1/391]		Loss: 0.9145
| Epoch [  3/ 10] Iter[ 21/391]		Loss: 0.7955
| Epoch [  3/ 10] Iter[ 41/391]		Loss: 0.9698
| Epoch [  3/ 10] Iter[ 61/391]		Loss: 0.9669
| Epoch [  3/ 10] Iter[ 81/391]		Loss: 0.7930
| Epoch [  3/ 10] Iter[101/391]		Loss: 0.8959
| Epoch [  3/ 10] Iter[121/391]		Loss: 0.7904
| Epoch [  3/ 10] Iter[141/391]		Loss: 1.1063
| Epoch [  3/ 10] Iter[161/391]		Loss: 0.7559
| Epoch [  3/ 10] Iter[181/391]		Loss: 0.7369
| Epoch [  3/ 10] Iter[201/391]		Loss: 0.8578
| Epoch [  3/ 10] Iter[221/391]		Loss: 0.8888
| Epoch [  3/ 10] Iter[241/391]		Loss: 0.7433
| Epoch [  3/ 10] Iter[261/391]		Loss: 0.7318
| Epoch [  3/ 10] Iter[281/391]		Loss: 0.8292
| Epoch [  3/ 10] Iter[301/391]		Loss: 0.9318
| Epoch [  3/ 10] Iter[321/391]		Loss: 0.7784
| Epoch [  3/ 10] Iter[341/391]		Loss: 0.7476
| Epoch [  3/ 10] Iter[361/391]		Loss: 0.7873
| Epoch [  3/ 10] Iter[381/391]		Loss: 0.7526
=> Early Training Epoch #4
| Epoch [  4/ 10] Iter[  1/391]		Loss: 0.6397
| Epoch [  4/ 10] Iter[ 21/391]		Loss: 0.5871
| Epoch [  4/ 10] Iter[ 41/391]		Loss: 0.6543
| Epoch [  4/ 10] Iter[ 61/391]		Loss: 0.7271
| Epoch [  4/ 10] Iter[ 81/391]		Loss: 0.5030
| Epoch [  4/ 10] Iter[101/391]		Loss: 0.6242
| Epoch [  4/ 10] Iter[121/391]		Loss: 0.6227
| Epoch [  4/ 10] Iter[141/391]		Loss: 0.6530
| Epoch [  4/ 10] Iter[161/391]		Loss: 0.7572
| Epoch [  4/ 10] Iter[181/391]		Loss: 0.6493
| Epoch [  4/ 10] Iter[201/391]		Loss: 0.8007
| Epoch [  4/ 10] Iter[221/391]		Loss: 0.5053
| Epoch [  4/ 10] Iter[241/391]		Loss: 0.7770
| Epoch [  4/ 10] Iter[261/391]		Loss: 0.6752
| Epoch [  4/ 10] Iter[281/391]		Loss: 0.5745
| Epoch [  4/ 10] Iter[301/391]		Loss: 0.6735
| Epoch [  4/ 10] Iter[321/391]		Loss: 0.6286
| Epoch [  4/ 10] Iter[341/391]		Loss: 0.5595
| Epoch [  4/ 10] Iter[361/391]		Loss: 0.5764
| Epoch [  4/ 10] Iter[381/391]		Loss: 0.6004
=> Early Training Epoch #5
| Epoch [  5/ 10] Iter[  1/391]		Loss: 0.6223
| Epoch [  5/ 10] Iter[ 21/391]		Loss: 0.6242
| Epoch [  5/ 10] Iter[ 41/391]		Loss: 0.6275
| Epoch [  5/ 10] Iter[ 61/391]		Loss: 0.7130
| Epoch [  5/ 10] Iter[ 81/391]		Loss: 0.5228
| Epoch [  5/ 10] Iter[101/391]		Loss: 0.5169
| Epoch [  5/ 10] Iter[121/391]		Loss: 0.6894
| Epoch [  5/ 10] Iter[141/391]		Loss: 0.5152
| Epoch [  5/ 10] Iter[161/391]		Loss: 0.5649
| Epoch [  5/ 10] Iter[181/391]		Loss: 0.6994
| Epoch [  5/ 10] Iter[201/391]		Loss: 0.4653
| Epoch [  5/ 10] Iter[221/391]		Loss: 0.6906
| Epoch [  5/ 10] Iter[241/391]		Loss: 0.5569
| Epoch [  5/ 10] Iter[261/391]		Loss: 0.5430
| Epoch [  5/ 10] Iter[281/391]		Loss: 0.4876
| Epoch [  5/ 10] Iter[301/391]		Loss: 0.4625
| Epoch [  5/ 10] Iter[321/391]		Loss: 0.5403
| Epoch [  5/ 10] Iter[341/391]		Loss: 0.5861
| Epoch [  5/ 10] Iter[361/391]		Loss: 0.5187
| Epoch [  5/ 10] Iter[381/391]		Loss: 0.5513
=> Early Training Epoch #6
| Epoch [  6/ 10] Iter[  1/391]		Loss: 0.4408
| Epoch [  6/ 10] Iter[ 21/391]		Loss: 0.5736
| Epoch [  6/ 10] Iter[ 41/391]		Loss: 0.3402
| Epoch [  6/ 10] Iter[ 61/391]		Loss: 0.4566
| Epoch [  6/ 10] Iter[ 81/391]		Loss: 0.5026
| Epoch [  6/ 10] Iter[101/391]		Loss: 0.4172
| Epoch [  6/ 10] Iter[121/391]		Loss: 0.3795
| Epoch [  6/ 10] Iter[141/391]		Loss: 0.4732
| Epoch [  6/ 10] Iter[161/391]		Loss: 0.5049
| Epoch [  6/ 10] Iter[181/391]		Loss: 0.4328
| Epoch [  6/ 10] Iter[201/391]		Loss: 0.4496
| Epoch [  6/ 10] Iter[221/391]		Loss: 0.3557
| Epoch [  6/ 10] Iter[241/391]		Loss: 0.4856
| Epoch [  6/ 10] Iter[261/391]		Loss: 0.4803
| Epoch [  6/ 10] Iter[281/391]		Loss: 0.4419
| Epoch [  6/ 10] Iter[301/391]		Loss: 0.5994
| Epoch [  6/ 10] Iter[321/391]		Loss: 0.5330
| Epoch [  6/ 10] Iter[341/391]		Loss: 0.4594
| Epoch [  6/ 10] Iter[361/391]		Loss: 0.4793
| Epoch [  6/ 10] Iter[381/391]		Loss: 0.5515
=> Early Training Epoch #7
| Epoch [  7/ 10] Iter[  1/391]		Loss: 0.3329
| Epoch [  7/ 10] Iter[ 21/391]		Loss: 0.3054
| Epoch [  7/ 10] Iter[ 41/391]		Loss: 0.2538
| Epoch [  7/ 10] Iter[ 61/391]		Loss: 0.3502
| Epoch [  7/ 10] Iter[ 81/391]		Loss: 0.2363
| Epoch [  7/ 10] Iter[101/391]		Loss: 0.4334
| Epoch [  7/ 10] Iter[121/391]		Loss: 0.4075
| Epoch [  7/ 10] Iter[141/391]		Loss: 0.3458
| Epoch [  7/ 10] Iter[161/391]		Loss: 0.4869
| Epoch [  7/ 10] Iter[181/391]		Loss: 0.5209
| Epoch [  7/ 10] Iter[201/391]		Loss: 0.3035
| Epoch [  7/ 10] Iter[221/391]		Loss: 0.5120
| Epoch [  7/ 10] Iter[241/391]		Loss: 0.5573
| Epoch [  7/ 10] Iter[261/391]		Loss: 0.4784
| Epoch [  7/ 10] Iter[281/391]		Loss: 0.3904
| Epoch [  7/ 10] Iter[301/391]		Loss: 0.3771
| Epoch [  7/ 10] Iter[321/391]		Loss: 0.4103
| Epoch [  7/ 10] Iter[341/391]		Loss: 0.4507
| Epoch [  7/ 10] Iter[361/391]		Loss: 0.4998
| Epoch [  7/ 10] Iter[381/391]		Loss: 0.2879
=> Early Training Epoch #8
| Epoch [  8/ 10] Iter[  1/391]		Loss: 0.4395
| Epoch [  8/ 10] Iter[ 21/391]		Loss: 0.3087
| Epoch [  8/ 10] Iter[ 41/391]		Loss: 0.2645
| Epoch [  8/ 10] Iter[ 61/391]		Loss: 0.2752
| Epoch [  8/ 10] Iter[ 81/391]		Loss: 0.2984
| Epoch [  8/ 10] Iter[101/391]		Loss: 0.4920
| Epoch [  8/ 10] Iter[121/391]		Loss: 0.3749
| Epoch [  8/ 10] Iter[141/391]		Loss: 0.4037
| Epoch [  8/ 10] Iter[161/391]		Loss: 0.4405
| Epoch [  8/ 10] Iter[181/391]		Loss: 0.4397
| Epoch [  8/ 10] Iter[201/391]		Loss: 0.5125
| Epoch [  8/ 10] Iter[221/391]		Loss: 0.3671
| Epoch [  8/ 10] Iter[241/391]		Loss: 0.3637
| Epoch [  8/ 10] Iter[261/391]		Loss: 0.5162
| Epoch [  8/ 10] Iter[281/391]		Loss: 0.3860
| Epoch [  8/ 10] Iter[301/391]		Loss: 0.5271
| Epoch [  8/ 10] Iter[321/391]		Loss: 0.3698
| Epoch [  8/ 10] Iter[341/391]		Loss: 0.3243
| Epoch [  8/ 10] Iter[361/391]		Loss: 0.4390
| Epoch [  8/ 10] Iter[381/391]		Loss: 0.3770
=> Early Training Epoch #9
| Epoch [  9/ 10] Iter[  1/391]		Loss: 0.2516
| Epoch [  9/ 10] Iter[ 21/391]		Loss: 0.2824
| Epoch [  9/ 10] Iter[ 41/391]		Loss: 0.1952
| Epoch [  9/ 10] Iter[ 61/391]		Loss: 0.2569
| Epoch [  9/ 10] Iter[ 81/391]		Loss: 0.3456
| Epoch [  9/ 10] Iter[101/391]		Loss: 0.3843
| Epoch [  9/ 10] Iter[121/391]		Loss: 0.2007
| Epoch [  9/ 10] Iter[141/391]		Loss: 0.3102
| Epoch [  9/ 10] Iter[161/391]		Loss: 0.3643
| Epoch [  9/ 10] Iter[181/391]		Loss: 0.3457
| Epoch [  9/ 10] Iter[201/391]		Loss: 0.1869
| Epoch [  9/ 10] Iter[221/391]		Loss: 0.4132
| Epoch [  9/ 10] Iter[241/391]		Loss: 0.5117
| Epoch [  9/ 10] Iter[261/391]		Loss: 0.3728
| Epoch [  9/ 10] Iter[281/391]		Loss: 0.4846
| Epoch [  9/ 10] Iter[301/391]		Loss: 0.3494
| Epoch [  9/ 10] Iter[321/391]		Loss: 0.3777
| Epoch [  9/ 10] Iter[341/391]		Loss: 0.2433
| Epoch [  9/ 10] Iter[361/391]		Loss: 0.4440
| Epoch [  9/ 10] Iter[381/391]		Loss: 0.3774
=> Early Training Epoch #0
| Epoch [  0/ 10] Iter[  1/391]		Loss: 2.4890
| Epoch [  0/ 10] Iter[ 21/391]		Loss: 2.3482
| Epoch [  0/ 10] Iter[ 41/391]		Loss: 2.1264
| Epoch [  0/ 10] Iter[ 61/391]		Loss: 2.0425
| Epoch [  0/ 10] Iter[ 81/391]		Loss: 1.9509
| Epoch [  0/ 10] Iter[101/391]		Loss: 1.9379
| Epoch [  0/ 10] Iter[121/391]		Loss: 1.9775
| Epoch [  0/ 10] Iter[141/391]		Loss: 1.8366
| Epoch [  0/ 10] Iter[161/391]		Loss: 1.9458
| Epoch [  0/ 10] Iter[181/391]		Loss: 1.7992
| Epoch [  0/ 10] Iter[201/391]		Loss: 1.8683
| Epoch [  0/ 10] Iter[221/391]		Loss: 1.7499
| Epoch [  0/ 10] Iter[241/391]		Loss: 1.7922
| Epoch [  0/ 10] Iter[261/391]		Loss: 1.6538
| Epoch [  0/ 10] Iter[281/391]		Loss: 1.5696
| Epoch [  0/ 10] Iter[301/391]		Loss: 1.6951
| Epoch [  0/ 10] Iter[321/391]		Loss: 1.6032
| Epoch [  0/ 10] Iter[341/391]		Loss: 1.5526
| Epoch [  0/ 10] Iter[361/391]		Loss: 1.3259
| Epoch [  0/ 10] Iter[381/391]		Loss: 1.5697
=> Early Training Epoch #1
| Epoch [  1/ 10] Iter[  1/391]		Loss: 1.4140
| Epoch [  1/ 10] Iter[ 21/391]		Loss: 1.3153
| Epoch [  1/ 10] Iter[ 41/391]		Loss: 1.5187
| Epoch [  1/ 10] Iter[ 61/391]		Loss: 1.3793
| Epoch [  1/ 10] Iter[ 81/391]		Loss: 1.4904
| Epoch [  1/ 10] Iter[101/391]		Loss: 1.5057
| Epoch [  1/ 10] Iter[121/391]		Loss: 1.2171
| Epoch [  1/ 10] Iter[141/391]		Loss: 1.3394
| Epoch [  1/ 10] Iter[161/391]		Loss: 1.3014
| Epoch [  1/ 10] Iter[181/391]		Loss: 1.3068
| Epoch [  1/ 10] Iter[201/391]		Loss: 1.2110
| Epoch [  1/ 10] Iter[221/391]		Loss: 1.2356
| Epoch [  1/ 10] Iter[241/391]		Loss: 1.4743
| Epoch [  1/ 10] Iter[261/391]		Loss: 1.1000
| Epoch [  1/ 10] Iter[281/391]		Loss: 1.2379
| Epoch [  1/ 10] Iter[301/391]		Loss: 1.1303
| Epoch [  1/ 10] Iter[321/391]		Loss: 1.1147
| Epoch [  1/ 10] Iter[341/391]		Loss: 1.2796
| Epoch [  1/ 10] Iter[361/391]		Loss: 1.0728
| Epoch [  1/ 10] Iter[381/391]		Loss: 1.2562
=> Early Training Epoch #2
| Epoch [  2/ 10] Iter[  1/391]		Loss: 1.3512
| Epoch [  2/ 10] Iter[ 21/391]		Loss: 1.2258
| Epoch [  2/ 10] Iter[ 41/391]		Loss: 1.1204
| Epoch [  2/ 10] Iter[ 61/391]		Loss: 1.0814
| Epoch [  2/ 10] Iter[ 81/391]		Loss: 1.2222
| Epoch [  2/ 10] Iter[101/391]		Loss: 0.9846
| Epoch [  2/ 10] Iter[121/391]		Loss: 1.0123
| Epoch [  2/ 10] Iter[141/391]		Loss: 1.0399
| Epoch [  2/ 10] Iter[161/391]		Loss: 1.0019
| Epoch [  2/ 10] Iter[181/391]		Loss: 1.0816
| Epoch [  2/ 10] Iter[201/391]		Loss: 1.0189
| Epoch [  2/ 10] Iter[221/391]		Loss: 0.8937
| Epoch [  2/ 10] Iter[241/391]		Loss: 0.9567
| Epoch [  2/ 10] Iter[261/391]		Loss: 0.8741
| Epoch [  2/ 10] Iter[281/391]		Loss: 0.9168
| Epoch [  2/ 10] Iter[301/391]		Loss: 1.0279
| Epoch [  2/ 10] Iter[321/391]		Loss: 0.9668
| Epoch [  2/ 10] Iter[341/391]		Loss: 1.0303
| Epoch [  2/ 10] Iter[361/391]		Loss: 0.8521
| Epoch [  2/ 10] Iter[381/391]		Loss: 0.8877
=> Early Training Epoch #3
| Epoch [  3/ 10] Iter[  1/391]		Loss: 0.9194
| Epoch [  3/ 10] Iter[ 21/391]		Loss: 0.7847
| Epoch [  3/ 10] Iter[ 41/391]		Loss: 0.9395
| Epoch [  3/ 10] Iter[ 61/391]		Loss: 0.8238
| Epoch [  3/ 10] Iter[ 81/391]		Loss: 0.7869
| Epoch [  3/ 10] Iter[101/391]		Loss: 0.9468
| Epoch [  3/ 10] Iter[121/391]		Loss: 0.8721
| Epoch [  3/ 10] Iter[141/391]		Loss: 0.8918
| Epoch [  3/ 10] Iter[161/391]		Loss: 0.7059
| Epoch [  3/ 10] Iter[181/391]		Loss: 0.6445
| Epoch [  3/ 10] Iter[201/391]		Loss: 0.7652
| Epoch [  3/ 10] Iter[221/391]		Loss: 0.6577
| Epoch [  3/ 10] Iter[241/391]		Loss: 0.8880
| Epoch [  3/ 10] Iter[261/391]		Loss: 0.7540
| Epoch [  3/ 10] Iter[281/391]		Loss: 0.7413
| Epoch [  3/ 10] Iter[301/391]		Loss: 0.7327
| Epoch [  3/ 10] Iter[321/391]		Loss: 0.7496
| Epoch [  3/ 10] Iter[341/391]		Loss: 0.8684
| Epoch [  3/ 10] Iter[361/391]		Loss: 0.8039
| Epoch [  3/ 10] Iter[381/391]		Loss: 0.7442
=> Early Training Epoch #4
| Epoch [  4/ 10] Iter[  1/391]		Loss: 0.7102
| Epoch [  4/ 10] Iter[ 21/391]		Loss: 0.6935
| Epoch [  4/ 10] Iter[ 41/391]		Loss: 0.6349
| Epoch [  4/ 10] Iter[ 61/391]		Loss: 0.6647
| Epoch [  4/ 10] Iter[ 81/391]		Loss: 0.6456
| Epoch [  4/ 10] Iter[101/391]		Loss: 0.7014
| Epoch [  4/ 10] Iter[121/391]		Loss: 0.7992
| Epoch [  4/ 10] Iter[141/391]		Loss: 0.8104
| Epoch [  4/ 10] Iter[161/391]		Loss: 0.7135
| Epoch [  4/ 10] Iter[181/391]		Loss: 0.8255
| Epoch [  4/ 10] Iter[201/391]		Loss: 0.6601
| Epoch [  4/ 10] Iter[221/391]		Loss: 0.8204
| Epoch [  4/ 10] Iter[241/391]		Loss: 0.7098
| Epoch [  4/ 10] Iter[261/391]		Loss: 0.5585
| Epoch [  4/ 10] Iter[281/391]		Loss: 0.7823
| Epoch [  4/ 10] Iter[301/391]		Loss: 0.7371
| Epoch [  4/ 10] Iter[321/391]		Loss: 0.7306
| Epoch [  4/ 10] Iter[341/391]		Loss: 0.6772
| Epoch [  4/ 10] Iter[361/391]		Loss: 0.7907
| Epoch [  4/ 10] Iter[381/391]		Loss: 0.5969
=> Early Training Epoch #5
| Epoch [  5/ 10] Iter[  1/391]		Loss: 0.7228
| Epoch [  5/ 10] Iter[ 21/391]		Loss: 0.6061
| Epoch [  5/ 10] Iter[ 41/391]		Loss: 0.6136
| Epoch [  5/ 10] Iter[ 61/391]		Loss: 0.4937
| Epoch [  5/ 10] Iter[ 81/391]		Loss: 0.5283
| Epoch [  5/ 10] Iter[101/391]		Loss: 0.4650
| Epoch [  5/ 10] Iter[121/391]		Loss: 0.5120
| Epoch [  5/ 10] Iter[141/391]		Loss: 0.5811
| Epoch [  5/ 10] Iter[161/391]		Loss: 0.5738
| Epoch [  5/ 10] Iter[181/391]		Loss: 0.7662
| Epoch [  5/ 10] Iter[201/391]		Loss: 0.6193
| Epoch [  5/ 10] Iter[221/391]		Loss: 0.5035
| Epoch [  5/ 10] Iter[241/391]		Loss: 0.6637
| Epoch [  5/ 10] Iter[261/391]		Loss: 0.6344
| Epoch [  5/ 10] Iter[281/391]		Loss: 0.6994
| Epoch [  5/ 10] Iter[301/391]		Loss: 0.5786
| Epoch [  5/ 10] Iter[321/391]		Loss: 0.5258
| Epoch [  5/ 10] Iter[341/391]		Loss: 0.6202
| Epoch [  5/ 10] Iter[361/391]		Loss: 0.5245
| Epoch [  5/ 10] Iter[381/391]		Loss: 0.5603
=> Early Training Epoch #6
| Epoch [  6/ 10] Iter[  1/391]		Loss: 0.4160
| Epoch [  6/ 10] Iter[ 21/391]		Loss: 0.5587
| Epoch [  6/ 10] Iter[ 41/391]		Loss: 0.6542
| Epoch [  6/ 10] Iter[ 61/391]		Loss: 0.4529
| Epoch [  6/ 10] Iter[ 81/391]		Loss: 0.5694
| Epoch [  6/ 10] Iter[101/391]		Loss: 0.6105
| Epoch [  6/ 10] Iter[121/391]		Loss: 0.5300
| Epoch [  6/ 10] Iter[141/391]		Loss: 0.3832
| Epoch [  6/ 10] Iter[161/391]		Loss: 0.5432
| Epoch [  6/ 10] Iter[181/391]		Loss: 0.5733
| Epoch [  6/ 10] Iter[201/391]		Loss: 0.5275
| Epoch [  6/ 10] Iter[221/391]		Loss: 0.5805
| Epoch [  6/ 10] Iter[241/391]		Loss: 0.4163
| Epoch [  6/ 10] Iter[261/391]		Loss: 0.6516
| Epoch [  6/ 10] Iter[281/391]		Loss: 0.3825
| Epoch [  6/ 10] Iter[301/391]		Loss: 0.5785
| Epoch [  6/ 10] Iter[321/391]		Loss: 0.5659
| Epoch [  6/ 10] Iter[341/391]		Loss: 0.6013
| Epoch [  6/ 10] Iter[361/391]		Loss: 0.5012
| Epoch [  6/ 10] Iter[381/391]		Loss: 0.5347
=> Early Training Epoch #7
| Epoch [  7/ 10] Iter[  1/391]		Loss: 0.4090
| Epoch [  7/ 10] Iter[ 21/391]		Loss: 0.2783
| Epoch [  7/ 10] Iter[ 41/391]		Loss: 0.4362
| Epoch [  7/ 10] Iter[ 61/391]		Loss: 0.4084
| Epoch [  7/ 10] Iter[ 81/391]		Loss: 0.4934
| Epoch [  7/ 10] Iter[101/391]		Loss: 0.3942
| Epoch [  7/ 10] Iter[121/391]		Loss: 0.3101
| Epoch [  7/ 10] Iter[141/391]		Loss: 0.3783
| Epoch [  7/ 10] Iter[161/391]		Loss: 0.3614
| Epoch [  7/ 10] Iter[181/391]		Loss: 0.5742
| Epoch [  7/ 10] Iter[201/391]		Loss: 0.3900
| Epoch [  7/ 10] Iter[221/391]		Loss: 0.5907
| Epoch [  7/ 10] Iter[241/391]		Loss: 0.5987
| Epoch [  7/ 10] Iter[261/391]		Loss: 0.6579
| Epoch [  7/ 10] Iter[281/391]		Loss: 0.3784
| Epoch [  7/ 10] Iter[301/391]		Loss: 0.3429
| Epoch [  7/ 10] Iter[321/391]		Loss: 0.5358
| Epoch [  7/ 10] Iter[341/391]		Loss: 0.4664
| Epoch [  7/ 10] Iter[361/391]		Loss: 0.3623
| Epoch [  7/ 10] Iter[381/391]		Loss: 0.4917
=> Early Training Epoch #8
| Epoch [  8/ 10] Iter[  1/391]		Loss: 0.2672
| Epoch [  8/ 10] Iter[ 21/391]		Loss: 0.3163
| Epoch [  8/ 10] Iter[ 41/391]		Loss: 0.3454
| Epoch [  8/ 10] Iter[ 61/391]		Loss: 0.3214
| Epoch [  8/ 10] Iter[ 81/391]		Loss: 0.2322
| Epoch [  8/ 10] Iter[101/391]		Loss: 0.3222
| Epoch [  8/ 10] Iter[121/391]		Loss: 0.3139
| Epoch [  8/ 10] Iter[141/391]		Loss: 0.5790
| Epoch [  8/ 10] Iter[161/391]		Loss: 0.3138
| Epoch [  8/ 10] Iter[181/391]		Loss: 0.4279
| Epoch [  8/ 10] Iter[201/391]		Loss: 0.2791
| Epoch [  8/ 10] Iter[221/391]		Loss: 0.4610
| Epoch [  8/ 10] Iter[241/391]		Loss: 0.4035
| Epoch [  8/ 10] Iter[261/391]		Loss: 0.4375
| Epoch [  8/ 10] Iter[281/391]		Loss: 0.3536
| Epoch [  8/ 10] Iter[301/391]		Loss: 0.3392
| Epoch [  8/ 10] Iter[321/391]		Loss: 0.4078
| Epoch [  8/ 10] Iter[341/391]		Loss: 0.3885
| Epoch [  8/ 10] Iter[361/391]		Loss: 0.3903
| Epoch [  8/ 10] Iter[381/391]		Loss: 0.3575
=> Early Training Epoch #9
| Epoch [  9/ 10] Iter[  1/391]		Loss: 0.3013
| Epoch [  9/ 10] Iter[ 21/391]		Loss: 0.3948
| Epoch [  9/ 10] Iter[ 41/391]		Loss: 0.2252
| Epoch [  9/ 10] Iter[ 61/391]		Loss: 0.1578
| Epoch [  9/ 10] Iter[ 81/391]		Loss: 0.2183
| Epoch [  9/ 10] Iter[101/391]		Loss: 0.2938
| Epoch [  9/ 10] Iter[121/391]		Loss: 0.1993
| Epoch [  9/ 10] Iter[141/391]		Loss: 0.2715
| Epoch [  9/ 10] Iter[161/391]		Loss: 0.3566
| Epoch [  9/ 10] Iter[181/391]		Loss: 0.5289
| Epoch [  9/ 10] Iter[201/391]		Loss: 0.2395
| Epoch [  9/ 10] Iter[221/391]		Loss: 0.4398
| Epoch [  9/ 10] Iter[241/391]		Loss: 0.2852
| Epoch [  9/ 10] Iter[261/391]		Loss: 0.3419
| Epoch [  9/ 10] Iter[281/391]		Loss: 0.3753
| Epoch [  9/ 10] Iter[301/391]		Loss: 0.4131
| Epoch [  9/ 10] Iter[321/391]		Loss: 0.3403
| Epoch [  9/ 10] Iter[341/391]		Loss: 0.2955
| Epoch [  9/ 10] Iter[361/391]		Loss: 0.5138
| Epoch [  9/ 10] Iter[381/391]		Loss: 0.4697
=> Early Training Epoch #0
| Epoch [  0/ 10] Iter[  1/391]		Loss: 2.5367
| Epoch [  0/ 10] Iter[ 21/391]		Loss: 2.3241
| Epoch [  0/ 10] Iter[ 41/391]		Loss: 2.2449
| Epoch [  0/ 10] Iter[ 61/391]		Loss: 2.2271
| Epoch [  0/ 10] Iter[ 81/391]		Loss: 2.1220
| Epoch [  0/ 10] Iter[101/391]		Loss: 2.0313
| Epoch [  0/ 10] Iter[121/391]		Loss: 2.1739
| Epoch [  0/ 10] Iter[141/391]		Loss: 1.9517
| Epoch [  0/ 10] Iter[161/391]		Loss: 1.9061
| Epoch [  0/ 10] Iter[181/391]		Loss: 1.8813
| Epoch [  0/ 10] Iter[201/391]		Loss: 1.8204
| Epoch [  0/ 10] Iter[221/391]		Loss: 1.7719
| Epoch [  0/ 10] Iter[241/391]		Loss: 1.7664
| Epoch [  0/ 10] Iter[261/391]		Loss: 1.5888
| Epoch [  0/ 10] Iter[281/391]		Loss: 1.5754
| Epoch [  0/ 10] Iter[301/391]		Loss: 1.4746
| Epoch [  0/ 10] Iter[321/391]		Loss: 1.4218
| Epoch [  0/ 10] Iter[341/391]		Loss: 1.7524
| Epoch [  0/ 10] Iter[361/391]		Loss: 1.5673
| Epoch [  0/ 10] Iter[381/391]		Loss: 1.5529
=> Early Training Epoch #1
| Epoch [  1/ 10] Iter[  1/391]		Loss: 1.5020
| Epoch [  1/ 10] Iter[ 21/391]		Loss: 1.3331
| Epoch [  1/ 10] Iter[ 41/391]		Loss: 1.3758
| Epoch [  1/ 10] Iter[ 61/391]		Loss: 1.4279
| Epoch [  1/ 10] Iter[ 81/391]		Loss: 1.3819
| Epoch [  1/ 10] Iter[101/391]		Loss: 1.4571
| Epoch [  1/ 10] Iter[121/391]		Loss: 1.3845
| Epoch [  1/ 10] Iter[141/391]		Loss: 1.2865
| Epoch [  1/ 10] Iter[161/391]		Loss: 1.3042
| Epoch [  1/ 10] Iter[181/391]		Loss: 1.3345
| Epoch [  1/ 10] Iter[201/391]		Loss: 1.4746
| Epoch [  1/ 10] Iter[221/391]		Loss: 1.3418
| Epoch [  1/ 10] Iter[241/391]		Loss: 1.2709
| Epoch [  1/ 10] Iter[261/391]		Loss: 1.3302
| Epoch [  1/ 10] Iter[281/391]		Loss: 1.2115
| Epoch [  1/ 10] Iter[301/391]		Loss: 1.1620
| Epoch [  1/ 10] Iter[321/391]		Loss: 1.1750
| Epoch [  1/ 10] Iter[341/391]		Loss: 1.1678
| Epoch [  1/ 10] Iter[361/391]		Loss: 1.2001
| Epoch [  1/ 10] Iter[381/391]		Loss: 0.9823
=> Early Training Epoch #2
| Epoch [  2/ 10] Iter[  1/391]		Loss: 1.0585
| Epoch [  2/ 10] Iter[ 21/391]		Loss: 1.2678
| Epoch [  2/ 10] Iter[ 41/391]		Loss: 1.0273
| Epoch [  2/ 10] Iter[ 61/391]		Loss: 1.2220
| Epoch [  2/ 10] Iter[ 81/391]		Loss: 1.0455
| Epoch [  2/ 10] Iter[101/391]		Loss: 1.1573
| Epoch [  2/ 10] Iter[121/391]		Loss: 0.9062
| Epoch [  2/ 10] Iter[141/391]		Loss: 1.0167
| Epoch [  2/ 10] Iter[161/391]		Loss: 1.0755
| Epoch [  2/ 10] Iter[181/391]		Loss: 0.9539
| Epoch [  2/ 10] Iter[201/391]		Loss: 0.8957
| Epoch [  2/ 10] Iter[221/391]		Loss: 1.0825
| Epoch [  2/ 10] Iter[241/391]		Loss: 0.8357
| Epoch [  2/ 10] Iter[261/391]		Loss: 0.9418
| Epoch [  2/ 10] Iter[281/391]		Loss: 0.9953
| Epoch [  2/ 10] Iter[301/391]		Loss: 0.9489
| Epoch [  2/ 10] Iter[321/391]		Loss: 0.7931
| Epoch [  2/ 10] Iter[341/391]		Loss: 1.1055
| Epoch [  2/ 10] Iter[361/391]		Loss: 1.0625
| Epoch [  2/ 10] Iter[381/391]		Loss: 0.8143
=> Early Training Epoch #3
| Epoch [  3/ 10] Iter[  1/391]		Loss: 0.8003
| Epoch [  3/ 10] Iter[ 21/391]		Loss: 0.7041
| Epoch [  3/ 10] Iter[ 41/391]		Loss: 0.9718
| Epoch [  3/ 10] Iter[ 61/391]		Loss: 0.8456
| Epoch [  3/ 10] Iter[ 81/391]		Loss: 0.8447
| Epoch [  3/ 10] Iter[101/391]		Loss: 0.7057
| Epoch [  3/ 10] Iter[121/391]		Loss: 0.8535
| Epoch [  3/ 10] Iter[141/391]		Loss: 0.8052
| Epoch [  3/ 10] Iter[161/391]		Loss: 0.7246
| Epoch [  3/ 10] Iter[181/391]		Loss: 0.7838
| Epoch [  3/ 10] Iter[201/391]		Loss: 0.8997
| Epoch [  3/ 10] Iter[221/391]		Loss: 0.6670
| Epoch [  3/ 10] Iter[241/391]		Loss: 0.8229
| Epoch [  3/ 10] Iter[261/391]		Loss: 0.8889
| Epoch [  3/ 10] Iter[281/391]		Loss: 0.8254
| Epoch [  3/ 10] Iter[301/391]		Loss: 0.8926
| Epoch [  3/ 10] Iter[321/391]		Loss: 0.7985
| Epoch [  3/ 10] Iter[341/391]		Loss: 0.7530
| Epoch [  3/ 10] Iter[361/391]		Loss: 0.7013
| Epoch [  3/ 10] Iter[381/391]		Loss: 0.5685
=> Early Training Epoch #4
| Epoch [  4/ 10] Iter[  1/391]		Loss: 0.7937
| Epoch [  4/ 10] Iter[ 21/391]		Loss: 0.6772
| Epoch [  4/ 10] Iter[ 41/391]		Loss: 0.5384
| Epoch [  4/ 10] Iter[ 61/391]		Loss: 0.7078
| Epoch [  4/ 10] Iter[ 81/391]		Loss: 0.5990
| Epoch [  4/ 10] Iter[101/391]		Loss: 0.6977
| Epoch [  4/ 10] Iter[121/391]		Loss: 0.6885
| Epoch [  4/ 10] Iter[141/391]		Loss: 0.5882
| Epoch [  4/ 10] Iter[161/391]		Loss: 0.8531
| Epoch [  4/ 10] Iter[181/391]		Loss: 0.7549
| Epoch [  4/ 10] Iter[201/391]		Loss: 0.6776
| Epoch [  4/ 10] Iter[221/391]		Loss: 0.5929
| Epoch [  4/ 10] Iter[241/391]		Loss: 0.4818
| Epoch [  4/ 10] Iter[261/391]		Loss: 0.5215
| Epoch [  4/ 10] Iter[281/391]		Loss: 0.6227
| Epoch [  4/ 10] Iter[301/391]		Loss: 0.6508
| Epoch [  4/ 10] Iter[321/391]		Loss: 0.7010
| Epoch [  4/ 10] Iter[341/391]		Loss: 0.6494
| Epoch [  4/ 10] Iter[361/391]		Loss: 0.6425
| Epoch [  4/ 10] Iter[381/391]		Loss: 0.4648
=> Early Training Epoch #5
| Epoch [  5/ 10] Iter[  1/391]		Loss: 0.4943
| Epoch [  5/ 10] Iter[ 21/391]		Loss: 0.4310
| Epoch [  5/ 10] Iter[ 41/391]		Loss: 0.4966
| Epoch [  5/ 10] Iter[ 61/391]		Loss: 0.5019
| Epoch [  5/ 10] Iter[ 81/391]		Loss: 0.4630
| Epoch [  5/ 10] Iter[101/391]		Loss: 0.5771
| Epoch [  5/ 10] Iter[121/391]		Loss: 0.4904
| Epoch [  5/ 10] Iter[141/391]		Loss: 0.6079
| Epoch [  5/ 10] Iter[161/391]		Loss: 0.4478
| Epoch [  5/ 10] Iter[181/391]		Loss: 0.5027
| Epoch [  5/ 10] Iter[201/391]		Loss: 0.7841
| Epoch [  5/ 10] Iter[221/391]		Loss: 0.5051
| Epoch [  5/ 10] Iter[241/391]		Loss: 0.4987
| Epoch [  5/ 10] Iter[261/391]		Loss: 0.4617
| Epoch [  5/ 10] Iter[281/391]		Loss: 0.7461
| Epoch [  5/ 10] Iter[301/391]		Loss: 0.4532
| Epoch [  5/ 10] Iter[321/391]		Loss: 0.4111
| Epoch [  5/ 10] Iter[341/391]		Loss: 0.4508
| Epoch [  5/ 10] Iter[361/391]		Loss: 0.4991
| Epoch [  5/ 10] Iter[381/391]		Loss: 0.3816
=> Early Training Epoch #6
| Epoch [  6/ 10] Iter[  1/391]		Loss: 0.3120
| Epoch [  6/ 10] Iter[ 21/391]		Loss: 0.3678
| Epoch [  6/ 10] Iter[ 41/391]		Loss: 0.4549
| Epoch [  6/ 10] Iter[ 61/391]		Loss: 0.5361
| Epoch [  6/ 10] Iter[ 81/391]		Loss: 0.5054
| Epoch [  6/ 10] Iter[101/391]		Loss: 0.3690
| Epoch [  6/ 10] Iter[121/391]		Loss: 0.3929
| Epoch [  6/ 10] Iter[141/391]		Loss: 0.5710
| Epoch [  6/ 10] Iter[161/391]		Loss: 0.4052
| Epoch [  6/ 10] Iter[181/391]		Loss: 0.3724
| Epoch [  6/ 10] Iter[201/391]		Loss: 0.4810
| Epoch [  6/ 10] Iter[221/391]		Loss: 0.5720
| Epoch [  6/ 10] Iter[241/391]		Loss: 0.4271
| Epoch [  6/ 10] Iter[261/391]		Loss: 0.4952
| Epoch [  6/ 10] Iter[281/391]		Loss: 0.5538
| Epoch [  6/ 10] Iter[301/391]		Loss: 0.5340
| Epoch [  6/ 10] Iter[321/391]		Loss: 0.5604
| Epoch [  6/ 10] Iter[341/391]		Loss: 0.6579
| Epoch [  6/ 10] Iter[361/391]		Loss: 0.4700
| Epoch [  6/ 10] Iter[381/391]		Loss: 0.4129
=> Early Training Epoch #7
| Epoch [  7/ 10] Iter[  1/391]		Loss: 0.3144
| Epoch [  7/ 10] Iter[ 21/391]		Loss: 0.3123
| Epoch [  7/ 10] Iter[ 41/391]		Loss: 0.2660
| Epoch [  7/ 10] Iter[ 61/391]		Loss: 0.3482
| Epoch [  7/ 10] Iter[ 81/391]		Loss: 0.4223
| Epoch [  7/ 10] Iter[101/391]		Loss: 0.3118
| Epoch [  7/ 10] Iter[121/391]		Loss: 0.3417
| Epoch [  7/ 10] Iter[141/391]		Loss: 0.4158
| Epoch [  7/ 10] Iter[161/391]		Loss: 0.4932
| Epoch [  7/ 10] Iter[181/391]		Loss: 0.4591
| Epoch [  7/ 10] Iter[201/391]		Loss: 0.4306
| Epoch [  7/ 10] Iter[221/391]		Loss: 0.2456
| Epoch [  7/ 10] Iter[241/391]		Loss: 0.4001
| Epoch [  7/ 10] Iter[261/391]		Loss: 0.5902
| Epoch [  7/ 10] Iter[281/391]		Loss: 0.4959
| Epoch [  7/ 10] Iter[301/391]		Loss: 0.4828
| Epoch [  7/ 10] Iter[321/391]		Loss: 0.3168
| Epoch [  7/ 10] Iter[341/391]		Loss: 0.3136
| Epoch [  7/ 10] Iter[361/391]		Loss: 0.3755
| Epoch [  7/ 10] Iter[381/391]		Loss: 0.3339
=> Early Training Epoch #8
| Epoch [  8/ 10] Iter[  1/391]		Loss: 0.2704
| Epoch [  8/ 10] Iter[ 21/391]		Loss: 0.3217
| Epoch [  8/ 10] Iter[ 41/391]		Loss: 0.1754
| Epoch [  8/ 10] Iter[ 61/391]		Loss: 0.3468
| Epoch [  8/ 10] Iter[ 81/391]		Loss: 0.2467
| Epoch [  8/ 10] Iter[101/391]		Loss: 0.2621
| Epoch [  8/ 10] Iter[121/391]		Loss: 0.3976
| Epoch [  8/ 10] Iter[141/391]		Loss: 0.2636
| Epoch [  8/ 10] Iter[161/391]		Loss: 0.2599
| Epoch [  8/ 10] Iter[181/391]		Loss: 0.2443
| Epoch [  8/ 10] Iter[201/391]		Loss: 0.3690
| Epoch [  8/ 10] Iter[221/391]		Loss: 0.2468
| Epoch [  8/ 10] Iter[241/391]		Loss: 0.2344
| Epoch [  8/ 10] Iter[261/391]		Loss: 0.3444
| Epoch [  8/ 10] Iter[281/391]		Loss: 0.3647
| Epoch [  8/ 10] Iter[301/391]		Loss: 0.5128
| Epoch [  8/ 10] Iter[321/391]		Loss: 0.3767
| Epoch [  8/ 10] Iter[341/391]		Loss: 0.3789
| Epoch [  8/ 10] Iter[361/391]		Loss: 0.3730
| Epoch [  8/ 10] Iter[381/391]		Loss: 0.3383
=> Early Training Epoch #9
| Epoch [  9/ 10] Iter[  1/391]		Loss: 0.1936
| Epoch [  9/ 10] Iter[ 21/391]		Loss: 0.2487
| Epoch [  9/ 10] Iter[ 41/391]		Loss: 0.1850
| Epoch [  9/ 10] Iter[ 61/391]		Loss: 0.2601
| Epoch [  9/ 10] Iter[ 81/391]		Loss: 0.2309
| Epoch [  9/ 10] Iter[101/391]		Loss: 0.3028
| Epoch [  9/ 10] Iter[121/391]		Loss: 0.2669
| Epoch [  9/ 10] Iter[141/391]		Loss: 0.3830
| Epoch [  9/ 10] Iter[161/391]		Loss: 0.4479
| Epoch [  9/ 10] Iter[181/391]		Loss: 0.2540
| Epoch [  9/ 10] Iter[201/391]		Loss: 0.3268
| Epoch [  9/ 10] Iter[221/391]		Loss: 0.3144
| Epoch [  9/ 10] Iter[241/391]		Loss: 0.3860
| Epoch [  9/ 10] Iter[261/391]		Loss: 0.2067
| Epoch [  9/ 10] Iter[281/391]		Loss: 0.3462
| Epoch [  9/ 10] Iter[301/391]		Loss: 0.3741
| Epoch [  9/ 10] Iter[321/391]		Loss: 0.3101
| Epoch [  9/ 10] Iter[341/391]		Loss: 0.2826
| Epoch [  9/ 10] Iter[361/391]		Loss: 0.3058
| Epoch [  9/ 10] Iter[381/391]		Loss: 0.3323
=> Early Training Epoch #0
| Epoch [  0/ 10] Iter[  1/391]		Loss: 2.4136
| Epoch [  0/ 10] Iter[ 21/391]		Loss: 3.9798
| Epoch [  0/ 10] Iter[ 41/391]		Loss: 2.0747
| Epoch [  0/ 10] Iter[ 61/391]		Loss: 1.9903
| Epoch [  0/ 10] Iter[ 81/391]		Loss: 2.1427
| Epoch [  0/ 10] Iter[101/391]		Loss: 1.8722
| Epoch [  0/ 10] Iter[121/391]		Loss: 1.9810
| Epoch [  0/ 10] Iter[141/391]		Loss: 1.7937
| Epoch [  0/ 10] Iter[161/391]		Loss: 1.8440
| Epoch [  0/ 10] Iter[181/391]		Loss: 1.7813
| Epoch [  0/ 10] Iter[201/391]		Loss: 1.6845
| Epoch [  0/ 10] Iter[221/391]		Loss: 1.5964
| Epoch [  0/ 10] Iter[241/391]		Loss: 1.5856
| Epoch [  0/ 10] Iter[261/391]		Loss: 1.6565
| Epoch [  0/ 10] Iter[281/391]		Loss: 1.5340
| Epoch [  0/ 10] Iter[301/391]		Loss: 1.6580
| Epoch [  0/ 10] Iter[321/391]		Loss: 1.3797
| Epoch [  0/ 10] Iter[341/391]		Loss: 1.6348
| Epoch [  0/ 10] Iter[361/391]		Loss: 1.5017
| Epoch [  0/ 10] Iter[381/391]		Loss: 1.4142
=> Early Training Epoch #1
| Epoch [  1/ 10] Iter[  1/391]		Loss: 1.6276
| Epoch [  1/ 10] Iter[ 21/391]		Loss: 1.3455
| Epoch [  1/ 10] Iter[ 41/391]		Loss: 1.3091
| Epoch [  1/ 10] Iter[ 61/391]		Loss: 1.3017
| Epoch [  1/ 10] Iter[ 81/391]		Loss: 1.4710
| Epoch [  1/ 10] Iter[101/391]		Loss: 1.3555
| Epoch [  1/ 10] Iter[121/391]		Loss: 1.5066
| Epoch [  1/ 10] Iter[141/391]		Loss: 1.2761
| Epoch [  1/ 10] Iter[161/391]		Loss: 1.4739
| Epoch [  1/ 10] Iter[181/391]		Loss: 1.3908
| Epoch [  1/ 10] Iter[201/391]		Loss: 1.3103
| Epoch [  1/ 10] Iter[221/391]		Loss: 1.3865
| Epoch [  1/ 10] Iter[241/391]		Loss: 1.3967
| Epoch [  1/ 10] Iter[261/391]		Loss: 1.3128
| Epoch [  1/ 10] Iter[281/391]		Loss: 1.0545
| Epoch [  1/ 10] Iter[301/391]		Loss: 1.0138
| Epoch [  1/ 10] Iter[321/391]		Loss: 1.3826
| Epoch [  1/ 10] Iter[341/391]		Loss: 1.1109
| Epoch [  1/ 10] Iter[361/391]		Loss: 1.0761
| Epoch [  1/ 10] Iter[381/391]		Loss: 1.0946
=> Early Training Epoch #2
| Epoch [  2/ 10] Iter[  1/391]		Loss: 1.1905
| Epoch [  2/ 10] Iter[ 21/391]		Loss: 1.1034
| Epoch [  2/ 10] Iter[ 41/391]		Loss: 1.1535
| Epoch [  2/ 10] Iter[ 61/391]		Loss: 0.9778
| Epoch [  2/ 10] Iter[ 81/391]		Loss: 0.9116
| Epoch [  2/ 10] Iter[101/391]		Loss: 1.1753
| Epoch [  2/ 10] Iter[121/391]		Loss: 1.0762
| Epoch [  2/ 10] Iter[141/391]		Loss: 1.0003
| Epoch [  2/ 10] Iter[161/391]		Loss: 1.0871
| Epoch [  2/ 10] Iter[181/391]		Loss: 0.9380
| Epoch [  2/ 10] Iter[201/391]		Loss: 1.0266
| Epoch [  2/ 10] Iter[221/391]		Loss: 0.9203
| Epoch [  2/ 10] Iter[241/391]		Loss: 1.0296
| Epoch [  2/ 10] Iter[261/391]		Loss: 1.0257
| Epoch [  2/ 10] Iter[281/391]		Loss: 0.9865
| Epoch [  2/ 10] Iter[301/391]		Loss: 0.9122
| Epoch [  2/ 10] Iter[321/391]		Loss: 0.8928
| Epoch [  2/ 10] Iter[341/391]		Loss: 0.7987
| Epoch [  2/ 10] Iter[361/391]		Loss: 1.1238
| Epoch [  2/ 10] Iter[381/391]		Loss: 0.8418
=> Early Training Epoch #3
| Epoch [  3/ 10] Iter[  1/391]		Loss: 0.8896
| Epoch [  3/ 10] Iter[ 21/391]		Loss: 0.9426
| Epoch [  3/ 10] Iter[ 41/391]		Loss: 0.7817
| Epoch [  3/ 10] Iter[ 61/391]		Loss: 0.8278
| Epoch [  3/ 10] Iter[ 81/391]		Loss: 0.9757
| Epoch [  3/ 10] Iter[101/391]		Loss: 0.8456
| Epoch [  3/ 10] Iter[121/391]		Loss: 0.9248
| Epoch [  3/ 10] Iter[141/391]		Loss: 0.7455
| Epoch [  3/ 10] Iter[161/391]		Loss: 0.6918
| Epoch [  3/ 10] Iter[181/391]		Loss: 0.8501
| Epoch [  3/ 10] Iter[201/391]		Loss: 0.7410
| Epoch [  3/ 10] Iter[221/391]		Loss: 0.6935
| Epoch [  3/ 10] Iter[241/391]		Loss: 0.9076
| Epoch [  3/ 10] Iter[261/391]		Loss: 0.6395
| Epoch [  3/ 10] Iter[281/391]		Loss: 0.9400
| Epoch [  3/ 10] Iter[301/391]		Loss: 0.7387
| Epoch [  3/ 10] Iter[321/391]		Loss: 0.7168
| Epoch [  3/ 10] Iter[341/391]		Loss: 0.8163
| Epoch [  3/ 10] Iter[361/391]		Loss: 0.7038
| Epoch [  3/ 10] Iter[381/391]		Loss: 0.8058
=> Early Training Epoch #4
| Epoch [  4/ 10] Iter[  1/391]		Loss: 0.7960
| Epoch [  4/ 10] Iter[ 21/391]		Loss: 0.5950
| Epoch [  4/ 10] Iter[ 41/391]		Loss: 0.8944
| Epoch [  4/ 10] Iter[ 61/391]		Loss: 0.5799
| Epoch [  4/ 10] Iter[ 81/391]		Loss: 0.5862
| Epoch [  4/ 10] Iter[101/391]		Loss: 0.5407
| Epoch [  4/ 10] Iter[121/391]		Loss: 0.6759
| Epoch [  4/ 10] Iter[141/391]		Loss: 0.7544
| Epoch [  4/ 10] Iter[161/391]		Loss: 0.7311
| Epoch [  4/ 10] Iter[181/391]		Loss: 0.6154
| Epoch [  4/ 10] Iter[201/391]		Loss: 0.4613
| Epoch [  4/ 10] Iter[221/391]		Loss: 0.6238
| Epoch [  4/ 10] Iter[241/391]		Loss: 0.5530
| Epoch [  4/ 10] Iter[261/391]		Loss: 0.6812
| Epoch [  4/ 10] Iter[281/391]		Loss: 0.5998
| Epoch [  4/ 10] Iter[301/391]		Loss: 0.5902
| Epoch [  4/ 10] Iter[321/391]		Loss: 0.6551
| Epoch [  4/ 10] Iter[341/391]		Loss: 0.6983
| Epoch [  4/ 10] Iter[361/391]		Loss: 0.6151
| Epoch [  4/ 10] Iter[381/391]		Loss: 0.7868
=> Early Training Epoch #5
| Epoch [  5/ 10] Iter[  1/391]		Loss: 0.5123
| Epoch [  5/ 10] Iter[ 21/391]		Loss: 0.6525
| Epoch [  5/ 10] Iter[ 41/391]		Loss: 0.4550
| Epoch [  5/ 10] Iter[ 61/391]		Loss: 0.4503
| Epoch [  5/ 10] Iter[ 81/391]		Loss: 0.4530
| Epoch [  5/ 10] Iter[101/391]		Loss: 0.7345
| Epoch [  5/ 10] Iter[121/391]		Loss: 0.4853
| Epoch [  5/ 10] Iter[141/391]		Loss: 0.5305
| Epoch [  5/ 10] Iter[161/391]		Loss: 0.5867
| Epoch [  5/ 10] Iter[181/391]		Loss: 0.5045
| Epoch [  5/ 10] Iter[201/391]		Loss: 0.7424
| Epoch [  5/ 10] Iter[221/391]		Loss: 0.4918
| Epoch [  5/ 10] Iter[241/391]		Loss: 0.6303
| Epoch [  5/ 10] Iter[261/391]		Loss: 0.5651
| Epoch [  5/ 10] Iter[281/391]		Loss: 0.5926
| Epoch [  5/ 10] Iter[301/391]		Loss: 0.5184
| Epoch [  5/ 10] Iter[321/391]		Loss: 0.5689
| Epoch [  5/ 10] Iter[341/391]		Loss: 0.5277
| Epoch [  5/ 10] Iter[361/391]		Loss: 0.5432
| Epoch [  5/ 10] Iter[381/391]		Loss: 0.6149
=> Early Training Epoch #6
| Epoch [  6/ 10] Iter[  1/391]		Loss: 0.3891
| Epoch [  6/ 10] Iter[ 21/391]		Loss: 0.3377
| Epoch [  6/ 10] Iter[ 41/391]		Loss: 0.3955
| Epoch [  6/ 10] Iter[ 61/391]		Loss: 0.4495
| Epoch [  6/ 10] Iter[ 81/391]		Loss: 0.3687
| Epoch [  6/ 10] Iter[101/391]		Loss: 0.5354
| Epoch [  6/ 10] Iter[121/391]		Loss: 0.3352
| Epoch [  6/ 10] Iter[141/391]		Loss: 0.4989
| Epoch [  6/ 10] Iter[161/391]		Loss: 0.4377
| Epoch [  6/ 10] Iter[181/391]		Loss: 0.5074
| Epoch [  6/ 10] Iter[201/391]		Loss: 0.4651
| Epoch [  6/ 10] Iter[221/391]		Loss: 0.4668
| Epoch [  6/ 10] Iter[241/391]		Loss: 0.4681
| Epoch [  6/ 10] Iter[261/391]		Loss: 0.6801
| Epoch [  6/ 10] Iter[281/391]		Loss: 0.5289
| Epoch [  6/ 10] Iter[301/391]		Loss: 0.4198
| Epoch [  6/ 10] Iter[321/391]		Loss: 0.6124
| Epoch [  6/ 10] Iter[341/391]		Loss: 0.4882
| Epoch [  6/ 10] Iter[361/391]		Loss: 0.3852
| Epoch [  6/ 10] Iter[381/391]		Loss: 0.3823
=> Early Training Epoch #7
| Epoch [  7/ 10] Iter[  1/391]		Loss: 0.2776
| Epoch [  7/ 10] Iter[ 21/391]		Loss: 0.2813
| Epoch [  7/ 10] Iter[ 41/391]		Loss: 0.4956
| Epoch [  7/ 10] Iter[ 61/391]		Loss: 0.3062
| Epoch [  7/ 10] Iter[ 81/391]		Loss: 0.4146
| Epoch [  7/ 10] Iter[101/391]		Loss: 0.3929
| Epoch [  7/ 10] Iter[121/391]		Loss: 0.4234
| Epoch [  7/ 10] Iter[141/391]		Loss: 0.4076
| Epoch [  7/ 10] Iter[161/391]		Loss: 0.4147
| Epoch [  7/ 10] Iter[181/391]		Loss: 0.4173
| Epoch [  7/ 10] Iter[201/391]		Loss: 0.4237
| Epoch [  7/ 10] Iter[221/391]		Loss: 0.3956
| Epoch [  7/ 10] Iter[241/391]		Loss: 0.3614
| Epoch [  7/ 10] Iter[261/391]		Loss: 0.5297
| Epoch [  7/ 10] Iter[281/391]		Loss: 0.4960
| Epoch [  7/ 10] Iter[301/391]		Loss: 0.4766
| Epoch [  7/ 10] Iter[321/391]		Loss: 0.3061
| Epoch [  7/ 10] Iter[341/391]		Loss: 0.3170
| Epoch [  7/ 10] Iter[361/391]		Loss: 0.3768
| Epoch [  7/ 10] Iter[381/391]		Loss: 0.2681
=> Early Training Epoch #8
| Epoch [  8/ 10] Iter[  1/391]		Loss: 0.3642
| Epoch [  8/ 10] Iter[ 21/391]		Loss: 0.1855
| Epoch [  8/ 10] Iter[ 41/391]		Loss: 0.1633
| Epoch [  8/ 10] Iter[ 61/391]		Loss: 0.2913
| Epoch [  8/ 10] Iter[ 81/391]		Loss: 0.2527
| Epoch [  8/ 10] Iter[101/391]		Loss: 0.3180
| Epoch [  8/ 10] Iter[121/391]		Loss: 0.3533
| Epoch [  8/ 10] Iter[141/391]		Loss: 0.3225
| Epoch [  8/ 10] Iter[161/391]		Loss: 0.2759
| Epoch [  8/ 10] Iter[181/391]		Loss: 0.4486
| Epoch [  8/ 10] Iter[201/391]		Loss: 0.4296
| Epoch [  8/ 10] Iter[221/391]		Loss: 0.5096
| Epoch [  8/ 10] Iter[241/391]		Loss: 0.3649
| Epoch [  8/ 10] Iter[261/391]		Loss: 0.3168
| Epoch [  8/ 10] Iter[281/391]		Loss: 0.4589
| Epoch [  8/ 10] Iter[301/391]		Loss: 0.5219
| Epoch [  8/ 10] Iter[321/391]		Loss: 0.3184
| Epoch [  8/ 10] Iter[341/391]		Loss: 0.3886
| Epoch [  8/ 10] Iter[361/391]		Loss: 0.2807
| Epoch [  8/ 10] Iter[381/391]		Loss: 0.4691
=> Early Training Epoch #9
| Epoch [  9/ 10] Iter[  1/391]		Loss: 0.2884
| Epoch [  9/ 10] Iter[ 21/391]		Loss: 0.3065
| Epoch [  9/ 10] Iter[ 41/391]		Loss: 0.1946
| Epoch [  9/ 10] Iter[ 61/391]		Loss: 0.2044
| Epoch [  9/ 10] Iter[ 81/391]		Loss: 0.2631
| Epoch [  9/ 10] Iter[101/391]		Loss: 0.3051
| Epoch [  9/ 10] Iter[121/391]		Loss: 0.3105
| Epoch [  9/ 10] Iter[141/391]		Loss: 0.4849
| Epoch [  9/ 10] Iter[161/391]		Loss: 0.3280
| Epoch [  9/ 10] Iter[181/391]		Loss: 0.2982
| Epoch [  9/ 10] Iter[201/391]		Loss: 0.3111
| Epoch [  9/ 10] Iter[221/391]		Loss: 0.3643
| Epoch [  9/ 10] Iter[241/391]		Loss: 0.4979
| Epoch [  9/ 10] Iter[261/391]		Loss: 0.2476
| Epoch [  9/ 10] Iter[281/391]		Loss: 0.4481
| Epoch [  9/ 10] Iter[301/391]		Loss: 0.3044
| Epoch [  9/ 10] Iter[321/391]		Loss: 0.2662
| Epoch [  9/ 10] Iter[341/391]		Loss: 0.3712
| Epoch [  9/ 10] Iter[361/391]		Loss: 0.2396
| Epoch [  9/ 10] Iter[381/391]		Loss: 0.4042
=> Early Training Epoch #0
| Epoch [  0/ 10] Iter[  1/391]		Loss: 2.2861
| Epoch [  0/ 10] Iter[ 21/391]		Loss: 4.9541
| Epoch [  0/ 10] Iter[ 41/391]		Loss: 2.3814
| Epoch [  0/ 10] Iter[ 61/391]		Loss: 2.6496
| Epoch [  0/ 10] Iter[ 81/391]		Loss: 2.0456
| Epoch [  0/ 10] Iter[101/391]		Loss: 1.9565
| Epoch [  0/ 10] Iter[121/391]		Loss: 1.9105
| Epoch [  0/ 10] Iter[141/391]		Loss: 1.8807
| Epoch [  0/ 10] Iter[161/391]		Loss: 1.8083
| Epoch [  0/ 10] Iter[181/391]		Loss: 1.7041
| Epoch [  0/ 10] Iter[201/391]		Loss: 1.7624
| Epoch [  0/ 10] Iter[221/391]		Loss: 1.8115
| Epoch [  0/ 10] Iter[241/391]		Loss: 1.7030
| Epoch [  0/ 10] Iter[261/391]		Loss: 1.6828
| Epoch [  0/ 10] Iter[281/391]		Loss: 1.6151
| Epoch [  0/ 10] Iter[301/391]		Loss: 1.7129
| Epoch [  0/ 10] Iter[321/391]		Loss: 1.5785
| Epoch [  0/ 10] Iter[341/391]		Loss: 1.4577
| Epoch [  0/ 10] Iter[361/391]		Loss: 1.5147
| Epoch [  0/ 10] Iter[381/391]		Loss: 1.4504
=> Early Training Epoch #1
| Epoch [  1/ 10] Iter[  1/391]		Loss: 1.5529
| Epoch [  1/ 10] Iter[ 21/391]		Loss: 1.4970
| Epoch [  1/ 10] Iter[ 41/391]		Loss: 1.3944
| Epoch [  1/ 10] Iter[ 61/391]		Loss: 1.3380
| Epoch [  1/ 10] Iter[ 81/391]		Loss: 1.3987
| Epoch [  1/ 10] Iter[101/391]		Loss: 1.4585
| Epoch [  1/ 10] Iter[121/391]		Loss: 1.5510
| Epoch [  1/ 10] Iter[141/391]		Loss: 1.3686
| Epoch [  1/ 10] Iter[161/391]		Loss: 1.3086
| Epoch [  1/ 10] Iter[181/391]		Loss: 1.2935
| Epoch [  1/ 10] Iter[201/391]		Loss: 1.4785
| Epoch [  1/ 10] Iter[221/391]		Loss: 1.1625
| Epoch [  1/ 10] Iter[241/391]		Loss: 1.2766
| Epoch [  1/ 10] Iter[261/391]		Loss: 1.1646
| Epoch [  1/ 10] Iter[281/391]		Loss: 1.3342
| Epoch [  1/ 10] Iter[301/391]		Loss: 1.2850
| Epoch [  1/ 10] Iter[321/391]		Loss: 1.2995
| Epoch [  1/ 10] Iter[341/391]		Loss: 1.3773
| Epoch [  1/ 10] Iter[361/391]		Loss: 1.2263
| Epoch [  1/ 10] Iter[381/391]		Loss: 1.1330
=> Early Training Epoch #2
| Epoch [  2/ 10] Iter[  1/391]		Loss: 1.1878
| Epoch [  2/ 10] Iter[ 21/391]		Loss: 1.1545
| Epoch [  2/ 10] Iter[ 41/391]		Loss: 1.3637
| Epoch [  2/ 10] Iter[ 61/391]		Loss: 1.1732
| Epoch [  2/ 10] Iter[ 81/391]		Loss: 1.2655
| Epoch [  2/ 10] Iter[101/391]		Loss: 0.9250
| Epoch [  2/ 10] Iter[121/391]		Loss: 1.0911
| Epoch [  2/ 10] Iter[141/391]		Loss: 1.1217
| Epoch [  2/ 10] Iter[161/391]		Loss: 1.0340
| Epoch [  2/ 10] Iter[181/391]		Loss: 1.2023
| Epoch [  2/ 10] Iter[201/391]		Loss: 1.0048
| Epoch [  2/ 10] Iter[221/391]		Loss: 0.9961
| Epoch [  2/ 10] Iter[241/391]		Loss: 1.0362
| Epoch [  2/ 10] Iter[261/391]		Loss: 0.8793
| Epoch [  2/ 10] Iter[281/391]		Loss: 1.0225
| Epoch [  2/ 10] Iter[301/391]		Loss: 1.1038
| Epoch [  2/ 10] Iter[321/391]		Loss: 0.9479
| Epoch [  2/ 10] Iter[341/391]		Loss: 1.1418
| Epoch [  2/ 10] Iter[361/391]		Loss: 0.8482
| Epoch [  2/ 10] Iter[381/391]		Loss: 0.9660
=> Early Training Epoch #3
| Epoch [  3/ 10] Iter[  1/391]		Loss: 0.9194
| Epoch [  3/ 10] Iter[ 21/391]		Loss: 0.8683
| Epoch [  3/ 10] Iter[ 41/391]		Loss: 0.9601
| Epoch [  3/ 10] Iter[ 61/391]		Loss: 1.0404
| Epoch [  3/ 10] Iter[ 81/391]		Loss: 0.8332
| Epoch [  3/ 10] Iter[101/391]		Loss: 0.8687
| Epoch [  3/ 10] Iter[121/391]		Loss: 1.0949
| Epoch [  3/ 10] Iter[141/391]		Loss: 0.8702
| Epoch [  3/ 10] Iter[161/391]		Loss: 0.8385
| Epoch [  3/ 10] Iter[181/391]		Loss: 0.7095
| Epoch [  3/ 10] Iter[201/391]		Loss: 0.9146
| Epoch [  3/ 10] Iter[221/391]		Loss: 0.9545
| Epoch [  3/ 10] Iter[241/391]		Loss: 0.8476
| Epoch [  3/ 10] Iter[261/391]		Loss: 0.6373
| Epoch [  3/ 10] Iter[281/391]		Loss: 0.7773
| Epoch [  3/ 10] Iter[301/391]		Loss: 0.8310
| Epoch [  3/ 10] Iter[321/391]		Loss: 0.8560
| Epoch [  3/ 10] Iter[341/391]		Loss: 0.9457
| Epoch [  3/ 10] Iter[361/391]		Loss: 0.8517
| Epoch [  3/ 10] Iter[381/391]		Loss: 0.7123
=> Early Training Epoch #4
| Epoch [  4/ 10] Iter[  1/391]		Loss: 0.6431
| Epoch [  4/ 10] Iter[ 21/391]		Loss: 0.7319
| Epoch [  4/ 10] Iter[ 41/391]		Loss: 0.6889
| Epoch [  4/ 10] Iter[ 61/391]		Loss: 0.7077
| Epoch [  4/ 10] Iter[ 81/391]		Loss: 0.7674
| Epoch [  4/ 10] Iter[101/391]		Loss: 0.8879
| Epoch [  4/ 10] Iter[121/391]		Loss: 0.6948
| Epoch [  4/ 10] Iter[141/391]		Loss: 0.6038
| Epoch [  4/ 10] Iter[161/391]		Loss: 0.7493
| Epoch [  4/ 10] Iter[181/391]		Loss: 0.7869
| Epoch [  4/ 10] Iter[201/391]		Loss: 0.7857
| Epoch [  4/ 10] Iter[221/391]		Loss: 0.8493
| Epoch [  4/ 10] Iter[241/391]		Loss: 0.6992
| Epoch [  4/ 10] Iter[261/391]		Loss: 0.6862
| Epoch [  4/ 10] Iter[281/391]		Loss: 0.6481
| Epoch [  4/ 10] Iter[301/391]		Loss: 0.7418
| Epoch [  4/ 10] Iter[321/391]		Loss: 0.6620
| Epoch [  4/ 10] Iter[341/391]		Loss: 0.6701
| Epoch [  4/ 10] Iter[361/391]		Loss: 0.7234
| Epoch [  4/ 10] Iter[381/391]		Loss: 0.7229
=> Early Training Epoch #5
| Epoch [  5/ 10] Iter[  1/391]		Loss: 0.5821
| Epoch [  5/ 10] Iter[ 21/391]		Loss: 0.6752
| Epoch [  5/ 10] Iter[ 41/391]		Loss: 0.5492
| Epoch [  5/ 10] Iter[ 61/391]		Loss: 0.5471
| Epoch [  5/ 10] Iter[ 81/391]		Loss: 0.7717
| Epoch [  5/ 10] Iter[101/391]		Loss: 0.6524
| Epoch [  5/ 10] Iter[121/391]		Loss: 0.6463
| Epoch [  5/ 10] Iter[141/391]		Loss: 0.6208
| Epoch [  5/ 10] Iter[161/391]		Loss: 0.6504
| Epoch [  5/ 10] Iter[181/391]		Loss: 0.5786
| Epoch [  5/ 10] Iter[201/391]		Loss: 0.6172
| Epoch [  5/ 10] Iter[221/391]		Loss: 0.7709
| Epoch [  5/ 10] Iter[241/391]		Loss: 0.5224
| Epoch [  5/ 10] Iter[261/391]		Loss: 0.5642
| Epoch [  5/ 10] Iter[281/391]		Loss: 0.7301
| Epoch [  5/ 10] Iter[301/391]		Loss: 0.6121
| Epoch [  5/ 10] Iter[321/391]		Loss: 0.6126
| Epoch [  5/ 10] Iter[341/391]		Loss: 0.7069
| Epoch [  5/ 10] Iter[361/391]		Loss: 0.6321
| Epoch [  5/ 10] Iter[381/391]		Loss: 0.7074
=> Early Training Epoch #6
| Epoch [  6/ 10] Iter[  1/391]		Loss: 0.5928
| Epoch [  6/ 10] Iter[ 21/391]		Loss: 0.5190
| Epoch [  6/ 10] Iter[ 41/391]		Loss: 0.4113
| Epoch [  6/ 10] Iter[ 61/391]		Loss: 0.5894
| Epoch [  6/ 10] Iter[ 81/391]		Loss: 0.4321
| Epoch [  6/ 10] Iter[101/391]		Loss: 0.5191
| Epoch [  6/ 10] Iter[121/391]		Loss: 0.5353
| Epoch [  6/ 10] Iter[141/391]		Loss: 0.4779
| Epoch [  6/ 10] Iter[161/391]		Loss: 0.5017
| Epoch [  6/ 10] Iter[181/391]		Loss: 0.4992
| Epoch [  6/ 10] Iter[201/391]		Loss: 0.5633
| Epoch [  6/ 10] Iter[221/391]		Loss: 0.4693
| Epoch [  6/ 10] Iter[241/391]		Loss: 0.5808
| Epoch [  6/ 10] Iter[261/391]		Loss: 0.4483
| Epoch [  6/ 10] Iter[281/391]		Loss: 0.3488
| Epoch [  6/ 10] Iter[301/391]		Loss: 0.5221
| Epoch [  6/ 10] Iter[321/391]		Loss: 0.5379
| Epoch [  6/ 10] Iter[341/391]		Loss: 0.5752
| Epoch [  6/ 10] Iter[361/391]		Loss: 0.6494
| Epoch [  6/ 10] Iter[381/391]		Loss: 0.5516
=> Early Training Epoch #7
| Epoch [  7/ 10] Iter[  1/391]		Loss: 0.4152
| Epoch [  7/ 10] Iter[ 21/391]		Loss: 0.4633
| Epoch [  7/ 10] Iter[ 41/391]		Loss: 0.2951
| Epoch [  7/ 10] Iter[ 61/391]		Loss: 0.3888
| Epoch [  7/ 10] Iter[ 81/391]		Loss: 0.5007
| Epoch [  7/ 10] Iter[101/391]		Loss: 0.4967
| Epoch [  7/ 10] Iter[121/391]		Loss: 0.3433
| Epoch [  7/ 10] Iter[141/391]		Loss: 0.2984
| Epoch [  7/ 10] Iter[161/391]		Loss: 0.4833
| Epoch [  7/ 10] Iter[181/391]		Loss: 0.3123
| Epoch [  7/ 10] Iter[201/391]		Loss: 0.4263
| Epoch [  7/ 10] Iter[221/391]		Loss: 0.4200
| Epoch [  7/ 10] Iter[241/391]		Loss: 0.3848
| Epoch [  7/ 10] Iter[261/391]		Loss: 0.5181
| Epoch [  7/ 10] Iter[281/391]		Loss: 0.3986
| Epoch [  7/ 10] Iter[301/391]		Loss: 0.3646
| Epoch [  7/ 10] Iter[321/391]		Loss: 0.4502
| Epoch [  7/ 10] Iter[341/391]		Loss: 0.4982
| Epoch [  7/ 10] Iter[361/391]		Loss: 0.4420
| Epoch [  7/ 10] Iter[381/391]		Loss: 0.5541
=> Early Training Epoch #8
| Epoch [  8/ 10] Iter[  1/391]		Loss: 0.4472
| Epoch [  8/ 10] Iter[ 21/391]		Loss: 0.2535
| Epoch [  8/ 10] Iter[ 41/391]		Loss: 0.3628
| Epoch [  8/ 10] Iter[ 61/391]		Loss: 0.2851
| Epoch [  8/ 10] Iter[ 81/391]		Loss: 0.2627
| Epoch [  8/ 10] Iter[101/391]		Loss: 0.3799
| Epoch [  8/ 10] Iter[121/391]		Loss: 0.3363
| Epoch [  8/ 10] Iter[141/391]		Loss: 0.3460
| Epoch [  8/ 10] Iter[161/391]		Loss: 0.2869
| Epoch [  8/ 10] Iter[181/391]		Loss: 0.3285
| Epoch [  8/ 10] Iter[201/391]		Loss: 0.4353
| Epoch [  8/ 10] Iter[221/391]		Loss: 0.3661
| Epoch [  8/ 10] Iter[241/391]		Loss: 0.4873
| Epoch [  8/ 10] Iter[261/391]		Loss: 0.4435
| Epoch [  8/ 10] Iter[281/391]		Loss: 0.2849
| Epoch [  8/ 10] Iter[301/391]		Loss: 0.4441
| Epoch [  8/ 10] Iter[321/391]		Loss: 0.3632
| Epoch [  8/ 10] Iter[341/391]		Loss: 0.3934
| Epoch [  8/ 10] Iter[361/391]		Loss: 0.4831
| Epoch [  8/ 10] Iter[381/391]		Loss: 0.3622
=> Early Training Epoch #9
| Epoch [  9/ 10] Iter[  1/391]		Loss: 0.4526
| Epoch [  9/ 10] Iter[ 21/391]		Loss: 0.2229
| Epoch [  9/ 10] Iter[ 41/391]		Loss: 0.1797
| Epoch [  9/ 10] Iter[ 61/391]		Loss: 0.2465
| Epoch [  9/ 10] Iter[ 81/391]		Loss: 0.2454
| Epoch [  9/ 10] Iter[101/391]		Loss: 0.3089
| Epoch [  9/ 10] Iter[121/391]		Loss: 0.2000
| Epoch [  9/ 10] Iter[141/391]		Loss: 0.2552
| Epoch [  9/ 10] Iter[161/391]		Loss: 0.2769
| Epoch [  9/ 10] Iter[181/391]		Loss: 0.2549
| Epoch [  9/ 10] Iter[201/391]		Loss: 0.2535
| Epoch [  9/ 10] Iter[221/391]		Loss: 0.4619
| Epoch [  9/ 10] Iter[241/391]		Loss: 0.3657
| Epoch [  9/ 10] Iter[261/391]		Loss: 0.3385
| Epoch [  9/ 10] Iter[281/391]		Loss: 0.2505
| Epoch [  9/ 10] Iter[301/391]		Loss: 0.3968
| Epoch [  9/ 10] Iter[321/391]		Loss: 0.2566
| Epoch [  9/ 10] Iter[341/391]		Loss: 0.2853
| Epoch [  9/ 10] Iter[361/391]		Loss: 0.3206
| Epoch [  9/ 10] Iter[381/391]		Loss: 0.3789
=> Early Training Epoch #0
| Epoch [  0/ 10] Iter[  1/391]		Loss: 2.6079
| Epoch [  0/ 10] Iter[ 21/391]		Loss: 2.7382
| Epoch [  0/ 10] Iter[ 41/391]		Loss: 2.2627
| Epoch [  0/ 10] Iter[ 61/391]		Loss: 2.2142
| Epoch [  0/ 10] Iter[ 81/391]		Loss: 2.2053
| Epoch [  0/ 10] Iter[101/391]		Loss: 2.0584
| Epoch [  0/ 10] Iter[121/391]		Loss: 1.9664
| Epoch [  0/ 10] Iter[141/391]		Loss: 2.0491
| Epoch [  0/ 10] Iter[161/391]		Loss: 1.9576
| Epoch [  0/ 10] Iter[181/391]		Loss: 1.9217
| Epoch [  0/ 10] Iter[201/391]		Loss: 1.9727
| Epoch [  0/ 10] Iter[221/391]		Loss: 1.9328
| Epoch [  0/ 10] Iter[241/391]		Loss: 1.7762
| Epoch [  0/ 10] Iter[261/391]		Loss: 1.6638
| Epoch [  0/ 10] Iter[281/391]		Loss: 1.5916
| Epoch [  0/ 10] Iter[301/391]		Loss: 1.6676
| Epoch [  0/ 10] Iter[321/391]		Loss: 1.6897
| Epoch [  0/ 10] Iter[341/391]		Loss: 1.6946
| Epoch [  0/ 10] Iter[361/391]		Loss: 1.4525
| Epoch [  0/ 10] Iter[381/391]		Loss: 1.6627
=> Early Training Epoch #1
| Epoch [  1/ 10] Iter[  1/391]		Loss: 1.6147
| Epoch [  1/ 10] Iter[ 21/391]		Loss: 1.5075
| Epoch [  1/ 10] Iter[ 41/391]		Loss: 1.4379
| Epoch [  1/ 10] Iter[ 61/391]		Loss: 1.3704
| Epoch [  1/ 10] Iter[ 81/391]		Loss: 1.5938
| Epoch [  1/ 10] Iter[101/391]		Loss: 1.3431
| Epoch [  1/ 10] Iter[121/391]		Loss: 1.4122
| Epoch [  1/ 10] Iter[141/391]		Loss: 1.5454
| Epoch [  1/ 10] Iter[161/391]		Loss: 1.2193
| Epoch [  1/ 10] Iter[181/391]		Loss: 1.5593
| Epoch [  1/ 10] Iter[201/391]		Loss: 1.4379
| Epoch [  1/ 10] Iter[221/391]		Loss: 1.3960
| Epoch [  1/ 10] Iter[241/391]		Loss: 1.2924
| Epoch [  1/ 10] Iter[261/391]		Loss: 1.3645
| Epoch [  1/ 10] Iter[281/391]		Loss: 1.1023
| Epoch [  1/ 10] Iter[301/391]		Loss: 1.3109
| Epoch [  1/ 10] Iter[321/391]		Loss: 1.4608
| Epoch [  1/ 10] Iter[341/391]		Loss: 1.1959
| Epoch [  1/ 10] Iter[361/391]		Loss: 1.1564
| Epoch [  1/ 10] Iter[381/391]		Loss: 1.1200
=> Early Training Epoch #2
| Epoch [  2/ 10] Iter[  1/391]		Loss: 1.2317
| Epoch [  2/ 10] Iter[ 21/391]		Loss: 1.2256
| Epoch [  2/ 10] Iter[ 41/391]		Loss: 1.1547
| Epoch [  2/ 10] Iter[ 61/391]		Loss: 1.1798
| Epoch [  2/ 10] Iter[ 81/391]		Loss: 1.1509
| Epoch [  2/ 10] Iter[101/391]		Loss: 1.0704
| Epoch [  2/ 10] Iter[121/391]		Loss: 1.0769
| Epoch [  2/ 10] Iter[141/391]		Loss: 1.0066
| Epoch [  2/ 10] Iter[161/391]		Loss: 0.9926
| Epoch [  2/ 10] Iter[181/391]		Loss: 0.9429
| Epoch [  2/ 10] Iter[201/391]		Loss: 1.0040
| Epoch [  2/ 10] Iter[221/391]		Loss: 1.1522
| Epoch [  2/ 10] Iter[241/391]		Loss: 0.9348
| Epoch [  2/ 10] Iter[261/391]		Loss: 0.9789
| Epoch [  2/ 10] Iter[281/391]		Loss: 0.9869
| Epoch [  2/ 10] Iter[301/391]		Loss: 1.0232
| Epoch [  2/ 10] Iter[321/391]		Loss: 1.0396
| Epoch [  2/ 10] Iter[341/391]		Loss: 0.8909
| Epoch [  2/ 10] Iter[361/391]		Loss: 1.0174
| Epoch [  2/ 10] Iter[381/391]		Loss: 1.0407
=> Early Training Epoch #3
| Epoch [  3/ 10] Iter[  1/391]		Loss: 0.9079
| Epoch [  3/ 10] Iter[ 21/391]		Loss: 0.7697
| Epoch [  3/ 10] Iter[ 41/391]		Loss: 0.8838
| Epoch [  3/ 10] Iter[ 61/391]		Loss: 0.9951
| Epoch [  3/ 10] Iter[ 81/391]		Loss: 0.8831
| Epoch [  3/ 10] Iter[101/391]		Loss: 0.8954
| Epoch [  3/ 10] Iter[121/391]		Loss: 0.8255
| Epoch [  3/ 10] Iter[141/391]		Loss: 0.9365
| Epoch [  3/ 10] Iter[161/391]		Loss: 1.0276
| Epoch [  3/ 10] Iter[181/391]		Loss: 0.9857
| Epoch [  3/ 10] Iter[201/391]		Loss: 0.9946
| Epoch [  3/ 10] Iter[221/391]		Loss: 0.8943
| Epoch [  3/ 10] Iter[241/391]		Loss: 0.7280
| Epoch [  3/ 10] Iter[261/391]		Loss: 0.7713
| Epoch [  3/ 10] Iter[281/391]		Loss: 0.9039
| Epoch [  3/ 10] Iter[301/391]		Loss: 0.8241
| Epoch [  3/ 10] Iter[321/391]		Loss: 0.8353
| Epoch [  3/ 10] Iter[341/391]		Loss: 0.8546
| Epoch [  3/ 10] Iter[361/391]		Loss: 0.7137
| Epoch [  3/ 10] Iter[381/391]		Loss: 0.6839
=> Early Training Epoch #4
| Epoch [  4/ 10] Iter[  1/391]		Loss: 0.6914
| Epoch [  4/ 10] Iter[ 21/391]		Loss: 0.6767
| Epoch [  4/ 10] Iter[ 41/391]		Loss: 0.7394
| Epoch [  4/ 10] Iter[ 61/391]		Loss: 0.7767
| Epoch [  4/ 10] Iter[ 81/391]		Loss: 0.7470
| Epoch [  4/ 10] Iter[101/391]		Loss: 0.6725
| Epoch [  4/ 10] Iter[121/391]		Loss: 0.6965
| Epoch [  4/ 10] Iter[141/391]		Loss: 0.6869
| Epoch [  4/ 10] Iter[161/391]		Loss: 0.6502
| Epoch [  4/ 10] Iter[181/391]		Loss: 0.6856
| Epoch [  4/ 10] Iter[201/391]		Loss: 0.8268
| Epoch [  4/ 10] Iter[221/391]		Loss: 0.7097
| Epoch [  4/ 10] Iter[241/391]		Loss: 0.5738
| Epoch [  4/ 10] Iter[261/391]		Loss: 0.7173
| Epoch [  4/ 10] Iter[281/391]		Loss: 0.7737
| Epoch [  4/ 10] Iter[301/391]		Loss: 0.7212
| Epoch [  4/ 10] Iter[321/391]		Loss: 0.7606
| Epoch [  4/ 10] Iter[341/391]		Loss: 0.7400
| Epoch [  4/ 10] Iter[361/391]		Loss: 0.6765
| Epoch [  4/ 10] Iter[381/391]		Loss: 0.6276
=> Early Training Epoch #5
| Epoch [  5/ 10] Iter[  1/391]		Loss: 0.6689
| Epoch [  5/ 10] Iter[ 21/391]		Loss: 0.5792
| Epoch [  5/ 10] Iter[ 41/391]		Loss: 0.6132
| Epoch [  5/ 10] Iter[ 61/391]		Loss: 0.5363
| Epoch [  5/ 10] Iter[ 81/391]		Loss: 0.6418
| Epoch [  5/ 10] Iter[101/391]		Loss: 0.5558
| Epoch [  5/ 10] Iter[121/391]		Loss: 0.5315
| Epoch [  5/ 10] Iter[141/391]		Loss: 0.6256
| Epoch [  5/ 10] Iter[161/391]		Loss: 0.6337
| Epoch [  5/ 10] Iter[181/391]		Loss: 0.5489
| Epoch [  5/ 10] Iter[201/391]		Loss: 0.6521
| Epoch [  5/ 10] Iter[221/391]		Loss: 0.8192
| Epoch [  5/ 10] Iter[241/391]		Loss: 0.5338
| Epoch [  5/ 10] Iter[261/391]		Loss: 0.5540
| Epoch [  5/ 10] Iter[281/391]		Loss: 0.5619
| Epoch [  5/ 10] Iter[301/391]		Loss: 0.5278
| Epoch [  5/ 10] Iter[321/391]		Loss: 0.5999
| Epoch [  5/ 10] Iter[341/391]		Loss: 0.6688
| Epoch [  5/ 10] Iter[361/391]		Loss: 0.6054
| Epoch [  5/ 10] Iter[381/391]		Loss: 0.7491
=> Early Training Epoch #6
| Epoch [  6/ 10] Iter[  1/391]		Loss: 0.5860
| Epoch [  6/ 10] Iter[ 21/391]		Loss: 0.5590
| Epoch [  6/ 10] Iter[ 41/391]		Loss: 0.4252
| Epoch [  6/ 10] Iter[ 61/391]		Loss: 0.5305
| Epoch [  6/ 10] Iter[ 81/391]		Loss: 0.4666
| Epoch [  6/ 10] Iter[101/391]		Loss: 0.3965
| Epoch [  6/ 10] Iter[121/391]		Loss: 0.3569
| Epoch [  6/ 10] Iter[141/391]		Loss: 0.5100
| Epoch [  6/ 10] Iter[161/391]		Loss: 0.6007
| Epoch [  6/ 10] Iter[181/391]		Loss: 0.4758
| Epoch [  6/ 10] Iter[201/391]		Loss: 0.3971
| Epoch [  6/ 10] Iter[221/391]		Loss: 0.4634
| Epoch [  6/ 10] Iter[241/391]		Loss: 0.4563
| Epoch [  6/ 10] Iter[261/391]		Loss: 0.5779
| Epoch [  6/ 10] Iter[281/391]		Loss: 0.6352
| Epoch [  6/ 10] Iter[301/391]		Loss: 0.3992
| Epoch [  6/ 10] Iter[321/391]		Loss: 0.4233
| Epoch [  6/ 10] Iter[341/391]		Loss: 0.5059
| Epoch [  6/ 10] Iter[361/391]		Loss: 0.4147
| Epoch [  6/ 10] Iter[381/391]		Loss: 0.5629
=> Early Training Epoch #7
| Epoch [  7/ 10] Iter[  1/391]		Loss: 0.4430
| Epoch [  7/ 10] Iter[ 21/391]		Loss: 0.4025
| Epoch [  7/ 10] Iter[ 41/391]		Loss: 0.2614
| Epoch [  7/ 10] Iter[ 61/391]		Loss: 0.4227
| Epoch [  7/ 10] Iter[ 81/391]		Loss: 0.3763
| Epoch [  7/ 10] Iter[101/391]		Loss: 0.4928
| Epoch [  7/ 10] Iter[121/391]		Loss: 0.3837
| Epoch [  7/ 10] Iter[141/391]		Loss: 0.4068
| Epoch [  7/ 10] Iter[161/391]		Loss: 0.3367
| Epoch [  7/ 10] Iter[181/391]		Loss: 0.4339
| Epoch [  7/ 10] Iter[201/391]		Loss: 0.4711
| Epoch [  7/ 10] Iter[221/391]		Loss: 0.3997
| Epoch [  7/ 10] Iter[241/391]		Loss: 0.3506
| Epoch [  7/ 10] Iter[261/391]		Loss: 0.5374
| Epoch [  7/ 10] Iter[281/391]		Loss: 0.3989
| Epoch [  7/ 10] Iter[301/391]		Loss: 0.4286
| Epoch [  7/ 10] Iter[321/391]		Loss: 0.3749
| Epoch [  7/ 10] Iter[341/391]		Loss: 0.5033
| Epoch [  7/ 10] Iter[361/391]		Loss: 0.3787
| Epoch [  7/ 10] Iter[381/391]		Loss: 0.3983
=> Early Training Epoch #8
| Epoch [  8/ 10] Iter[  1/391]		Loss: 0.4005
| Epoch [  8/ 10] Iter[ 21/391]		Loss: 0.3337
| Epoch [  8/ 10] Iter[ 41/391]		Loss: 0.2231
| Epoch [  8/ 10] Iter[ 61/391]		Loss: 0.3228
| Epoch [  8/ 10] Iter[ 81/391]		Loss: 0.2194
| Epoch [  8/ 10] Iter[101/391]		Loss: 0.3100
| Epoch [  8/ 10] Iter[121/391]		Loss: 0.3227
| Epoch [  8/ 10] Iter[141/391]		Loss: 0.4565
| Epoch [  8/ 10] Iter[161/391]		Loss: 0.3672
| Epoch [  8/ 10] Iter[181/391]		Loss: 0.4309
| Epoch [  8/ 10] Iter[201/391]		Loss: 0.1678
| Epoch [  8/ 10] Iter[221/391]		Loss: 0.4120
| Epoch [  8/ 10] Iter[241/391]		Loss: 0.4822
| Epoch [  8/ 10] Iter[261/391]		Loss: 0.3880
| Epoch [  8/ 10] Iter[281/391]		Loss: 0.2999
| Epoch [  8/ 10] Iter[301/391]		Loss: 0.3162
| Epoch [  8/ 10] Iter[321/391]		Loss: 0.4236
| Epoch [  8/ 10] Iter[341/391]		Loss: 0.3643
| Epoch [  8/ 10] Iter[361/391]		Loss: 0.4357
| Epoch [  8/ 10] Iter[381/391]		Loss: 0.4851
=> Early Training Epoch #9
| Epoch [  9/ 10] Iter[  1/391]		Loss: 0.4334
| Epoch [  9/ 10] Iter[ 21/391]		Loss: 0.2286
| Epoch [  9/ 10] Iter[ 41/391]		Loss: 0.2113
| Epoch [  9/ 10] Iter[ 61/391]		Loss: 0.2868
| Epoch [  9/ 10] Iter[ 81/391]		Loss: 0.2773
| Epoch [  9/ 10] Iter[101/391]		Loss: 0.3063
| Epoch [  9/ 10] Iter[121/391]		Loss: 0.2426
| Epoch [  9/ 10] Iter[141/391]		Loss: 0.3809
| Epoch [  9/ 10] Iter[161/391]		Loss: 0.3099
| Epoch [  9/ 10] Iter[181/391]		Loss: 0.3417
| Epoch [  9/ 10] Iter[201/391]		Loss: 0.4822
| Epoch [  9/ 10] Iter[221/391]		Loss: 0.4379
| Epoch [  9/ 10] Iter[241/391]		Loss: 0.2185
| Epoch [  9/ 10] Iter[261/391]		Loss: 0.2649
| Epoch [  9/ 10] Iter[281/391]		Loss: 0.3996
| Epoch [  9/ 10] Iter[301/391]		Loss: 0.3938
| Epoch [  9/ 10] Iter[321/391]		Loss: 0.3512
| Epoch [  9/ 10] Iter[341/391]		Loss: 0.3899
| Epoch [  9/ 10] Iter[361/391]		Loss: 0.4419
| Epoch [  9/ 10] Iter[381/391]		Loss: 0.4733
=> Early Training Epoch #0
| Epoch [  0/ 10] Iter[  1/391]		Loss: 2.4811
| Epoch [  0/ 10] Iter[ 21/391]		Loss: 2.4214
| Epoch [  0/ 10] Iter[ 41/391]		Loss: 2.2853
| Epoch [  0/ 10] Iter[ 61/391]		Loss: 2.2866
| Epoch [  0/ 10] Iter[ 81/391]		Loss: 2.1599
| Epoch [  0/ 10] Iter[101/391]		Loss: 1.9986
| Epoch [  0/ 10] Iter[121/391]		Loss: 1.9610
| Epoch [  0/ 10] Iter[141/391]		Loss: 1.9061
| Epoch [  0/ 10] Iter[161/391]		Loss: 1.9939
| Epoch [  0/ 10] Iter[181/391]		Loss: 1.8107
| Epoch [  0/ 10] Iter[201/391]		Loss: 1.8988
| Epoch [  0/ 10] Iter[221/391]		Loss: 1.7103
| Epoch [  0/ 10] Iter[241/391]		Loss: 2.0020
| Epoch [  0/ 10] Iter[261/391]		Loss: 1.6274
| Epoch [  0/ 10] Iter[281/391]		Loss: 1.6875
| Epoch [  0/ 10] Iter[301/391]		Loss: 1.6821
| Epoch [  0/ 10] Iter[321/391]		Loss: 1.4863
| Epoch [  0/ 10] Iter[341/391]		Loss: 1.4330
| Epoch [  0/ 10] Iter[361/391]		Loss: 1.5470
| Epoch [  0/ 10] Iter[381/391]		Loss: 1.4868
=> Early Training Epoch #1
| Epoch [  1/ 10] Iter[  1/391]		Loss: 1.4714
| Epoch [  1/ 10] Iter[ 21/391]		Loss: 1.5362
| Epoch [  1/ 10] Iter[ 41/391]		Loss: 1.4892
| Epoch [  1/ 10] Iter[ 61/391]		Loss: 1.3777
| Epoch [  1/ 10] Iter[ 81/391]		Loss: 1.4019
| Epoch [  1/ 10] Iter[101/391]		Loss: 1.4466
| Epoch [  1/ 10] Iter[121/391]		Loss: 1.4558
| Epoch [  1/ 10] Iter[141/391]		Loss: 1.3598
| Epoch [  1/ 10] Iter[161/391]		Loss: 1.4179
| Epoch [  1/ 10] Iter[181/391]		Loss: 1.5549
| Epoch [  1/ 10] Iter[201/391]		Loss: 1.2686
| Epoch [  1/ 10] Iter[221/391]		Loss: 1.3197
| Epoch [  1/ 10] Iter[241/391]		Loss: 1.4804
| Epoch [  1/ 10] Iter[261/391]		Loss: 1.3179
| Epoch [  1/ 10] Iter[281/391]		Loss: 1.2170
| Epoch [  1/ 10] Iter[301/391]		Loss: 1.0334
| Epoch [  1/ 10] Iter[321/391]		Loss: 1.1168
| Epoch [  1/ 10] Iter[341/391]		Loss: 1.1404
| Epoch [  1/ 10] Iter[361/391]		Loss: 1.3868
| Epoch [  1/ 10] Iter[381/391]		Loss: 1.2571
=> Early Training Epoch #2
| Epoch [  2/ 10] Iter[  1/391]		Loss: 1.2278
| Epoch [  2/ 10] Iter[ 21/391]		Loss: 1.0649
| Epoch [  2/ 10] Iter[ 41/391]		Loss: 1.1956
| Epoch [  2/ 10] Iter[ 61/391]		Loss: 1.1823
| Epoch [  2/ 10] Iter[ 81/391]		Loss: 1.0985
| Epoch [  2/ 10] Iter[101/391]		Loss: 1.3476
| Epoch [  2/ 10] Iter[121/391]		Loss: 1.1728
| Epoch [  2/ 10] Iter[141/391]		Loss: 1.0122
| Epoch [  2/ 10] Iter[161/391]		Loss: 0.9907
| Epoch [  2/ 10] Iter[181/391]		Loss: 1.0987
| Epoch [  2/ 10] Iter[201/391]		Loss: 1.0343
| Epoch [  2/ 10] Iter[221/391]		Loss: 0.8236
| Epoch [  2/ 10] Iter[241/391]		Loss: 0.9733
| Epoch [  2/ 10] Iter[261/391]		Loss: 1.1032
| Epoch [  2/ 10] Iter[281/391]		Loss: 1.0154
| Epoch [  2/ 10] Iter[301/391]		Loss: 1.0446
| Epoch [  2/ 10] Iter[321/391]		Loss: 0.9746
| Epoch [  2/ 10] Iter[341/391]		Loss: 0.9973
| Epoch [  2/ 10] Iter[361/391]		Loss: 1.0809
| Epoch [  2/ 10] Iter[381/391]		Loss: 0.8586
=> Early Training Epoch #3
| Epoch [  3/ 10] Iter[  1/391]		Loss: 0.9295
| Epoch [  3/ 10] Iter[ 21/391]		Loss: 0.8617
| Epoch [  3/ 10] Iter[ 41/391]		Loss: 0.8894
| Epoch [  3/ 10] Iter[ 61/391]		Loss: 0.8161
| Epoch [  3/ 10] Iter[ 81/391]		Loss: 0.9283
| Epoch [  3/ 10] Iter[101/391]		Loss: 0.8751
| Epoch [  3/ 10] Iter[121/391]		Loss: 0.8441
| Epoch [  3/ 10] Iter[141/391]		Loss: 0.9588
| Epoch [  3/ 10] Iter[161/391]		Loss: 1.0871
| Epoch [  3/ 10] Iter[181/391]		Loss: 0.8553
| Epoch [  3/ 10] Iter[201/391]		Loss: 0.8034
| Epoch [  3/ 10] Iter[221/391]		Loss: 0.6493
| Epoch [  3/ 10] Iter[241/391]		Loss: 0.7700
| Epoch [  3/ 10] Iter[261/391]		Loss: 0.6838
| Epoch [  3/ 10] Iter[281/391]		Loss: 0.8146
| Epoch [  3/ 10] Iter[301/391]		Loss: 0.7205
| Epoch [  3/ 10] Iter[321/391]		Loss: 0.8499
| Epoch [  3/ 10] Iter[341/391]		Loss: 0.6740
| Epoch [  3/ 10] Iter[361/391]		Loss: 0.9180
| Epoch [  3/ 10] Iter[381/391]		Loss: 0.8175
=> Early Training Epoch #4
| Epoch [  4/ 10] Iter[  1/391]		Loss: 0.7200
| Epoch [  4/ 10] Iter[ 21/391]		Loss: 0.6897
| Epoch [  4/ 10] Iter[ 41/391]		Loss: 0.8643
| Epoch [  4/ 10] Iter[ 61/391]		Loss: 0.5984
| Epoch [  4/ 10] Iter[ 81/391]		Loss: 0.7882
| Epoch [  4/ 10] Iter[101/391]		Loss: 0.7189
| Epoch [  4/ 10] Iter[121/391]		Loss: 0.8011
| Epoch [  4/ 10] Iter[141/391]		Loss: 0.5090
| Epoch [  4/ 10] Iter[161/391]		Loss: 0.8560
| Epoch [  4/ 10] Iter[181/391]		Loss: 0.7643
| Epoch [  4/ 10] Iter[201/391]		Loss: 0.6087
| Epoch [  4/ 10] Iter[221/391]		Loss: 0.8401
| Epoch [  4/ 10] Iter[241/391]		Loss: 0.8294
| Epoch [  4/ 10] Iter[261/391]		Loss: 0.6905
| Epoch [  4/ 10] Iter[281/391]		Loss: 0.7445
| Epoch [  4/ 10] Iter[301/391]		Loss: 0.6633
| Epoch [  4/ 10] Iter[321/391]		Loss: 0.7398
| Epoch [  4/ 10] Iter[341/391]		Loss: 0.7779
| Epoch [  4/ 10] Iter[361/391]		Loss: 0.6799
| Epoch [  4/ 10] Iter[381/391]		Loss: 0.6921
=> Early Training Epoch #5
| Epoch [  5/ 10] Iter[  1/391]		Loss: 0.5542
| Epoch [  5/ 10] Iter[ 21/391]		Loss: 0.5522
| Epoch [  5/ 10] Iter[ 41/391]		Loss: 0.6026
| Epoch [  5/ 10] Iter[ 61/391]		Loss: 0.6209
| Epoch [  5/ 10] Iter[ 81/391]		Loss: 0.5740
| Epoch [  5/ 10] Iter[101/391]		Loss: 0.6756
| Epoch [  5/ 10] Iter[121/391]		Loss: 0.5262
| Epoch [  5/ 10] Iter[141/391]		Loss: 0.7092
| Epoch [  5/ 10] Iter[161/391]		Loss: 0.6892
| Epoch [  5/ 10] Iter[181/391]		Loss: 0.6234
| Epoch [  5/ 10] Iter[201/391]		Loss: 0.5478
| Epoch [  5/ 10] Iter[221/391]		Loss: 0.5307
| Epoch [  5/ 10] Iter[241/391]		Loss: 0.7635
| Epoch [  5/ 10] Iter[261/391]		Loss: 0.4128
| Epoch [  5/ 10] Iter[281/391]		Loss: 0.7369
| Epoch [  5/ 10] Iter[301/391]		Loss: 0.6681
| Epoch [  5/ 10] Iter[321/391]		Loss: 0.5130
| Epoch [  5/ 10] Iter[341/391]		Loss: 0.6248
| Epoch [  5/ 10] Iter[361/391]		Loss: 0.6402
| Epoch [  5/ 10] Iter[381/391]		Loss: 0.6783
=> Early Training Epoch #6
| Epoch [  6/ 10] Iter[  1/391]		Loss: 0.3568
| Epoch [  6/ 10] Iter[ 21/391]		Loss: 0.5192
| Epoch [  6/ 10] Iter[ 41/391]		Loss: 0.5830
| Epoch [  6/ 10] Iter[ 61/391]		Loss: 0.4494
| Epoch [  6/ 10] Iter[ 81/391]		Loss: 0.3498
| Epoch [  6/ 10] Iter[101/391]		Loss: 0.5332
| Epoch [  6/ 10] Iter[121/391]		Loss: 0.3720
| Epoch [  6/ 10] Iter[141/391]		Loss: 0.4304
| Epoch [  6/ 10] Iter[161/391]		Loss: 0.5271
| Epoch [  6/ 10] Iter[181/391]		Loss: 0.4558
| Epoch [  6/ 10] Iter[201/391]		Loss: 0.5826
| Epoch [  6/ 10] Iter[221/391]		Loss: 0.5697
| Epoch [  6/ 10] Iter[241/391]		Loss: 0.5076
| Epoch [  6/ 10] Iter[261/391]		Loss: 0.4650
| Epoch [  6/ 10] Iter[281/391]		Loss: 0.4503
| Epoch [  6/ 10] Iter[301/391]		Loss: 0.5084
| Epoch [  6/ 10] Iter[321/391]		Loss: 0.4307
| Epoch [  6/ 10] Iter[341/391]		Loss: 0.5594
| Epoch [  6/ 10] Iter[361/391]		Loss: 0.5096
| Epoch [  6/ 10] Iter[381/391]		Loss: 0.7544
=> Early Training Epoch #7
| Epoch [  7/ 10] Iter[  1/391]		Loss: 0.4616
| Epoch [  7/ 10] Iter[ 21/391]		Loss: 0.5423
| Epoch [  7/ 10] Iter[ 41/391]		Loss: 0.2526
| Epoch [  7/ 10] Iter[ 61/391]		Loss: 0.3901
| Epoch [  7/ 10] Iter[ 81/391]		Loss: 0.4638
| Epoch [  7/ 10] Iter[101/391]		Loss: 0.5359
| Epoch [  7/ 10] Iter[121/391]		Loss: 0.4838
| Epoch [  7/ 10] Iter[141/391]		Loss: 0.4504
| Epoch [  7/ 10] Iter[161/391]		Loss: 0.4969
| Epoch [  7/ 10] Iter[181/391]		Loss: 0.4878
| Epoch [  7/ 10] Iter[201/391]		Loss: 0.4464
| Epoch [  7/ 10] Iter[221/391]		Loss: 0.3679
| Epoch [  7/ 10] Iter[241/391]		Loss: 0.4483
| Epoch [  7/ 10] Iter[261/391]		Loss: 0.4602
| Epoch [  7/ 10] Iter[281/391]		Loss: 0.5268
| Epoch [  7/ 10] Iter[301/391]		Loss: 0.3783
| Epoch [  7/ 10] Iter[321/391]		Loss: 0.4139
| Epoch [  7/ 10] Iter[341/391]		Loss: 0.5524
| Epoch [  7/ 10] Iter[361/391]		Loss: 0.5330
| Epoch [  7/ 10] Iter[381/391]		Loss: 0.5105
=> Early Training Epoch #8
| Epoch [  8/ 10] Iter[  1/391]		Loss: 0.4238
| Epoch [  8/ 10] Iter[ 21/391]		Loss: 0.4032
| Epoch [  8/ 10] Iter[ 41/391]		Loss: 0.2896
| Epoch [  8/ 10] Iter[ 61/391]		Loss: 0.3477
| Epoch [  8/ 10] Iter[ 81/391]		Loss: 0.3485
| Epoch [  8/ 10] Iter[101/391]		Loss: 0.4098
| Epoch [  8/ 10] Iter[121/391]		Loss: 0.3795
| Epoch [  8/ 10] Iter[141/391]		Loss: 0.3608
| Epoch [  8/ 10] Iter[161/391]		Loss: 0.4899
| Epoch [  8/ 10] Iter[181/391]		Loss: 0.3317
| Epoch [  8/ 10] Iter[201/391]		Loss: 0.3635
| Epoch [  8/ 10] Iter[221/391]		Loss: 0.4549
| Epoch [  8/ 10] Iter[241/391]		Loss: 0.3396
| Epoch [  8/ 10] Iter[261/391]		Loss: 0.5404
| Epoch [  8/ 10] Iter[281/391]		Loss: 0.3818
| Epoch [  8/ 10] Iter[301/391]		Loss: 0.3892
| Epoch [  8/ 10] Iter[321/391]		Loss: 0.5117
| Epoch [  8/ 10] Iter[341/391]		Loss: 0.4123
| Epoch [  8/ 10] Iter[361/391]		Loss: 0.3849
| Epoch [  8/ 10] Iter[381/391]		Loss: 0.3873
=> Early Training Epoch #9
| Epoch [  9/ 10] Iter[  1/391]		Loss: 0.2109
| Epoch [  9/ 10] Iter[ 21/391]		Loss: 0.2458
| Epoch [  9/ 10] Iter[ 41/391]		Loss: 0.3259
| Epoch [  9/ 10] Iter[ 61/391]		Loss: 0.2217
| Epoch [  9/ 10] Iter[ 81/391]		Loss: 0.2548
| Epoch [  9/ 10] Iter[101/391]		Loss: 0.2678
| Epoch [  9/ 10] Iter[121/391]		Loss: 0.4043
| Epoch [  9/ 10] Iter[141/391]		Loss: 0.4012
| Epoch [  9/ 10] Iter[161/391]		Loss: 0.3106
| Epoch [  9/ 10] Iter[181/391]		Loss: 0.3147
| Epoch [  9/ 10] Iter[201/391]		Loss: 0.3782
| Epoch [  9/ 10] Iter[221/391]		Loss: 0.4035
| Epoch [  9/ 10] Iter[241/391]		Loss: 0.3936
| Epoch [  9/ 10] Iter[261/391]		Loss: 0.3062
| Epoch [  9/ 10] Iter[281/391]		Loss: 0.2956
| Epoch [  9/ 10] Iter[301/391]		Loss: 0.3560
| Epoch [  9/ 10] Iter[321/391]		Loss: 0.3106
| Epoch [  9/ 10] Iter[341/391]		Loss: 0.2880
| Epoch [  9/ 10] Iter[361/391]		Loss: 0.4401
| Epoch [  9/ 10] Iter[381/391]		Loss: 0.4076
=> Early Training Epoch #0
| Epoch [  0/ 10] Iter[  1/391]		Loss: 2.4095
| Epoch [  0/ 10] Iter[ 21/391]		Loss: 2.4097
| Epoch [  0/ 10] Iter[ 41/391]		Loss: 2.0495
| Epoch [  0/ 10] Iter[ 61/391]		Loss: 2.0674
| Epoch [  0/ 10] Iter[ 81/391]		Loss: 1.9255
| Epoch [  0/ 10] Iter[101/391]		Loss: 1.8837
| Epoch [  0/ 10] Iter[121/391]		Loss: 1.9030
| Epoch [  0/ 10] Iter[141/391]		Loss: 1.6836
| Epoch [  0/ 10] Iter[161/391]		Loss: 1.6952
| Epoch [  0/ 10] Iter[181/391]		Loss: 1.5528
| Epoch [  0/ 10] Iter[201/391]		Loss: 1.5813
| Epoch [  0/ 10] Iter[221/391]		Loss: 1.5414
| Epoch [  0/ 10] Iter[241/391]		Loss: 1.7517
| Epoch [  0/ 10] Iter[261/391]		Loss: 1.7079
| Epoch [  0/ 10] Iter[281/391]		Loss: 1.6058
| Epoch [  0/ 10] Iter[301/391]		Loss: 1.6018
| Epoch [  0/ 10] Iter[321/391]		Loss: 1.5690
| Epoch [  0/ 10] Iter[341/391]		Loss: 1.4214
| Epoch [  0/ 10] Iter[361/391]		Loss: 1.4460
| Epoch [  0/ 10] Iter[381/391]		Loss: 1.5454
=> Early Training Epoch #1
| Epoch [  1/ 10] Iter[  1/391]		Loss: 1.3332
| Epoch [  1/ 10] Iter[ 21/391]		Loss: 1.3220
| Epoch [  1/ 10] Iter[ 41/391]		Loss: 1.4698
| Epoch [  1/ 10] Iter[ 61/391]		Loss: 1.3155
| Epoch [  1/ 10] Iter[ 81/391]		Loss: 1.3734
| Epoch [  1/ 10] Iter[101/391]		Loss: 1.2926
| Epoch [  1/ 10] Iter[121/391]		Loss: 1.4309
| Epoch [  1/ 10] Iter[141/391]		Loss: 1.3604
| Epoch [  1/ 10] Iter[161/391]		Loss: 1.2435
| Epoch [  1/ 10] Iter[181/391]		Loss: 1.2498
| Epoch [  1/ 10] Iter[201/391]		Loss: 1.1563
| Epoch [  1/ 10] Iter[221/391]		Loss: 1.4276
| Epoch [  1/ 10] Iter[241/391]		Loss: 1.3267
| Epoch [  1/ 10] Iter[261/391]		Loss: 1.0953
| Epoch [  1/ 10] Iter[281/391]		Loss: 1.1789
| Epoch [  1/ 10] Iter[301/391]		Loss: 1.1654
| Epoch [  1/ 10] Iter[321/391]		Loss: 1.3086
| Epoch [  1/ 10] Iter[341/391]		Loss: 1.2651
| Epoch [  1/ 10] Iter[361/391]		Loss: 1.1779
| Epoch [  1/ 10] Iter[381/391]		Loss: 1.2243
=> Early Training Epoch #2
| Epoch [  2/ 10] Iter[  1/391]		Loss: 1.1850
| Epoch [  2/ 10] Iter[ 21/391]		Loss: 1.0595
| Epoch [  2/ 10] Iter[ 41/391]		Loss: 1.1700
| Epoch [  2/ 10] Iter[ 61/391]		Loss: 1.0601
| Epoch [  2/ 10] Iter[ 81/391]		Loss: 0.9201
| Epoch [  2/ 10] Iter[101/391]		Loss: 0.9356
| Epoch [  2/ 10] Iter[121/391]		Loss: 0.9077
| Epoch [  2/ 10] Iter[141/391]		Loss: 0.8746
| Epoch [  2/ 10] Iter[161/391]		Loss: 0.9616
| Epoch [  2/ 10] Iter[181/391]		Loss: 1.0299
| Epoch [  2/ 10] Iter[201/391]		Loss: 0.8891
| Epoch [  2/ 10] Iter[221/391]		Loss: 0.9902
| Epoch [  2/ 10] Iter[241/391]		Loss: 0.9504
| Epoch [  2/ 10] Iter[261/391]		Loss: 0.9914
| Epoch [  2/ 10] Iter[281/391]		Loss: 0.9057
| Epoch [  2/ 10] Iter[301/391]		Loss: 1.0451
| Epoch [  2/ 10] Iter[321/391]		Loss: 1.0063
| Epoch [  2/ 10] Iter[341/391]		Loss: 1.0982
| Epoch [  2/ 10] Iter[361/391]		Loss: 0.9706
| Epoch [  2/ 10] Iter[381/391]		Loss: 1.0207
=> Early Training Epoch #3
| Epoch [  3/ 10] Iter[  1/391]		Loss: 0.9466
| Epoch [  3/ 10] Iter[ 21/391]		Loss: 0.8803
| Epoch [  3/ 10] Iter[ 41/391]		Loss: 0.9309
| Epoch [  3/ 10] Iter[ 61/391]		Loss: 0.8727
| Epoch [  3/ 10] Iter[ 81/391]		Loss: 0.9069
| Epoch [  3/ 10] Iter[101/391]		Loss: 0.7019
| Epoch [  3/ 10] Iter[121/391]		Loss: 0.8303
| Epoch [  3/ 10] Iter[141/391]		Loss: 0.7246
| Epoch [  3/ 10] Iter[161/391]		Loss: 0.6732
| Epoch [  3/ 10] Iter[181/391]		Loss: 0.8057
| Epoch [  3/ 10] Iter[201/391]		Loss: 0.7025
| Epoch [  3/ 10] Iter[221/391]		Loss: 0.8784
| Epoch [  3/ 10] Iter[241/391]		Loss: 0.7462
| Epoch [  3/ 10] Iter[261/391]		Loss: 0.8772
| Epoch [  3/ 10] Iter[281/391]		Loss: 0.7248
| Epoch [  3/ 10] Iter[301/391]		Loss: 0.6886
| Epoch [  3/ 10] Iter[321/391]		Loss: 0.6507
| Epoch [  3/ 10] Iter[341/391]		Loss: 0.7537
| Epoch [  3/ 10] Iter[361/391]		Loss: 0.7841
| Epoch [  3/ 10] Iter[381/391]		Loss: 0.6159
=> Early Training Epoch #4
| Epoch [  4/ 10] Iter[  1/391]		Loss: 0.6604
| Epoch [  4/ 10] Iter[ 21/391]		Loss: 0.5152
| Epoch [  4/ 10] Iter[ 41/391]		Loss: 0.5962
| Epoch [  4/ 10] Iter[ 61/391]		Loss: 0.6115
| Epoch [  4/ 10] Iter[ 81/391]		Loss: 0.6097
| Epoch [  4/ 10] Iter[101/391]		Loss: 0.5475
| Epoch [  4/ 10] Iter[121/391]		Loss: 0.6489
| Epoch [  4/ 10] Iter[141/391]		Loss: 0.6936
| Epoch [  4/ 10] Iter[161/391]		Loss: 0.6127
| Epoch [  4/ 10] Iter[181/391]		Loss: 0.6580
| Epoch [  4/ 10] Iter[201/391]		Loss: 0.6603
| Epoch [  4/ 10] Iter[221/391]		Loss: 0.8746
| Epoch [  4/ 10] Iter[241/391]		Loss: 0.7856
| Epoch [  4/ 10] Iter[261/391]		Loss: 0.7615
| Epoch [  4/ 10] Iter[281/391]		Loss: 0.6416
| Epoch [  4/ 10] Iter[301/391]		Loss: 0.5346
| Epoch [  4/ 10] Iter[321/391]		Loss: 0.5622
| Epoch [  4/ 10] Iter[341/391]		Loss: 0.5782
| Epoch [  4/ 10] Iter[361/391]		Loss: 0.6247
| Epoch [  4/ 10] Iter[381/391]		Loss: 0.4729
=> Early Training Epoch #5
| Epoch [  5/ 10] Iter[  1/391]		Loss: 0.6178
| Epoch [  5/ 10] Iter[ 21/391]		Loss: 0.5750
| Epoch [  5/ 10] Iter[ 41/391]		Loss: 0.4278
| Epoch [  5/ 10] Iter[ 61/391]		Loss: 0.5590
| Epoch [  5/ 10] Iter[ 81/391]		Loss: 0.5868
| Epoch [  5/ 10] Iter[101/391]		Loss: 0.5673
| Epoch [  5/ 10] Iter[121/391]		Loss: 0.5126
| Epoch [  5/ 10] Iter[141/391]		Loss: 0.5860
| Epoch [  5/ 10] Iter[161/391]		Loss: 0.5830
| Epoch [  5/ 10] Iter[181/391]		Loss: 0.5428
| Epoch [  5/ 10] Iter[201/391]		Loss: 0.5489
| Epoch [  5/ 10] Iter[221/391]		Loss: 0.5638
| Epoch [  5/ 10] Iter[241/391]		Loss: 0.6779
| Epoch [  5/ 10] Iter[261/391]		Loss: 0.5200
| Epoch [  5/ 10] Iter[281/391]		Loss: 0.4622
| Epoch [  5/ 10] Iter[301/391]		Loss: 0.4766
| Epoch [  5/ 10] Iter[321/391]		Loss: 0.3647
| Epoch [  5/ 10] Iter[341/391]		Loss: 0.5278
| Epoch [  5/ 10] Iter[361/391]		Loss: 0.6101
| Epoch [  5/ 10] Iter[381/391]		Loss: 0.6982
=> Early Training Epoch #6
| Epoch [  6/ 10] Iter[  1/391]		Loss: 0.3935
| Epoch [  6/ 10] Iter[ 21/391]		Loss: 0.2974
| Epoch [  6/ 10] Iter[ 41/391]		Loss: 0.3271
| Epoch [  6/ 10] Iter[ 61/391]		Loss: 0.5291
| Epoch [  6/ 10] Iter[ 81/391]		Loss: 0.3905
| Epoch [  6/ 10] Iter[101/391]		Loss: 0.5312
| Epoch [  6/ 10] Iter[121/391]		Loss: 0.4114
| Epoch [  6/ 10] Iter[141/391]		Loss: 0.4724
| Epoch [  6/ 10] Iter[161/391]		Loss: 0.6187
| Epoch [  6/ 10] Iter[181/391]		Loss: 0.5570
| Epoch [  6/ 10] Iter[201/391]		Loss: 0.4231
| Epoch [  6/ 10] Iter[221/391]		Loss: 0.5274
| Epoch [  6/ 10] Iter[241/391]		Loss: 0.4468
| Epoch [  6/ 10] Iter[261/391]		Loss: 0.5662
| Epoch [  6/ 10] Iter[281/391]		Loss: 0.4127
| Epoch [  6/ 10] Iter[301/391]		Loss: 0.4375
| Epoch [  6/ 10] Iter[321/391]		Loss: 0.4579
| Epoch [  6/ 10] Iter[341/391]		Loss: 0.5654
| Epoch [  6/ 10] Iter[361/391]		Loss: 0.4306
| Epoch [  6/ 10] Iter[381/391]		Loss: 0.3853
=> Early Training Epoch #7
| Epoch [  7/ 10] Iter[  1/391]		Loss: 0.4047
| Epoch [  7/ 10] Iter[ 21/391]		Loss: 0.2761
| Epoch [  7/ 10] Iter[ 41/391]		Loss: 0.3345
| Epoch [  7/ 10] Iter[ 61/391]		Loss: 0.2818
| Epoch [  7/ 10] Iter[ 81/391]		Loss: 0.2321
| Epoch [  7/ 10] Iter[101/391]		Loss: 0.4083
| Epoch [  7/ 10] Iter[121/391]		Loss: 0.5175
| Epoch [  7/ 10] Iter[141/391]		Loss: 0.5061
| Epoch [  7/ 10] Iter[161/391]		Loss: 0.2249
| Epoch [  7/ 10] Iter[181/391]		Loss: 0.4202
| Epoch [  7/ 10] Iter[201/391]		Loss: 0.4171
| Epoch [  7/ 10] Iter[221/391]		Loss: 0.3955
| Epoch [  7/ 10] Iter[241/391]		Loss: 0.3713
| Epoch [  7/ 10] Iter[261/391]		Loss: 0.3313
| Epoch [  7/ 10] Iter[281/391]		Loss: 0.3382
| Epoch [  7/ 10] Iter[301/391]		Loss: 0.4192
| Epoch [  7/ 10] Iter[321/391]		Loss: 0.4135
| Epoch [  7/ 10] Iter[341/391]		Loss: 0.3776
| Epoch [  7/ 10] Iter[361/391]		Loss: 0.3557
| Epoch [  7/ 10] Iter[381/391]		Loss: 0.6685
=> Early Training Epoch #8
| Epoch [  8/ 10] Iter[  1/391]		Loss: 0.3247
| Epoch [  8/ 10] Iter[ 21/391]		Loss: 0.2886
| Epoch [  8/ 10] Iter[ 41/391]		Loss: 0.3363
| Epoch [  8/ 10] Iter[ 61/391]		Loss: 0.3098
| Epoch [  8/ 10] Iter[ 81/391]		Loss: 0.3643
| Epoch [  8/ 10] Iter[101/391]		Loss: 0.3544
| Epoch [  8/ 10] Iter[121/391]		Loss: 0.3490
| Epoch [  8/ 10] Iter[141/391]		Loss: 0.3887
| Epoch [  8/ 10] Iter[161/391]		Loss: 0.4674
| Epoch [  8/ 10] Iter[181/391]		Loss: 0.4194
| Epoch [  8/ 10] Iter[201/391]		Loss: 0.2854
| Epoch [  8/ 10] Iter[221/391]		Loss: 0.3466
| Epoch [  8/ 10] Iter[241/391]		Loss: 0.4344
| Epoch [  8/ 10] Iter[261/391]		Loss: 0.4089
| Epoch [  8/ 10] Iter[281/391]		Loss: 0.3955
| Epoch [  8/ 10] Iter[301/391]		Loss: 0.2285
| Epoch [  8/ 10] Iter[321/391]		Loss: 0.4188
| Epoch [  8/ 10] Iter[341/391]		Loss: 0.4044
| Epoch [  8/ 10] Iter[361/391]		Loss: 0.3576
| Epoch [  8/ 10] Iter[381/391]		Loss: 0.4447
=> Early Training Epoch #9
| Epoch [  9/ 10] Iter[  1/391]		Loss: 0.3439
| Epoch [  9/ 10] Iter[ 21/391]		Loss: 0.1971
| Epoch [  9/ 10] Iter[ 41/391]		Loss: 0.1665
| Epoch [  9/ 10] Iter[ 61/391]		Loss: 0.3403
| Epoch [  9/ 10] Iter[ 81/391]		Loss: 0.3475
| Epoch [  9/ 10] Iter[101/391]		Loss: 0.4183
| Epoch [  9/ 10] Iter[121/391]		Loss: 0.1942
| Epoch [  9/ 10] Iter[141/391]		Loss: 0.3956
| Epoch [  9/ 10] Iter[161/391]		Loss: 0.3061
| Epoch [  9/ 10] Iter[181/391]		Loss: 0.3269
| Epoch [  9/ 10] Iter[201/391]		Loss: 0.4314
| Epoch [  9/ 10] Iter[221/391]		Loss: 0.2707
| Epoch [  9/ 10] Iter[241/391]		Loss: 0.2414
| Epoch [  9/ 10] Iter[261/391]		Loss: 0.4392
| Epoch [  9/ 10] Iter[281/391]		Loss: 0.3908
| Epoch [  9/ 10] Iter[301/391]		Loss: 0.3057
| Epoch [  9/ 10] Iter[321/391]		Loss: 0.2448
| Epoch [  9/ 10] Iter[341/391]		Loss: 0.2233
| Epoch [  9/ 10] Iter[361/391]		Loss: 0.3119
| Epoch [  9/ 10] Iter[381/391]		Loss: 0.3425
=> Early Training Epoch #0
| Epoch [  0/ 10] Iter[  1/391]		Loss: 2.3582
| Epoch [  0/ 10] Iter[ 21/391]		Loss: 2.5758
| Epoch [  0/ 10] Iter[ 41/391]		Loss: 2.0653
| Epoch [  0/ 10] Iter[ 61/391]		Loss: 1.9648
| Epoch [  0/ 10] Iter[ 81/391]		Loss: 1.9660
| Epoch [  0/ 10] Iter[101/391]		Loss: 1.8311
| Epoch [  0/ 10] Iter[121/391]		Loss: 1.8520
| Epoch [  0/ 10] Iter[141/391]		Loss: 1.8952
| Epoch [  0/ 10] Iter[161/391]		Loss: 1.7067
| Epoch [  0/ 10] Iter[181/391]		Loss: 1.6226
| Epoch [  0/ 10] Iter[201/391]		Loss: 1.5148
| Epoch [  0/ 10] Iter[221/391]		Loss: 1.6374
| Epoch [  0/ 10] Iter[241/391]		Loss: 1.5550
| Epoch [  0/ 10] Iter[261/391]		Loss: 1.5426
| Epoch [  0/ 10] Iter[281/391]		Loss: 1.4489
| Epoch [  0/ 10] Iter[301/391]		Loss: 1.3782
| Epoch [  0/ 10] Iter[321/391]		Loss: 1.2904
| Epoch [  0/ 10] Iter[341/391]		Loss: 1.4567
| Epoch [  0/ 10] Iter[361/391]		Loss: 1.4283
| Epoch [  0/ 10] Iter[381/391]		Loss: 1.2464
=> Early Training Epoch #1
| Epoch [  1/ 10] Iter[  1/391]		Loss: 1.3958
| Epoch [  1/ 10] Iter[ 21/391]		Loss: 1.3303
| Epoch [  1/ 10] Iter[ 41/391]		Loss: 1.1480
| Epoch [  1/ 10] Iter[ 61/391]		Loss: 1.2380
| Epoch [  1/ 10] Iter[ 81/391]		Loss: 1.3487
| Epoch [  1/ 10] Iter[101/391]		Loss: 1.3283
| Epoch [  1/ 10] Iter[121/391]		Loss: 1.1957
| Epoch [  1/ 10] Iter[141/391]		Loss: 1.2293
| Epoch [  1/ 10] Iter[161/391]		Loss: 1.1302
| Epoch [  1/ 10] Iter[181/391]		Loss: 1.1202
| Epoch [  1/ 10] Iter[201/391]		Loss: 1.0913
| Epoch [  1/ 10] Iter[221/391]		Loss: 1.2139
| Epoch [  1/ 10] Iter[241/391]		Loss: 1.2207
| Epoch [  1/ 10] Iter[261/391]		Loss: 1.0336
| Epoch [  1/ 10] Iter[281/391]		Loss: 1.1262
| Epoch [  1/ 10] Iter[301/391]		Loss: 1.0042
| Epoch [  1/ 10] Iter[321/391]		Loss: 1.0894
| Epoch [  1/ 10] Iter[341/391]		Loss: 0.9877
| Epoch [  1/ 10] Iter[361/391]		Loss: 0.9689
| Epoch [  1/ 10] Iter[381/391]		Loss: 1.0079
=> Early Training Epoch #2
| Epoch [  2/ 10] Iter[  1/391]		Loss: 0.9721
| Epoch [  2/ 10] Iter[ 21/391]		Loss: 0.8904
| Epoch [  2/ 10] Iter[ 41/391]		Loss: 0.9502
| Epoch [  2/ 10] Iter[ 61/391]		Loss: 1.0500
| Epoch [  2/ 10] Iter[ 81/391]		Loss: 0.8531
| Epoch [  2/ 10] Iter[101/391]		Loss: 0.8860
| Epoch [  2/ 10] Iter[121/391]		Loss: 1.0393
| Epoch [  2/ 10] Iter[141/391]		Loss: 0.7456
| Epoch [  2/ 10] Iter[161/391]		Loss: 0.9167
| Epoch [  2/ 10] Iter[181/391]		Loss: 0.9068
| Epoch [  2/ 10] Iter[201/391]		Loss: 0.8786
| Epoch [  2/ 10] Iter[221/391]		Loss: 0.9761
| Epoch [  2/ 10] Iter[241/391]		Loss: 0.7870
| Epoch [  2/ 10] Iter[261/391]		Loss: 1.0182
| Epoch [  2/ 10] Iter[281/391]		Loss: 0.9542
| Epoch [  2/ 10] Iter[301/391]		Loss: 0.7677
| Epoch [  2/ 10] Iter[321/391]		Loss: 0.8216
| Epoch [  2/ 10] Iter[341/391]		Loss: 0.8353
| Epoch [  2/ 10] Iter[361/391]		Loss: 0.8167
| Epoch [  2/ 10] Iter[381/391]		Loss: 0.8450
=> Early Training Epoch #3
| Epoch [  3/ 10] Iter[  1/391]		Loss: 0.6580
| Epoch [  3/ 10] Iter[ 21/391]		Loss: 0.6473
| Epoch [  3/ 10] Iter[ 41/391]		Loss: 0.7907
| Epoch [  3/ 10] Iter[ 61/391]		Loss: 0.7500
| Epoch [  3/ 10] Iter[ 81/391]		Loss: 0.6494
| Epoch [  3/ 10] Iter[101/391]		Loss: 0.7359
| Epoch [  3/ 10] Iter[121/391]		Loss: 0.8194
| Epoch [  3/ 10] Iter[141/391]		Loss: 0.7720
| Epoch [  3/ 10] Iter[161/391]		Loss: 0.6750
| Epoch [  3/ 10] Iter[181/391]		Loss: 0.6964
| Epoch [  3/ 10] Iter[201/391]		Loss: 0.8495
| Epoch [  3/ 10] Iter[221/391]		Loss: 0.6513
| Epoch [  3/ 10] Iter[241/391]		Loss: 0.7535
| Epoch [  3/ 10] Iter[261/391]		Loss: 0.8500
| Epoch [  3/ 10] Iter[281/391]		Loss: 0.7001
| Epoch [  3/ 10] Iter[301/391]		Loss: 0.7087
| Epoch [  3/ 10] Iter[321/391]		Loss: 0.5536
| Epoch [  3/ 10] Iter[341/391]		Loss: 0.5880
| Epoch [  3/ 10] Iter[361/391]		Loss: 0.6526
| Epoch [  3/ 10] Iter[381/391]		Loss: 0.5746
=> Early Training Epoch #4
| Epoch [  4/ 10] Iter[  1/391]		Loss: 0.6084
| Epoch [  4/ 10] Iter[ 21/391]		Loss: 0.7031
| Epoch [  4/ 10] Iter[ 41/391]		Loss: 0.5803
| Epoch [  4/ 10] Iter[ 61/391]		Loss: 0.5211
| Epoch [  4/ 10] Iter[ 81/391]		Loss: 0.6523
| Epoch [  4/ 10] Iter[101/391]		Loss: 0.5363
| Epoch [  4/ 10] Iter[121/391]		Loss: 0.6833
| Epoch [  4/ 10] Iter[141/391]		Loss: 0.5090
| Epoch [  4/ 10] Iter[161/391]		Loss: 0.4868
| Epoch [  4/ 10] Iter[181/391]		Loss: 0.6560
| Epoch [  4/ 10] Iter[201/391]		Loss: 0.6553
| Epoch [  4/ 10] Iter[221/391]		Loss: 0.5446
| Epoch [  4/ 10] Iter[241/391]		Loss: 0.6093
| Epoch [  4/ 10] Iter[261/391]		Loss: 0.6572
| Epoch [  4/ 10] Iter[281/391]		Loss: 0.5593
| Epoch [  4/ 10] Iter[301/391]		Loss: 0.5082
| Epoch [  4/ 10] Iter[321/391]		Loss: 0.5018
| Epoch [  4/ 10] Iter[341/391]		Loss: 0.4684
| Epoch [  4/ 10] Iter[361/391]		Loss: 0.6551
| Epoch [  4/ 10] Iter[381/391]		Loss: 0.5256
=> Early Training Epoch #5
| Epoch [  5/ 10] Iter[  1/391]		Loss: 0.4569
| Epoch [  5/ 10] Iter[ 21/391]		Loss: 0.4423
| Epoch [  5/ 10] Iter[ 41/391]		Loss: 0.5221
| Epoch [  5/ 10] Iter[ 61/391]		Loss: 0.6133
| Epoch [  5/ 10] Iter[ 81/391]		Loss: 0.4630
| Epoch [  5/ 10] Iter[101/391]		Loss: 0.4674
| Epoch [  5/ 10] Iter[121/391]		Loss: 0.4594
| Epoch [  5/ 10] Iter[141/391]		Loss: 0.4049
| Epoch [  5/ 10] Iter[161/391]		Loss: 0.4410
| Epoch [  5/ 10] Iter[181/391]		Loss: 0.4068
| Epoch [  5/ 10] Iter[201/391]		Loss: 0.4262
| Epoch [  5/ 10] Iter[221/391]		Loss: 0.3460
| Epoch [  5/ 10] Iter[241/391]		Loss: 0.4242
| Epoch [  5/ 10] Iter[261/391]		Loss: 0.5388
| Epoch [  5/ 10] Iter[281/391]		Loss: 0.5278
| Epoch [  5/ 10] Iter[301/391]		Loss: 0.4116
| Epoch [  5/ 10] Iter[321/391]		Loss: 0.3772
| Epoch [  5/ 10] Iter[341/391]		Loss: 0.4634
| Epoch [  5/ 10] Iter[361/391]		Loss: 0.6411
| Epoch [  5/ 10] Iter[381/391]		Loss: 0.4920
=> Early Training Epoch #6
| Epoch [  6/ 10] Iter[  1/391]		Loss: 0.3850
| Epoch [  6/ 10] Iter[ 21/391]		Loss: 0.3965
| Epoch [  6/ 10] Iter[ 41/391]		Loss: 0.2893
| Epoch [  6/ 10] Iter[ 61/391]		Loss: 0.3961
| Epoch [  6/ 10] Iter[ 81/391]		Loss: 0.4022
| Epoch [  6/ 10] Iter[101/391]		Loss: 0.3874
| Epoch [  6/ 10] Iter[121/391]		Loss: 0.3319
| Epoch [  6/ 10] Iter[141/391]		Loss: 0.5085
| Epoch [  6/ 10] Iter[161/391]		Loss: 0.3521
| Epoch [  6/ 10] Iter[181/391]		Loss: 0.3916
| Epoch [  6/ 10] Iter[201/391]		Loss: 0.3609
| Epoch [  6/ 10] Iter[221/391]		Loss: 0.4017
| Epoch [  6/ 10] Iter[241/391]		Loss: 0.5543
| Epoch [  6/ 10] Iter[261/391]		Loss: 0.3856
| Epoch [  6/ 10] Iter[281/391]		Loss: 0.3991
| Epoch [  6/ 10] Iter[301/391]		Loss: 0.3752
| Epoch [  6/ 10] Iter[321/391]		Loss: 0.4059
| Epoch [  6/ 10] Iter[341/391]		Loss: 0.5033
| Epoch [  6/ 10] Iter[361/391]		Loss: 0.3566
| Epoch [  6/ 10] Iter[381/391]		Loss: 0.4852
=> Early Training Epoch #7
| Epoch [  7/ 10] Iter[  1/391]		Loss: 0.3738
| Epoch [  7/ 10] Iter[ 21/391]		Loss: 0.3786
| Epoch [  7/ 10] Iter[ 41/391]		Loss: 0.3587
| Epoch [  7/ 10] Iter[ 61/391]		Loss: 0.2500
| Epoch [  7/ 10] Iter[ 81/391]		Loss: 0.4033
| Epoch [  7/ 10] Iter[101/391]		Loss: 0.3589
| Epoch [  7/ 10] Iter[121/391]		Loss: 0.3519
| Epoch [  7/ 10] Iter[141/391]		Loss: 0.3882
| Epoch [  7/ 10] Iter[161/391]		Loss: 0.3541
| Epoch [  7/ 10] Iter[181/391]		Loss: 0.3940
| Epoch [  7/ 10] Iter[201/391]		Loss: 0.3784
| Epoch [  7/ 10] Iter[221/391]		Loss: 0.2875
| Epoch [  7/ 10] Iter[241/391]		Loss: 0.4356
| Epoch [  7/ 10] Iter[261/391]		Loss: 0.3158
| Epoch [  7/ 10] Iter[281/391]		Loss: 0.3053
| Epoch [  7/ 10] Iter[301/391]		Loss: 0.4813
| Epoch [  7/ 10] Iter[321/391]		Loss: 0.3444
| Epoch [  7/ 10] Iter[341/391]		Loss: 0.4273
| Epoch [  7/ 10] Iter[361/391]		Loss: 0.3647
| Epoch [  7/ 10] Iter[381/391]		Loss: 0.5102
=> Early Training Epoch #8
| Epoch [  8/ 10] Iter[  1/391]		Loss: 0.3474
| Epoch [  8/ 10] Iter[ 21/391]		Loss: 0.2416
| Epoch [  8/ 10] Iter[ 41/391]		Loss: 0.3150
| Epoch [  8/ 10] Iter[ 61/391]		Loss: 0.2494
| Epoch [  8/ 10] Iter[ 81/391]		Loss: 0.3666
| Epoch [  8/ 10] Iter[101/391]		Loss: 0.3878
| Epoch [  8/ 10] Iter[121/391]		Loss: 0.2631
| Epoch [  8/ 10] Iter[141/391]		Loss: 0.4302
| Epoch [  8/ 10] Iter[161/391]		Loss: 0.3773
| Epoch [  8/ 10] Iter[181/391]		Loss: 0.3685
| Epoch [  8/ 10] Iter[201/391]		Loss: 0.3529
| Epoch [  8/ 10] Iter[221/391]		Loss: 0.3031
| Epoch [  8/ 10] Iter[241/391]		Loss: 0.3476
| Epoch [  8/ 10] Iter[261/391]		Loss: 0.4136
| Epoch [  8/ 10] Iter[281/391]		Loss: 0.2599
| Epoch [  8/ 10] Iter[301/391]		Loss: 0.3651
| Epoch [  8/ 10] Iter[321/391]		Loss: 0.2777
| Epoch [  8/ 10] Iter[341/391]		Loss: 0.3007
| Epoch [  8/ 10] Iter[361/391]		Loss: 0.3087
| Epoch [  8/ 10] Iter[381/391]		Loss: 0.3960
=> Early Training Epoch #9
| Epoch [  9/ 10] Iter[  1/391]		Loss: 0.3233
| Epoch [  9/ 10] Iter[ 21/391]		Loss: 0.2403
| Epoch [  9/ 10] Iter[ 41/391]		Loss: 0.2821
| Epoch [  9/ 10] Iter[ 61/391]		Loss: 0.1801
| Epoch [  9/ 10] Iter[ 81/391]		Loss: 0.1354
| Epoch [  9/ 10] Iter[101/391]		Loss: 0.2591
| Epoch [  9/ 10] Iter[121/391]		Loss: 0.2836
| Epoch [  9/ 10] Iter[141/391]		Loss: 0.1678
| Epoch [  9/ 10] Iter[161/391]		Loss: 0.2256
| Epoch [  9/ 10] Iter[181/391]		Loss: 0.3258
| Epoch [  9/ 10] Iter[201/391]		Loss: 0.5325
| Epoch [  9/ 10] Iter[221/391]		Loss: 0.3392
| Epoch [  9/ 10] Iter[241/391]		Loss: 0.2448
| Epoch [  9/ 10] Iter[261/391]		Loss: 0.2780
| Epoch [  9/ 10] Iter[281/391]		Loss: 0.3855
| Epoch [  9/ 10] Iter[301/391]		Loss: 0.4851
| Epoch [  9/ 10] Iter[321/391]		Loss: 0.5152
| Epoch [  9/ 10] Iter[341/391]		Loss: 0.4509
| Epoch [  9/ 10] Iter[361/391]		Loss: 0.3039
| Epoch [  9/ 10] Iter[381/391]		Loss: 0.2876
=> Early Training Epoch #0
| Epoch [  0/ 10] Iter[  1/391]		Loss: 2.5195
| Epoch [  0/ 10] Iter[ 21/391]		Loss: 2.1509
| Epoch [  0/ 10] Iter[ 41/391]		Loss: 2.1321
| Epoch [  0/ 10] Iter[ 61/391]		Loss: 1.9449
| Epoch [  0/ 10] Iter[ 81/391]		Loss: 1.8920
| Epoch [  0/ 10] Iter[101/391]		Loss: 1.8171
| Epoch [  0/ 10] Iter[121/391]		Loss: 1.7605
| Epoch [  0/ 10] Iter[141/391]		Loss: 1.7463
| Epoch [  0/ 10] Iter[161/391]		Loss: 1.6192
| Epoch [  0/ 10] Iter[181/391]		Loss: 1.7060
| Epoch [  0/ 10] Iter[201/391]		Loss: 1.5868
| Epoch [  0/ 10] Iter[221/391]		Loss: 1.4762
| Epoch [  0/ 10] Iter[241/391]		Loss: 1.6004
| Epoch [  0/ 10] Iter[261/391]		Loss: 1.6087
| Epoch [  0/ 10] Iter[281/391]		Loss: 1.5165
| Epoch [  0/ 10] Iter[301/391]		Loss: 1.2841
| Epoch [  0/ 10] Iter[321/391]		Loss: 1.5526
| Epoch [  0/ 10] Iter[341/391]		Loss: 1.3494
| Epoch [  0/ 10] Iter[361/391]		Loss: 1.2969
| Epoch [  0/ 10] Iter[381/391]		Loss: 1.4357
=> Early Training Epoch #1
| Epoch [  1/ 10] Iter[  1/391]		Loss: 1.3384
| Epoch [  1/ 10] Iter[ 21/391]		Loss: 1.3596
| Epoch [  1/ 10] Iter[ 41/391]		Loss: 1.2240
| Epoch [  1/ 10] Iter[ 61/391]		Loss: 1.2861
| Epoch [  1/ 10] Iter[ 81/391]		Loss: 1.2339
| Epoch [  1/ 10] Iter[101/391]		Loss: 1.2481
| Epoch [  1/ 10] Iter[121/391]		Loss: 1.3021
| Epoch [  1/ 10] Iter[141/391]		Loss: 1.2203
| Epoch [  1/ 10] Iter[161/391]		Loss: 1.2292
| Epoch [  1/ 10] Iter[181/391]		Loss: 1.2207
| Epoch [  1/ 10] Iter[201/391]		Loss: 1.0432
| Epoch [  1/ 10] Iter[221/391]		Loss: 1.3901
| Epoch [  1/ 10] Iter[241/391]		Loss: 1.2450
| Epoch [  1/ 10] Iter[261/391]		Loss: 1.1116
| Epoch [  1/ 10] Iter[281/391]		Loss: 1.3999
| Epoch [  1/ 10] Iter[301/391]		Loss: 1.2178
| Epoch [  1/ 10] Iter[321/391]		Loss: 0.9560
| Epoch [  1/ 10] Iter[341/391]		Loss: 1.1135
| Epoch [  1/ 10] Iter[361/391]		Loss: 1.1618
| Epoch [  1/ 10] Iter[381/391]		Loss: 0.9996
=> Early Training Epoch #2
| Epoch [  2/ 10] Iter[  1/391]		Loss: 0.9593
| Epoch [  2/ 10] Iter[ 21/391]		Loss: 1.0779
| Epoch [  2/ 10] Iter[ 41/391]		Loss: 0.9458
| Epoch [  2/ 10] Iter[ 61/391]		Loss: 1.0541
| Epoch [  2/ 10] Iter[ 81/391]		Loss: 1.0682
| Epoch [  2/ 10] Iter[101/391]		Loss: 1.1021
| Epoch [  2/ 10] Iter[121/391]		Loss: 0.9319
| Epoch [  2/ 10] Iter[141/391]		Loss: 1.0011
| Epoch [  2/ 10] Iter[161/391]		Loss: 1.0572
| Epoch [  2/ 10] Iter[181/391]		Loss: 1.1435
| Epoch [  2/ 10] Iter[201/391]		Loss: 1.0118
| Epoch [  2/ 10] Iter[221/391]		Loss: 0.7820
| Epoch [  2/ 10] Iter[241/391]		Loss: 0.8885
| Epoch [  2/ 10] Iter[261/391]		Loss: 0.8890
| Epoch [  2/ 10] Iter[281/391]		Loss: 0.9469
| Epoch [  2/ 10] Iter[301/391]		Loss: 0.9357
| Epoch [  2/ 10] Iter[321/391]		Loss: 0.6990
| Epoch [  2/ 10] Iter[341/391]		Loss: 0.8282
| Epoch [  2/ 10] Iter[361/391]		Loss: 0.8819
| Epoch [  2/ 10] Iter[381/391]		Loss: 0.9059
=> Early Training Epoch #3
| Epoch [  3/ 10] Iter[  1/391]		Loss: 0.7014
| Epoch [  3/ 10] Iter[ 21/391]		Loss: 0.6865
| Epoch [  3/ 10] Iter[ 41/391]		Loss: 0.7344
| Epoch [  3/ 10] Iter[ 61/391]		Loss: 0.8850
| Epoch [  3/ 10] Iter[ 81/391]		Loss: 0.8707
| Epoch [  3/ 10] Iter[101/391]		Loss: 0.7207
| Epoch [  3/ 10] Iter[121/391]		Loss: 0.8927
| Epoch [  3/ 10] Iter[141/391]		Loss: 0.7186
| Epoch [  3/ 10] Iter[161/391]		Loss: 0.8052
| Epoch [  3/ 10] Iter[181/391]		Loss: 0.8556
| Epoch [  3/ 10] Iter[201/391]		Loss: 0.6916
| Epoch [  3/ 10] Iter[221/391]		Loss: 0.9010
| Epoch [  3/ 10] Iter[241/391]		Loss: 0.8857
| Epoch [  3/ 10] Iter[261/391]		Loss: 0.5840
| Epoch [  3/ 10] Iter[281/391]		Loss: 0.6271
| Epoch [  3/ 10] Iter[301/391]		Loss: 0.7393
| Epoch [  3/ 10] Iter[321/391]		Loss: 0.6147
| Epoch [  3/ 10] Iter[341/391]		Loss: 0.8342
| Epoch [  3/ 10] Iter[361/391]		Loss: 0.5828
| Epoch [  3/ 10] Iter[381/391]		Loss: 0.7606
=> Early Training Epoch #4
| Epoch [  4/ 10] Iter[  1/391]		Loss: 0.7262
| Epoch [  4/ 10] Iter[ 21/391]		Loss: 0.5778
| Epoch [  4/ 10] Iter[ 41/391]		Loss: 0.7772
| Epoch [  4/ 10] Iter[ 61/391]		Loss: 0.6175
| Epoch [  4/ 10] Iter[ 81/391]		Loss: 0.7386
| Epoch [  4/ 10] Iter[101/391]		Loss: 0.6385
| Epoch [  4/ 10] Iter[121/391]		Loss: 0.6979
| Epoch [  4/ 10] Iter[141/391]		Loss: 0.6408
| Epoch [  4/ 10] Iter[161/391]		Loss: 0.6716
| Epoch [  4/ 10] Iter[181/391]		Loss: 0.5296
| Epoch [  4/ 10] Iter[201/391]		Loss: 0.5902
| Epoch [  4/ 10] Iter[221/391]		Loss: 0.6911
| Epoch [  4/ 10] Iter[241/391]		Loss: 0.5439
| Epoch [  4/ 10] Iter[261/391]		Loss: 0.6598
| Epoch [  4/ 10] Iter[281/391]		Loss: 0.8710
| Epoch [  4/ 10] Iter[301/391]		Loss: 0.6697
| Epoch [  4/ 10] Iter[321/391]		Loss: 0.5782
| Epoch [  4/ 10] Iter[341/391]		Loss: 0.7037
| Epoch [  4/ 10] Iter[361/391]		Loss: 0.7272
| Epoch [  4/ 10] Iter[381/391]		Loss: 0.6774
=> Early Training Epoch #5
| Epoch [  5/ 10] Iter[  1/391]		Loss: 0.4917
| Epoch [  5/ 10] Iter[ 21/391]		Loss: 0.3875
| Epoch [  5/ 10] Iter[ 41/391]		Loss: 0.5872
| Epoch [  5/ 10] Iter[ 61/391]		Loss: 0.4676
| Epoch [  5/ 10] Iter[ 81/391]		Loss: 0.6972
| Epoch [  5/ 10] Iter[101/391]		Loss: 0.5766
| Epoch [  5/ 10] Iter[121/391]		Loss: 0.6635
| Epoch [  5/ 10] Iter[141/391]		Loss: 0.5143
| Epoch [  5/ 10] Iter[161/391]		Loss: 0.5100
| Epoch [  5/ 10] Iter[181/391]		Loss: 0.5336
| Epoch [  5/ 10] Iter[201/391]		Loss: 0.7154
| Epoch [  5/ 10] Iter[221/391]		Loss: 0.6703
| Epoch [  5/ 10] Iter[241/391]		Loss: 0.6873
| Epoch [  5/ 10] Iter[261/391]		Loss: 0.5002
| Epoch [  5/ 10] Iter[281/391]		Loss: 0.6127
| Epoch [  5/ 10] Iter[301/391]		Loss: 0.4603
| Epoch [  5/ 10] Iter[321/391]		Loss: 0.4514
| Epoch [  5/ 10] Iter[341/391]		Loss: 0.7343
| Epoch [  5/ 10] Iter[361/391]		Loss: 0.5891
| Epoch [  5/ 10] Iter[381/391]		Loss: 0.5240
=> Early Training Epoch #6
| Epoch [  6/ 10] Iter[  1/391]		Loss: 0.5166
| Epoch [  6/ 10] Iter[ 21/391]		Loss: 0.2773
| Epoch [  6/ 10] Iter[ 41/391]		Loss: 0.4546
| Epoch [  6/ 10] Iter[ 61/391]		Loss: 0.3808
| Epoch [  6/ 10] Iter[ 81/391]		Loss: 0.5022
| Epoch [  6/ 10] Iter[101/391]		Loss: 0.4520
| Epoch [  6/ 10] Iter[121/391]		Loss: 0.4402
| Epoch [  6/ 10] Iter[141/391]		Loss: 0.4457
| Epoch [  6/ 10] Iter[161/391]		Loss: 0.4427
| Epoch [  6/ 10] Iter[181/391]		Loss: 0.5093
| Epoch [  6/ 10] Iter[201/391]		Loss: 0.4637
| Epoch [  6/ 10] Iter[221/391]		Loss: 0.4857
| Epoch [  6/ 10] Iter[241/391]		Loss: 0.4063
| Epoch [  6/ 10] Iter[261/391]		Loss: 0.3822
| Epoch [  6/ 10] Iter[281/391]		Loss: 0.5742
| Epoch [  6/ 10] Iter[301/391]		Loss: 0.5000
| Epoch [  6/ 10] Iter[321/391]		Loss: 0.5550
| Epoch [  6/ 10] Iter[341/391]		Loss: 0.4298
| Epoch [  6/ 10] Iter[361/391]		Loss: 0.4487
| Epoch [  6/ 10] Iter[381/391]		Loss: 0.5280
=> Early Training Epoch #7
| Epoch [  7/ 10] Iter[  1/391]		Loss: 0.2793
| Epoch [  7/ 10] Iter[ 21/391]		Loss: 0.3288
| Epoch [  7/ 10] Iter[ 41/391]		Loss: 0.3122
| Epoch [  7/ 10] Iter[ 61/391]		Loss: 0.2551
| Epoch [  7/ 10] Iter[ 81/391]		Loss: 0.3246
| Epoch [  7/ 10] Iter[101/391]		Loss: 0.3410
| Epoch [  7/ 10] Iter[121/391]		Loss: 0.2684
| Epoch [  7/ 10] Iter[141/391]		Loss: 0.3895
| Epoch [  7/ 10] Iter[161/391]		Loss: 0.3829
| Epoch [  7/ 10] Iter[181/391]		Loss: 0.3469
| Epoch [  7/ 10] Iter[201/391]		Loss: 0.3664
| Epoch [  7/ 10] Iter[221/391]		Loss: 0.2838
| Epoch [  7/ 10] Iter[241/391]		Loss: 0.3698
| Epoch [  7/ 10] Iter[261/391]		Loss: 0.3978
| Epoch [  7/ 10] Iter[281/391]		Loss: 0.4821
| Epoch [  7/ 10] Iter[301/391]		Loss: 0.4548
| Epoch [  7/ 10] Iter[321/391]		Loss: 0.4134
| Epoch [  7/ 10] Iter[341/391]		Loss: 0.4606
| Epoch [  7/ 10] Iter[361/391]		Loss: 0.3501
| Epoch [  7/ 10] Iter[381/391]		Loss: 0.4233
=> Early Training Epoch #8
| Epoch [  8/ 10] Iter[  1/391]		Loss: 0.2416
| Epoch [  8/ 10] Iter[ 21/391]		Loss: 0.2781
| Epoch [  8/ 10] Iter[ 41/391]		Loss: 0.2908
| Epoch [  8/ 10] Iter[ 61/391]		Loss: 0.3380
| Epoch [  8/ 10] Iter[ 81/391]		Loss: 0.3353
| Epoch [  8/ 10] Iter[101/391]		Loss: 0.5085
| Epoch [  8/ 10] Iter[121/391]		Loss: 0.2507
| Epoch [  8/ 10] Iter[141/391]		Loss: 0.1839
| Epoch [  8/ 10] Iter[161/391]		Loss: 0.3476
| Epoch [  8/ 10] Iter[181/391]		Loss: 0.2170
| Epoch [  8/ 10] Iter[201/391]		Loss: 0.3605
| Epoch [  8/ 10] Iter[221/391]		Loss: 0.3258
| Epoch [  8/ 10] Iter[241/391]		Loss: 0.3096
| Epoch [  8/ 10] Iter[261/391]		Loss: 0.3764
| Epoch [  8/ 10] Iter[281/391]		Loss: 0.3386
| Epoch [  8/ 10] Iter[301/391]		Loss: 0.5452
| Epoch [  8/ 10] Iter[321/391]		Loss: 0.2707
| Epoch [  8/ 10] Iter[341/391]		Loss: 0.3503
| Epoch [  8/ 10] Iter[361/391]		Loss: 0.3152
| Epoch [  8/ 10] Iter[381/391]		Loss: 0.3976
=> Early Training Epoch #9
| Epoch [  9/ 10] Iter[  1/391]		Loss: 0.2668
| Epoch [  9/ 10] Iter[ 21/391]		Loss: 0.1794
| Epoch [  9/ 10] Iter[ 41/391]		Loss: 0.2881
| Epoch [  9/ 10] Iter[ 61/391]		Loss: 0.2076
| Epoch [  9/ 10] Iter[ 81/391]		Loss: 0.2061
| Epoch [  9/ 10] Iter[101/391]		Loss: 0.3119
| Epoch [  9/ 10] Iter[121/391]		Loss: 0.3525
| Epoch [  9/ 10] Iter[141/391]		Loss: 0.2411
| Epoch [  9/ 10] Iter[161/391]		Loss: 0.5740
| Epoch [  9/ 10] Iter[181/391]		Loss: 0.3035
| Epoch [  9/ 10] Iter[201/391]		Loss: 0.3400
| Epoch [  9/ 10] Iter[221/391]		Loss: 0.3185
| Epoch [  9/ 10] Iter[241/391]		Loss: 0.4631
| Epoch [  9/ 10] Iter[261/391]		Loss: 0.3274
| Epoch [  9/ 10] Iter[281/391]		Loss: 0.3619
| Epoch [  9/ 10] Iter[301/391]		Loss: 0.3270
| Epoch [  9/ 10] Iter[321/391]		Loss: 0.2266
| Epoch [  9/ 10] Iter[341/391]		Loss: 0.2417
| Epoch [  9/ 10] Iter[361/391]		Loss: 0.2959
| Epoch [  9/ 10] Iter[381/391]		Loss: 0.3880
=> selecting time:  1212.2443182468414
=> number of seletcted samples:  10000
=> Saving checkpoint for epoch 0, with Prec@1 0.000000.
Epoch: [0][0/79]	Time 3.272 (3.272)	Loss 2.4930 (2.4930)	Prec@1 4.688 (4.688)
Epoch: [0][20/79]	Time 0.047 (0.200)	Loss 2.4941 (4.3615)	Prec@1 8.594 (10.231)
Epoch: [0][40/79]	Time 0.048 (0.125)	Loss 2.2445 (3.3777)	Prec@1 17.969 (11.319)
Epoch: [0][60/79]	Time 0.046 (0.100)	Loss 2.1848 (2.9974)	Prec@1 24.219 (13.038)
training time:  7.08679986000061
Test: [0/79]	Time 0.795 (0.795)	Loss 2.1953 (2.1953)	Prec@1 15.625 (15.625)
Test: [20/79]	Time 0.032 (0.069)	Loss 2.1921 (2.1963)	Prec@1 15.625 (15.885)
Test: [40/79]	Time 0.033 (0.054)	Loss 2.1841 (2.1907)	Prec@1 15.625 (16.178)
Test: [60/79]	Time 0.032 (0.047)	Loss 2.1026 (2.1909)	Prec@1 19.531 (16.381)
 * Prec@1 16.140
=> Saving checkpoint for epoch 0, with Prec@1 16.140000.
Epoch: [1][0/79]	Time 0.833 (0.833)	Loss 2.1309 (2.1309)	Prec@1 19.531 (19.531)
Epoch: [1][20/79]	Time 0.048 (0.084)	Loss 2.1671 (2.1783)	Prec@1 14.844 (15.811)
Epoch: [1][40/79]	Time 0.048 (0.065)	Loss 2.0862 (2.1696)	Prec@1 25.000 (16.654)
Epoch: [1][60/79]	Time 0.046 (0.059)	Loss 2.0428 (2.1526)	Prec@1 25.000 (17.572)
training time:  4.602166652679443
Test: [0/79]	Time 0.816 (0.816)	Loss 1.8939 (1.8939)	Prec@1 25.000 (25.000)
Test: [20/79]	Time 0.030 (0.067)	Loss 2.0197 (1.9799)	Prec@1 21.094 (24.330)
Test: [40/79]	Time 0.032 (0.049)	Loss 2.0315 (1.9747)	Prec@1 23.438 (24.848)
Test: [60/79]	Time 0.032 (0.044)	Loss 1.9466 (1.9729)	Prec@1 28.125 (24.872)
 * Prec@1 25.120
=> Saving checkpoint for epoch 1, with Prec@1 25.120000.
Epoch: [2][0/79]	Time 0.838 (0.838)	Loss 2.0714 (2.0714)	Prec@1 14.844 (14.844)
Epoch: [2][20/79]	Time 0.047 (0.084)	Loss 2.1141 (2.1050)	Prec@1 18.750 (19.978)
Epoch: [2][40/79]	Time 0.047 (0.066)	Loss 2.1205 (2.0981)	Prec@1 23.438 (20.846)
Epoch: [2][60/79]	Time 0.047 (0.060)	Loss 2.1123 (2.0916)	Prec@1 16.406 (20.223)
training time:  4.6034181118011475
Test: [0/79]	Time 0.799 (0.799)	Loss 1.8856 (1.8856)	Prec@1 21.094 (21.094)
Test: [20/79]	Time 0.032 (0.068)	Loss 1.9736 (1.9279)	Prec@1 20.312 (23.847)
Test: [40/79]	Time 0.032 (0.051)	Loss 1.9976 (1.9110)	Prec@1 23.438 (24.619)
Test: [60/79]	Time 0.031 (0.045)	Loss 1.8699 (1.9149)	Prec@1 26.562 (24.257)
 * Prec@1 24.270
Epoch: [3][0/79]	Time 0.844 (0.844)	Loss 2.1324 (2.1324)	Prec@1 16.406 (16.406)
Epoch: [3][20/79]	Time 0.046 (0.084)	Loss 2.0061 (2.0542)	Prec@1 24.219 (21.615)
Epoch: [3][40/79]	Time 0.046 (0.065)	Loss 2.0756 (2.0482)	Prec@1 23.438 (22.180)
Epoch: [3][60/79]	Time 0.045 (0.059)	Loss 2.0570 (2.0368)	Prec@1 22.656 (22.413)
training time:  4.568824768066406
Test: [0/79]	Time 0.794 (0.794)	Loss 1.8347 (1.8347)	Prec@1 25.781 (25.781)
Test: [20/79]	Time 0.032 (0.068)	Loss 1.9433 (1.8952)	Prec@1 25.000 (24.330)
Test: [40/79]	Time 0.032 (0.051)	Loss 1.9219 (1.8755)	Prec@1 24.219 (24.714)
Test: [60/79]	Time 0.031 (0.045)	Loss 1.9028 (1.8813)	Prec@1 26.562 (24.885)
 * Prec@1 25.350
=> Saving checkpoint for epoch 3, with Prec@1 25.350000.
Epoch: [4][0/79]	Time 0.835 (0.835)	Loss 2.1556 (2.1556)	Prec@1 20.312 (20.312)
Epoch: [4][20/79]	Time 0.046 (0.083)	Loss 2.0433 (2.0219)	Prec@1 19.531 (22.433)
Epoch: [4][40/79]	Time 0.045 (0.065)	Loss 1.9902 (2.0126)	Prec@1 25.781 (22.675)
Epoch: [4][60/79]	Time 0.048 (0.059)	Loss 1.9509 (2.0010)	Prec@1 23.438 (23.105)
training time:  4.574115991592407
Test: [0/79]	Time 0.767 (0.767)	Loss 1.9153 (1.9153)	Prec@1 20.312 (20.312)
Test: [20/79]	Time 0.032 (0.067)	Loss 1.9972 (1.9199)	Prec@1 23.438 (24.107)
Test: [40/79]	Time 0.032 (0.050)	Loss 1.9980 (1.9017)	Prec@1 14.844 (24.733)
Test: [60/79]	Time 0.031 (0.044)	Loss 1.9997 (1.9002)	Prec@1 17.188 (24.398)
 * Prec@1 24.520
Epoch: [5][0/79]	Time 0.809 (0.809)	Loss 2.0272 (2.0272)	Prec@1 22.656 (22.656)
Epoch: [5][20/79]	Time 0.046 (0.082)	Loss 1.9597 (2.0103)	Prec@1 25.000 (22.396)
Epoch: [5][40/79]	Time 0.045 (0.064)	Loss 2.0195 (1.9875)	Prec@1 15.625 (23.171)
Epoch: [5][60/79]	Time 0.047 (0.059)	Loss 2.0705 (1.9872)	Prec@1 19.531 (23.450)
training time:  4.553471326828003
Test: [0/79]	Time 0.795 (0.795)	Loss 1.7750 (1.7750)	Prec@1 31.250 (31.250)
Test: [20/79]	Time 0.164 (0.075)	Loss 1.9406 (1.8551)	Prec@1 24.219 (29.948)
Test: [40/79]	Time 0.032 (0.054)	Loss 1.9021 (1.8399)	Prec@1 25.781 (30.621)
Test: [60/79]	Time 0.031 (0.047)	Loss 1.8353 (1.8414)	Prec@1 33.594 (30.571)
 * Prec@1 30.410
=> Saving checkpoint for epoch 5, with Prec@1 30.410000.
Epoch: [6][0/79]	Time 0.802 (0.802)	Loss 2.0377 (2.0377)	Prec@1 21.875 (21.875)
Epoch: [6][20/79]	Time 0.043 (0.082)	Loss 1.8721 (1.9759)	Prec@1 27.344 (25.186)
Epoch: [6][40/79]	Time 0.045 (0.064)	Loss 1.9324 (1.9619)	Prec@1 25.781 (25.038)
Epoch: [6][60/79]	Time 0.044 (0.058)	Loss 1.9603 (1.9558)	Prec@1 25.000 (25.269)
training time:  4.5196473598480225
Test: [0/79]	Time 0.759 (0.759)	Loss 1.7286 (1.7286)	Prec@1 34.375 (34.375)
Test: [20/79]	Time 0.031 (0.067)	Loss 1.8233 (1.7789)	Prec@1 32.812 (31.734)
Test: [40/79]	Time 0.031 (0.050)	Loss 1.8460 (1.7655)	Prec@1 27.344 (32.146)
Test: [60/79]	Time 0.032 (0.044)	Loss 1.8146 (1.7651)	Prec@1 25.781 (31.788)
 * Prec@1 31.620
=> Saving checkpoint for epoch 6, with Prec@1 31.620000.
Epoch: [7][0/79]	Time 0.851 (0.851)	Loss 1.9745 (1.9745)	Prec@1 20.312 (20.312)
Epoch: [7][20/79]	Time 0.046 (0.085)	Loss 1.8733 (1.9201)	Prec@1 32.812 (26.339)
Epoch: [7][40/79]	Time 0.045 (0.066)	Loss 1.8604 (1.9300)	Prec@1 26.562 (26.029)
Epoch: [7][60/79]	Time 0.047 (0.060)	Loss 1.9483 (1.9329)	Prec@1 28.906 (26.076)
training time:  4.606189727783203
Test: [0/79]	Time 0.793 (0.793)	Loss 1.9085 (1.9085)	Prec@1 22.656 (22.656)
Test: [20/79]	Time 0.032 (0.068)	Loss 1.9318 (1.8833)	Prec@1 28.125 (28.646)
Test: [40/79]	Time 0.032 (0.050)	Loss 1.9415 (1.8687)	Prec@1 27.344 (29.002)
Test: [60/79]	Time 0.032 (0.044)	Loss 1.8738 (1.8717)	Prec@1 30.469 (29.150)
 * Prec@1 28.830
Epoch: [8][0/79]	Time 0.802 (0.802)	Loss 2.0930 (2.0930)	Prec@1 22.656 (22.656)
Epoch: [8][20/79]	Time 0.045 (0.082)	Loss 1.8891 (1.9367)	Prec@1 31.250 (27.009)
Epoch: [8][40/79]	Time 0.046 (0.065)	Loss 1.8464 (1.9190)	Prec@1 32.031 (27.306)
Epoch: [8][60/79]	Time 0.044 (0.059)	Loss 2.1482 (1.9172)	Prec@1 17.969 (26.934)
training time:  4.5913848876953125
Test: [0/79]	Time 0.761 (0.761)	Loss 1.8958 (1.8958)	Prec@1 26.562 (26.562)
Test: [20/79]	Time 0.032 (0.067)	Loss 1.9337 (1.8826)	Prec@1 22.656 (26.897)
Test: [40/79]	Time 0.032 (0.050)	Loss 1.9368 (1.8630)	Prec@1 21.094 (27.534)
Test: [60/79]	Time 0.032 (0.045)	Loss 1.9493 (1.8720)	Prec@1 23.438 (27.177)
 * Prec@1 27.140
Epoch: [9][0/79]	Time 0.805 (0.805)	Loss 1.9659 (1.9659)	Prec@1 25.000 (25.000)
Epoch: [9][20/79]	Time 0.045 (0.083)	Loss 1.9038 (1.9089)	Prec@1 28.125 (27.865)
Epoch: [9][40/79]	Time 0.045 (0.065)	Loss 1.8416 (1.9000)	Prec@1 27.344 (27.572)
Epoch: [9][60/79]	Time 0.047 (0.059)	Loss 1.8947 (1.8990)	Prec@1 30.469 (27.485)
training time:  4.572401285171509
Test: [0/79]	Time 0.774 (0.774)	Loss 1.6870 (1.6870)	Prec@1 37.500 (37.500)
Test: [20/79]	Time 0.030 (0.065)	Loss 1.7250 (1.7458)	Prec@1 36.719 (30.655)
Test: [40/79]	Time 0.029 (0.048)	Loss 1.8074 (1.7379)	Prec@1 32.812 (29.992)
Test: [60/79]	Time 0.032 (0.042)	Loss 1.7463 (1.7384)	Prec@1 32.812 (29.495)
 * Prec@1 29.620
Epoch: [10][0/79]	Time 0.803 (0.803)	Loss 1.9024 (1.9024)	Prec@1 30.469 (30.469)
Epoch: [10][20/79]	Time 0.046 (0.083)	Loss 1.8333 (1.8921)	Prec@1 31.250 (27.679)
Epoch: [10][40/79]	Time 0.048 (0.065)	Loss 1.8287 (1.8899)	Prec@1 26.562 (27.534)
Epoch: [10][60/79]	Time 0.045 (0.059)	Loss 1.8755 (1.8815)	Prec@1 29.688 (27.907)
training time:  4.558830499649048
Test: [0/79]	Time 0.804 (0.804)	Loss 1.7148 (1.7148)	Prec@1 29.688 (29.688)
Test: [20/79]	Time 0.032 (0.075)	Loss 1.8702 (1.7459)	Prec@1 31.250 (33.966)
Test: [40/79]	Time 0.032 (0.054)	Loss 1.8199 (1.7283)	Prec@1 30.469 (35.309)
Test: [60/79]	Time 0.032 (0.047)	Loss 1.7560 (1.7269)	Prec@1 37.500 (35.220)
 * Prec@1 34.920
=> Saving checkpoint for epoch 10, with Prec@1 34.920000.
Epoch: [11][0/79]	Time 0.795 (0.795)	Loss 1.9984 (1.9984)	Prec@1 23.438 (23.438)
Epoch: [11][20/79]	Time 0.047 (0.082)	Loss 1.8066 (1.8946)	Prec@1 36.719 (28.720)
Epoch: [11][40/79]	Time 0.046 (0.065)	Loss 1.8474 (1.8823)	Prec@1 28.125 (28.697)
Epoch: [11][60/79]	Time 0.042 (0.059)	Loss 1.8380 (1.8618)	Prec@1 26.562 (28.842)
training time:  4.546155214309692
Test: [0/79]	Time 0.772 (0.772)	Loss 1.6482 (1.6482)	Prec@1 37.500 (37.500)
Test: [20/79]	Time 0.032 (0.067)	Loss 1.6797 (1.6780)	Prec@1 39.062 (36.979)
Test: [40/79]	Time 0.032 (0.050)	Loss 1.7968 (1.6686)	Prec@1 30.469 (37.481)
Test: [60/79]	Time 0.031 (0.045)	Loss 1.6295 (1.6692)	Prec@1 40.625 (37.513)
 * Prec@1 37.230
=> Saving checkpoint for epoch 11, with Prec@1 37.230000.
Epoch: [12][0/79]	Time 0.794 (0.794)	Loss 1.9090 (1.9090)	Prec@1 25.000 (25.000)
Epoch: [12][20/79]	Time 0.045 (0.082)	Loss 1.8061 (1.8365)	Prec@1 28.125 (28.795)
Epoch: [12][40/79]	Time 0.045 (0.064)	Loss 1.7948 (1.8484)	Prec@1 32.812 (29.002)
Epoch: [12][60/79]	Time 0.047 (0.062)	Loss 1.9047 (1.8415)	Prec@1 28.125 (29.316)
training time:  4.763145208358765
Test: [0/79]	Time 0.765 (0.765)	Loss 1.6519 (1.6519)	Prec@1 35.938 (35.938)
Test: [20/79]	Time 0.032 (0.066)	Loss 1.6155 (1.6503)	Prec@1 40.625 (35.900)
Test: [40/79]	Time 0.031 (0.050)	Loss 1.7580 (1.6440)	Prec@1 32.031 (36.871)
Test: [60/79]	Time 0.031 (0.044)	Loss 1.5714 (1.6411)	Prec@1 42.969 (36.706)
 * Prec@1 36.730
Epoch: [13][0/79]	Time 0.817 (0.817)	Loss 1.8933 (1.8933)	Prec@1 28.125 (28.125)
Epoch: [13][20/79]	Time 0.046 (0.089)	Loss 1.7890 (1.8208)	Prec@1 32.812 (29.129)
Epoch: [13][40/79]	Time 0.046 (0.068)	Loss 1.8424 (1.8264)	Prec@1 34.375 (30.069)
Epoch: [13][60/79]	Time 0.046 (0.061)	Loss 1.7911 (1.8177)	Prec@1 36.719 (30.213)
training time:  4.70538592338562
Test: [0/79]	Time 0.770 (0.770)	Loss 1.6941 (1.6941)	Prec@1 39.844 (39.844)
Test: [20/79]	Time 0.032 (0.067)	Loss 1.6710 (1.6984)	Prec@1 42.188 (34.673)
Test: [40/79]	Time 0.032 (0.050)	Loss 1.8127 (1.6948)	Prec@1 28.125 (34.680)
Test: [60/79]	Time 0.031 (0.044)	Loss 1.7847 (1.7025)	Prec@1 25.000 (34.183)
 * Prec@1 34.140
Epoch: [14][0/79]	Time 0.802 (0.802)	Loss 1.9693 (1.9693)	Prec@1 25.000 (25.000)
Epoch: [14][20/79]	Time 0.046 (0.080)	Loss 1.6604 (1.8502)	Prec@1 37.500 (29.501)
Epoch: [14][40/79]	Time 0.044 (0.063)	Loss 1.7541 (1.8328)	Prec@1 28.906 (30.050)
Epoch: [14][60/79]	Time 0.046 (0.057)	Loss 1.8872 (1.8210)	Prec@1 24.219 (30.379)
training time:  4.465611696243286
Test: [0/79]	Time 0.775 (0.775)	Loss 1.6060 (1.6060)	Prec@1 39.844 (39.844)
Test: [20/79]	Time 0.032 (0.067)	Loss 1.6057 (1.6350)	Prec@1 36.719 (36.719)
Test: [40/79]	Time 0.033 (0.050)	Loss 1.7141 (1.6266)	Prec@1 30.469 (36.700)
Test: [60/79]	Time 0.031 (0.044)	Loss 1.6227 (1.6262)	Prec@1 39.844 (36.514)
 * Prec@1 36.260
Epoch: [15][0/79]	Time 0.802 (0.802)	Loss 1.8011 (1.8011)	Prec@1 36.719 (36.719)
Epoch: [15][20/79]	Time 0.046 (0.082)	Loss 1.7569 (1.7714)	Prec@1 33.594 (32.143)
Epoch: [15][40/79]	Time 0.046 (0.064)	Loss 1.6510 (1.7782)	Prec@1 31.250 (31.307)
Epoch: [15][60/79]	Time 0.046 (0.058)	Loss 1.7447 (1.7773)	Prec@1 29.688 (31.942)
training time:  4.48780369758606
Test: [0/79]	Time 0.770 (0.770)	Loss 1.5579 (1.5579)	Prec@1 35.156 (35.156)
Test: [20/79]	Time 0.030 (0.067)	Loss 1.5753 (1.5410)	Prec@1 38.281 (40.141)
Test: [40/79]	Time 0.033 (0.050)	Loss 1.6393 (1.5340)	Prec@1 32.812 (40.244)
Test: [60/79]	Time 0.030 (0.044)	Loss 1.5087 (1.5374)	Prec@1 39.062 (40.535)
 * Prec@1 40.270
=> Saving checkpoint for epoch 15, with Prec@1 40.270000.
Epoch: [16][0/79]	Time 0.852 (0.852)	Loss 1.8389 (1.8389)	Prec@1 27.344 (27.344)
Epoch: [16][20/79]	Time 0.047 (0.084)	Loss 1.8605 (1.7724)	Prec@1 28.125 (31.920)
Epoch: [16][40/79]	Time 0.047 (0.066)	Loss 1.7458 (1.7529)	Prec@1 28.906 (33.289)
Epoch: [16][60/79]	Time 0.046 (0.059)	Loss 1.8819 (1.7563)	Prec@1 35.156 (32.889)
training time:  4.6111814975738525
Test: [0/79]	Time 0.824 (0.824)	Loss 1.4580 (1.4580)	Prec@1 43.750 (43.750)
Test: [20/79]	Time 0.034 (0.069)	Loss 1.5516 (1.5251)	Prec@1 45.312 (42.001)
Test: [40/79]	Time 0.032 (0.051)	Loss 1.6213 (1.5301)	Prec@1 34.375 (41.254)
Test: [60/79]	Time 0.031 (0.045)	Loss 1.5317 (1.5338)	Prec@1 43.750 (40.446)
 * Prec@1 40.660
=> Saving checkpoint for epoch 16, with Prec@1 40.660000.
Epoch: [17][0/79]	Time 0.867 (0.867)	Loss 1.7815 (1.7815)	Prec@1 31.250 (31.250)
Epoch: [17][20/79]	Time 0.046 (0.085)	Loss 1.7769 (1.7617)	Prec@1 31.250 (32.180)
Epoch: [17][40/79]	Time 0.044 (0.065)	Loss 1.7567 (1.7469)	Prec@1 30.469 (33.422)
Epoch: [17][60/79]	Time 0.043 (0.058)	Loss 1.6204 (1.7410)	Prec@1 32.031 (33.299)
training time:  4.527163982391357
Test: [0/79]	Time 0.824 (0.824)	Loss 1.6465 (1.6465)	Prec@1 39.844 (39.844)
Test: [20/79]	Time 0.031 (0.068)	Loss 1.5709 (1.5945)	Prec@1 40.625 (38.765)
Test: [40/79]	Time 0.030 (0.050)	Loss 1.7152 (1.5819)	Prec@1 32.031 (39.310)
Test: [60/79]	Time 0.031 (0.044)	Loss 1.5977 (1.5854)	Prec@1 33.594 (39.075)
 * Prec@1 38.910
Epoch: [18][0/79]	Time 0.859 (0.859)	Loss 1.8097 (1.8097)	Prec@1 28.906 (28.906)
Epoch: [18][20/79]	Time 0.049 (0.085)	Loss 1.7410 (1.7858)	Prec@1 34.375 (31.176)
Epoch: [18][40/79]	Time 0.047 (0.066)	Loss 1.6853 (1.7529)	Prec@1 38.281 (33.956)
Epoch: [18][60/79]	Time 0.046 (0.059)	Loss 1.7453 (1.7438)	Prec@1 34.375 (34.260)
training time:  4.6078832149505615
Test: [0/79]	Time 0.777 (0.777)	Loss 1.6985 (1.6985)	Prec@1 37.500 (37.500)
Test: [20/79]	Time 0.032 (0.073)	Loss 1.7064 (1.7096)	Prec@1 35.938 (36.161)
Test: [40/79]	Time 0.033 (0.053)	Loss 1.8336 (1.7065)	Prec@1 32.031 (36.852)
Test: [60/79]	Time 0.031 (0.046)	Loss 1.7388 (1.7118)	Prec@1 35.156 (36.206)
 * Prec@1 36.080
Epoch: [19][0/79]	Time 0.811 (0.811)	Loss 1.6793 (1.6793)	Prec@1 36.719 (36.719)
Epoch: [19][20/79]	Time 0.044 (0.082)	Loss 1.6401 (1.7099)	Prec@1 35.156 (33.780)
Epoch: [19][40/79]	Time 0.044 (0.064)	Loss 1.8081 (1.6969)	Prec@1 28.906 (34.508)
Epoch: [19][60/79]	Time 0.049 (0.058)	Loss 1.7835 (1.6914)	Prec@1 32.031 (34.900)
training time:  4.527884006500244
Test: [0/79]	Time 0.771 (0.771)	Loss 1.6054 (1.6054)	Prec@1 37.500 (37.500)
Test: [20/79]	Time 0.032 (0.067)	Loss 1.5937 (1.6590)	Prec@1 40.625 (36.347)
Test: [40/79]	Time 0.033 (0.050)	Loss 1.7960 (1.6541)	Prec@1 36.719 (36.795)
Test: [60/79]	Time 0.031 (0.044)	Loss 1.7055 (1.6641)	Prec@1 34.375 (36.206)
 * Prec@1 36.330
Epoch: [20][0/79]	Time 0.821 (0.821)	Loss 1.6637 (1.6637)	Prec@1 37.500 (37.500)
Epoch: [20][20/79]	Time 0.046 (0.083)	Loss 1.6549 (1.6655)	Prec@1 36.719 (35.863)
Epoch: [20][40/79]	Time 0.047 (0.065)	Loss 1.7611 (1.6790)	Prec@1 31.250 (35.575)
Epoch: [20][60/79]	Time 0.043 (0.059)	Loss 1.5775 (1.6804)	Prec@1 41.406 (35.553)
training time:  4.580230712890625
Test: [0/79]	Time 0.816 (0.816)	Loss 1.4629 (1.4629)	Prec@1 42.969 (42.969)
Test: [20/79]	Time 0.032 (0.069)	Loss 1.5483 (1.5742)	Prec@1 42.969 (39.509)
Test: [40/79]	Time 0.032 (0.051)	Loss 1.6704 (1.5673)	Prec@1 34.375 (40.415)
Test: [60/79]	Time 0.028 (0.044)	Loss 1.6354 (1.5717)	Prec@1 33.594 (39.754)
 * Prec@1 39.670
Epoch: [21][0/79]	Time 0.810 (0.810)	Loss 1.6668 (1.6668)	Prec@1 37.500 (37.500)
Epoch: [21][20/79]	Time 0.048 (0.082)	Loss 1.6687 (1.6626)	Prec@1 35.938 (36.868)
Epoch: [21][40/79]	Time 0.048 (0.064)	Loss 1.4902 (1.6343)	Prec@1 42.969 (37.843)
Epoch: [21][60/79]	Time 0.045 (0.058)	Loss 1.7823 (1.6471)	Prec@1 30.469 (37.282)
training time:  4.495767593383789
Test: [0/79]	Time 0.817 (0.817)	Loss 1.3734 (1.3734)	Prec@1 50.000 (50.000)
Test: [20/79]	Time 0.030 (0.067)	Loss 1.4073 (1.4189)	Prec@1 46.094 (45.945)
Test: [40/79]	Time 0.033 (0.050)	Loss 1.5195 (1.4162)	Prec@1 39.062 (45.808)
Test: [60/79]	Time 0.031 (0.044)	Loss 1.4362 (1.4230)	Prec@1 39.062 (44.877)
 * Prec@1 44.610
=> Saving checkpoint for epoch 21, with Prec@1 44.610000.
Epoch: [22][0/79]	Time 0.853 (0.853)	Loss 1.6250 (1.6250)	Prec@1 35.938 (35.938)
Epoch: [22][20/79]	Time 0.046 (0.084)	Loss 1.5312 (1.6176)	Prec@1 46.875 (38.951)
Epoch: [22][40/79]	Time 0.043 (0.065)	Loss 1.8130 (1.6180)	Prec@1 34.375 (38.148)
Epoch: [22][60/79]	Time 0.046 (0.059)	Loss 1.7642 (1.6210)	Prec@1 35.156 (37.782)
training time:  4.587085247039795
Test: [0/79]	Time 0.812 (0.812)	Loss 1.4183 (1.4183)	Prec@1 42.188 (42.188)
Test: [20/79]	Time 0.031 (0.069)	Loss 1.3965 (1.4083)	Prec@1 46.094 (45.387)
Test: [40/79]	Time 0.031 (0.051)	Loss 1.4654 (1.3990)	Prec@1 40.625 (45.827)
Test: [60/79]	Time 0.032 (0.045)	Loss 1.4290 (1.4003)	Prec@1 43.750 (46.107)
 * Prec@1 46.080
=> Saving checkpoint for epoch 22, with Prec@1 46.080000.
Epoch: [23][0/79]	Time 0.846 (0.846)	Loss 1.5366 (1.5366)	Prec@1 42.969 (42.969)
Epoch: [23][20/79]	Time 0.047 (0.084)	Loss 1.5911 (1.6041)	Prec@1 39.844 (39.881)
Epoch: [23][40/79]	Time 0.048 (0.066)	Loss 1.7254 (1.5973)	Prec@1 32.812 (39.806)
Epoch: [23][60/79]	Time 0.044 (0.058)	Loss 1.7398 (1.6008)	Prec@1 33.594 (39.536)
training time:  4.5385963916778564
Test: [0/79]	Time 0.817 (0.817)	Loss 1.4480 (1.4480)	Prec@1 42.969 (42.969)
Test: [20/79]	Time 0.032 (0.069)	Loss 1.3838 (1.4317)	Prec@1 49.219 (44.866)
Test: [40/79]	Time 0.031 (0.051)	Loss 1.4745 (1.4286)	Prec@1 36.719 (44.131)
Test: [60/79]	Time 0.032 (0.045)	Loss 1.4464 (1.4351)	Prec@1 44.531 (43.865)
 * Prec@1 43.520
Epoch: [24][0/79]	Time 0.850 (0.850)	Loss 1.6008 (1.6008)	Prec@1 36.719 (36.719)
Epoch: [24][20/79]	Time 0.047 (0.091)	Loss 1.5374 (1.5934)	Prec@1 41.406 (38.467)
Epoch: [24][40/79]	Time 0.045 (0.069)	Loss 1.4489 (1.5919)	Prec@1 46.875 (39.177)
Epoch: [24][60/79]	Time 0.045 (0.062)	Loss 1.4964 (1.5805)	Prec@1 40.625 (39.921)
training time:  4.737027406692505
Test: [0/79]	Time 0.817 (0.817)	Loss 1.3550 (1.3550)	Prec@1 41.406 (41.406)
Test: [20/79]	Time 0.031 (0.069)	Loss 1.3608 (1.3928)	Prec@1 52.344 (45.945)
Test: [40/79]	Time 0.031 (0.051)	Loss 1.4360 (1.3952)	Prec@1 40.625 (45.427)
Test: [60/79]	Time 0.032 (0.045)	Loss 1.4671 (1.4017)	Prec@1 39.844 (44.723)
 * Prec@1 44.470
Epoch: [25][0/79]	Time 0.874 (0.874)	Loss 1.5360 (1.5360)	Prec@1 49.219 (49.219)
Epoch: [25][20/79]	Time 0.046 (0.086)	Loss 1.4296 (1.5602)	Prec@1 43.750 (40.439)
Epoch: [25][40/79]	Time 0.047 (0.068)	Loss 1.5233 (1.5556)	Prec@1 44.531 (40.644)
Epoch: [25][60/79]	Time 0.047 (0.061)	Loss 1.5797 (1.5513)	Prec@1 34.375 (40.561)
training time:  4.695317506790161
Test: [0/79]	Time 0.809 (0.809)	Loss 1.5881 (1.5881)	Prec@1 41.406 (41.406)
Test: [20/79]	Time 0.032 (0.069)	Loss 1.5530 (1.5591)	Prec@1 43.750 (41.890)
Test: [40/79]	Time 0.032 (0.051)	Loss 1.5998 (1.5584)	Prec@1 37.500 (41.425)
Test: [60/79]	Time 0.031 (0.045)	Loss 1.5766 (1.5574)	Prec@1 35.156 (41.240)
 * Prec@1 41.240
Epoch: [26][0/79]	Time 0.840 (0.840)	Loss 1.5966 (1.5966)	Prec@1 44.531 (44.531)
Epoch: [26][20/79]	Time 0.047 (0.084)	Loss 1.4395 (1.5460)	Prec@1 49.219 (40.253)
Epoch: [26][40/79]	Time 0.047 (0.066)	Loss 1.6077 (1.5486)	Prec@1 42.969 (41.139)
Epoch: [26][60/79]	Time 0.046 (0.059)	Loss 1.5945 (1.5411)	Prec@1 41.406 (41.060)
training time:  4.584769248962402
Test: [0/79]	Time 0.807 (0.807)	Loss 1.3553 (1.3553)	Prec@1 50.000 (50.000)
Test: [20/79]	Time 0.033 (0.069)	Loss 1.3773 (1.3949)	Prec@1 46.094 (46.094)
Test: [40/79]	Time 0.032 (0.051)	Loss 1.4178 (1.3981)	Prec@1 45.312 (45.903)
Test: [60/79]	Time 0.031 (0.045)	Loss 1.4282 (1.3956)	Prec@1 41.406 (46.068)
 * Prec@1 45.860
Epoch: [27][0/79]	Time 0.843 (0.843)	Loss 1.5417 (1.5417)	Prec@1 39.844 (39.844)
Epoch: [27][20/79]	Time 0.048 (0.085)	Loss 1.4190 (1.5100)	Prec@1 49.219 (43.415)
Epoch: [27][40/79]	Time 0.046 (0.066)	Loss 1.4647 (1.5158)	Prec@1 41.406 (42.683)
Epoch: [27][60/79]	Time 0.046 (0.060)	Loss 1.5170 (1.5153)	Prec@1 41.406 (42.405)
training time:  4.61450982093811
Test: [0/79]	Time 0.782 (0.782)	Loss 1.4247 (1.4247)	Prec@1 45.312 (45.312)
Test: [20/79]	Time 0.032 (0.067)	Loss 1.3910 (1.4555)	Prec@1 47.656 (42.857)
Test: [40/79]	Time 0.032 (0.050)	Loss 1.5630 (1.4553)	Prec@1 38.281 (42.321)
Test: [60/79]	Time 0.032 (0.044)	Loss 1.4919 (1.4599)	Prec@1 35.938 (41.919)
 * Prec@1 42.080
Epoch: [28][0/79]	Time 0.819 (0.819)	Loss 1.4391 (1.4391)	Prec@1 46.094 (46.094)
Epoch: [28][20/79]	Time 0.050 (0.083)	Loss 1.6457 (1.4976)	Prec@1 33.594 (43.006)
Epoch: [28][40/79]	Time 0.047 (0.065)	Loss 1.6183 (1.4721)	Prec@1 41.406 (43.521)
Epoch: [28][60/79]	Time 0.045 (0.059)	Loss 1.5496 (1.4765)	Prec@1 42.969 (43.558)
training time:  4.555318593978882
Test: [0/79]	Time 0.777 (0.777)	Loss 1.3168 (1.3168)	Prec@1 46.875 (46.875)
Test: [20/79]	Time 0.032 (0.067)	Loss 1.2974 (1.3681)	Prec@1 53.906 (47.247)
Test: [40/79]	Time 0.032 (0.050)	Loss 1.5432 (1.3768)	Prec@1 35.156 (46.361)
Test: [60/79]	Time 0.031 (0.044)	Loss 1.4299 (1.3864)	Prec@1 40.625 (45.428)
 * Prec@1 45.260
Epoch: [29][0/79]	Time 0.824 (0.824)	Loss 1.5808 (1.5808)	Prec@1 43.750 (43.750)
Epoch: [29][20/79]	Time 0.044 (0.083)	Loss 1.4910 (1.4994)	Prec@1 45.312 (43.564)
Epoch: [29][40/79]	Time 0.046 (0.065)	Loss 1.4472 (1.4767)	Prec@1 45.312 (44.531)
Epoch: [29][60/79]	Time 0.046 (0.059)	Loss 1.6140 (1.4745)	Prec@1 37.500 (43.904)
training time:  4.545086145401001
Test: [0/79]	Time 0.781 (0.781)	Loss 1.6159 (1.6159)	Prec@1 35.938 (35.938)
Test: [20/79]	Time 0.032 (0.074)	Loss 1.5856 (1.7400)	Prec@1 40.625 (38.914)
Test: [40/79]	Time 0.032 (0.054)	Loss 1.8570 (1.7598)	Prec@1 34.375 (37.595)
Test: [60/79]	Time 0.031 (0.047)	Loss 1.8083 (1.7720)	Prec@1 36.719 (37.474)
 * Prec@1 37.470
Epoch: [30][0/79]	Time 0.807 (0.807)	Loss 1.5438 (1.5438)	Prec@1 34.375 (34.375)
Epoch: [30][20/79]	Time 0.044 (0.083)	Loss 1.2768 (1.4453)	Prec@1 50.781 (45.610)
Epoch: [30][40/79]	Time 0.047 (0.065)	Loss 1.4590 (1.4399)	Prec@1 46.094 (45.293)
Epoch: [30][60/79]	Time 0.046 (0.059)	Loss 1.3706 (1.4352)	Prec@1 42.188 (45.274)
training time:  4.567076683044434
Test: [0/79]	Time 0.802 (0.802)	Loss 1.2933 (1.2933)	Prec@1 48.438 (48.438)
Test: [20/79]	Time 0.032 (0.069)	Loss 1.1786 (1.3770)	Prec@1 56.250 (47.321)
Test: [40/79]	Time 0.032 (0.051)	Loss 1.4287 (1.3614)	Prec@1 43.750 (47.504)
Test: [60/79]	Time 0.031 (0.045)	Loss 1.3957 (1.3620)	Prec@1 46.875 (47.285)
 * Prec@1 46.930
=> Saving checkpoint for epoch 30, with Prec@1 46.930000.
Epoch: [31][0/79]	Time 0.850 (0.850)	Loss 1.4052 (1.4052)	Prec@1 49.219 (49.219)
Epoch: [31][20/79]	Time 0.046 (0.085)	Loss 1.3212 (1.4203)	Prec@1 46.875 (47.545)
Epoch: [31][40/79]	Time 0.048 (0.066)	Loss 1.3345 (1.4073)	Prec@1 46.875 (47.351)
Epoch: [31][60/79]	Time 0.047 (0.059)	Loss 1.3917 (1.4048)	Prec@1 48.438 (46.926)
training time:  4.602608680725098
Test: [0/79]	Time 0.832 (0.832)	Loss 1.3367 (1.3367)	Prec@1 46.094 (46.094)
Test: [20/79]	Time 0.032 (0.069)	Loss 1.2428 (1.4383)	Prec@1 55.469 (45.536)
Test: [40/79]	Time 0.032 (0.051)	Loss 1.4749 (1.4349)	Prec@1 40.625 (45.103)
Test: [60/79]	Time 0.033 (0.045)	Loss 1.5123 (1.4354)	Prec@1 41.406 (45.184)
 * Prec@1 44.900
Epoch: [32][0/79]	Time 0.860 (0.860)	Loss 1.4283 (1.4283)	Prec@1 43.750 (43.750)
Epoch: [32][20/79]	Time 0.047 (0.085)	Loss 1.4403 (1.3970)	Prec@1 42.969 (47.247)
Epoch: [32][40/79]	Time 0.046 (0.066)	Loss 1.2273 (1.3787)	Prec@1 53.906 (47.809)
Epoch: [32][60/79]	Time 0.046 (0.059)	Loss 1.5896 (1.3761)	Prec@1 41.406 (47.631)
training time:  4.599982500076294
Test: [0/79]	Time 0.827 (0.827)	Loss 1.4271 (1.4271)	Prec@1 53.906 (53.906)
Test: [20/79]	Time 0.032 (0.070)	Loss 1.3405 (1.5548)	Prec@1 50.000 (42.522)
Test: [40/79]	Time 0.031 (0.051)	Loss 1.6592 (1.5528)	Prec@1 39.844 (42.073)
Test: [60/79]	Time 0.031 (0.045)	Loss 1.6173 (1.5600)	Prec@1 40.625 (42.008)
 * Prec@1 42.030
Epoch: [33][0/79]	Time 0.851 (0.851)	Loss 1.3886 (1.3886)	Prec@1 41.406 (41.406)
Epoch: [33][20/79]	Time 0.049 (0.085)	Loss 1.3548 (1.3683)	Prec@1 43.750 (47.135)
Epoch: [33][40/79]	Time 0.049 (0.066)	Loss 1.2210 (1.3570)	Prec@1 54.688 (47.866)
Epoch: [33][60/79]	Time 0.045 (0.060)	Loss 1.2573 (1.3451)	Prec@1 50.000 (48.617)
training time:  4.613480567932129
Test: [0/79]	Time 0.827 (0.827)	Loss 1.1103 (1.1103)	Prec@1 60.938 (60.938)
Test: [20/79]	Time 0.031 (0.070)	Loss 1.1233 (1.2327)	Prec@1 58.594 (52.530)
Test: [40/79]	Time 0.031 (0.052)	Loss 1.2857 (1.2195)	Prec@1 48.438 (53.125)
Test: [60/79]	Time 0.032 (0.045)	Loss 1.2751 (1.2293)	Prec@1 53.125 (52.728)
 * Prec@1 52.610
=> Saving checkpoint for epoch 33, with Prec@1 52.610000.
Epoch: [34][0/79]	Time 0.851 (0.851)	Loss 1.2814 (1.2814)	Prec@1 49.219 (49.219)
Epoch: [34][20/79]	Time 0.048 (0.084)	Loss 1.2100 (1.3221)	Prec@1 52.344 (48.921)
Epoch: [34][40/79]	Time 0.047 (0.066)	Loss 1.2391 (1.3178)	Prec@1 50.781 (49.486)
Epoch: [34][60/79]	Time 0.046 (0.059)	Loss 1.2281 (1.3175)	Prec@1 51.562 (49.667)
training time:  4.616741895675659
Test: [0/79]	Time 0.819 (0.819)	Loss 1.2303 (1.2303)	Prec@1 57.812 (57.812)
Test: [20/79]	Time 0.034 (0.069)	Loss 1.1341 (1.3304)	Prec@1 60.938 (51.079)
Test: [40/79]	Time 0.032 (0.051)	Loss 1.4387 (1.3380)	Prec@1 46.875 (50.438)
Test: [60/79]	Time 0.032 (0.045)	Loss 1.3520 (1.3406)	Prec@1 45.312 (49.821)
 * Prec@1 49.920
Epoch: [35][0/79]	Time 0.850 (0.850)	Loss 1.2638 (1.2638)	Prec@1 52.344 (52.344)
Epoch: [35][20/79]	Time 0.045 (0.089)	Loss 1.5693 (1.3075)	Prec@1 45.312 (51.451)
Epoch: [35][40/79]	Time 0.043 (0.067)	Loss 1.3589 (1.2858)	Prec@1 48.438 (51.334)
Epoch: [35][60/79]	Time 0.044 (0.060)	Loss 1.3473 (1.2897)	Prec@1 50.781 (51.217)
training time:  4.5988383293151855
Test: [0/79]	Time 0.782 (0.782)	Loss 1.3404 (1.3404)	Prec@1 48.438 (48.438)
Test: [20/79]	Time 0.032 (0.068)	Loss 1.3412 (1.5115)	Prec@1 52.344 (46.057)
Test: [40/79]	Time 0.032 (0.050)	Loss 1.4391 (1.4991)	Prec@1 49.219 (46.361)
Test: [60/79]	Time 0.031 (0.045)	Loss 1.7099 (1.5144)	Prec@1 36.719 (46.030)
 * Prec@1 46.040
Epoch: [36][0/79]	Time 0.809 (0.809)	Loss 1.2340 (1.2340)	Prec@1 53.125 (53.125)
Epoch: [36][20/79]	Time 0.047 (0.082)	Loss 1.2210 (1.2513)	Prec@1 50.781 (52.269)
Epoch: [36][40/79]	Time 0.046 (0.064)	Loss 1.2869 (1.2355)	Prec@1 46.875 (52.401)
Epoch: [36][60/79]	Time 0.045 (0.058)	Loss 1.2252 (1.2346)	Prec@1 60.156 (52.587)
training time:  4.5574469566345215
Test: [0/79]	Time 0.781 (0.781)	Loss 1.1790 (1.1790)	Prec@1 53.906 (53.906)
Test: [20/79]	Time 0.032 (0.066)	Loss 1.1663 (1.2244)	Prec@1 50.781 (53.162)
Test: [40/79]	Time 0.032 (0.049)	Loss 1.2362 (1.2245)	Prec@1 50.781 (52.687)
Test: [60/79]	Time 0.030 (0.044)	Loss 1.2321 (1.2296)	Prec@1 46.094 (52.357)
 * Prec@1 52.130
Epoch: [37][0/79]	Time 0.819 (0.819)	Loss 1.2128 (1.2128)	Prec@1 51.562 (51.562)
Epoch: [37][20/79]	Time 0.048 (0.082)	Loss 1.2358 (1.2551)	Prec@1 52.344 (51.897)
Epoch: [37][40/79]	Time 0.047 (0.065)	Loss 1.2637 (1.2421)	Prec@1 50.000 (52.572)
Epoch: [37][60/79]	Time 0.045 (0.059)	Loss 1.2647 (1.2396)	Prec@1 46.875 (52.600)
training time:  4.576132774353027
Test: [0/79]	Time 0.773 (0.773)	Loss 1.3165 (1.3165)	Prec@1 53.125 (53.125)
Test: [20/79]	Time 0.032 (0.067)	Loss 1.1707 (1.3630)	Prec@1 57.031 (51.004)
Test: [40/79]	Time 0.032 (0.050)	Loss 1.3804 (1.3482)	Prec@1 48.438 (50.095)
Test: [60/79]	Time 0.032 (0.044)	Loss 1.2928 (1.3435)	Prec@1 48.438 (49.923)
 * Prec@1 49.470
Epoch: [38][0/79]	Time 0.825 (0.825)	Loss 1.3968 (1.3968)	Prec@1 47.656 (47.656)
Epoch: [38][20/79]	Time 0.046 (0.083)	Loss 1.4004 (1.2251)	Prec@1 39.062 (52.827)
Epoch: [38][40/79]	Time 0.045 (0.065)	Loss 1.2681 (1.1975)	Prec@1 50.000 (54.459)
Epoch: [38][60/79]	Time 0.047 (0.059)	Loss 1.0866 (1.1978)	Prec@1 55.469 (53.970)
training time:  4.552683591842651
Test: [0/79]	Time 0.806 (0.806)	Loss 1.2275 (1.2275)	Prec@1 55.469 (55.469)
Test: [20/79]	Time 0.032 (0.068)	Loss 1.1553 (1.3002)	Prec@1 54.688 (50.409)
Test: [40/79]	Time 0.029 (0.050)	Loss 1.2295 (1.2924)	Prec@1 48.438 (50.324)
Test: [60/79]	Time 0.030 (0.044)	Loss 1.4191 (1.2883)	Prec@1 45.312 (50.320)
 * Prec@1 49.970
Epoch: [39][0/79]	Time 0.860 (0.860)	Loss 1.1160 (1.1160)	Prec@1 60.156 (60.156)
Epoch: [39][20/79]	Time 0.041 (0.084)	Loss 1.1609 (1.2149)	Prec@1 56.250 (53.051)
Epoch: [39][40/79]	Time 0.041 (0.064)	Loss 1.0709 (1.1911)	Prec@1 57.031 (54.249)
Epoch: [39][60/79]	Time 0.042 (0.058)	Loss 1.1115 (1.1840)	Prec@1 57.812 (54.380)
training time:  4.472347974777222
Test: [0/79]	Time 0.821 (0.821)	Loss 1.1131 (1.1131)	Prec@1 57.812 (57.812)
Test: [20/79]	Time 0.032 (0.069)	Loss 1.1130 (1.2892)	Prec@1 57.812 (51.339)
Test: [40/79]	Time 0.032 (0.051)	Loss 1.3330 (1.2963)	Prec@1 49.219 (50.648)
Test: [60/79]	Time 0.032 (0.045)	Loss 1.5236 (1.3046)	Prec@1 44.531 (50.692)
 * Prec@1 50.560
Epoch: [40][0/79]	Time 0.854 (0.854)	Loss 1.2884 (1.2884)	Prec@1 48.438 (48.438)
Epoch: [40][20/79]	Time 0.048 (0.085)	Loss 1.3004 (1.1327)	Prec@1 50.000 (55.915)
Epoch: [40][40/79]	Time 0.047 (0.066)	Loss 1.1124 (1.1341)	Prec@1 60.938 (56.822)
Epoch: [40][60/79]	Time 0.046 (0.060)	Loss 1.2390 (1.1367)	Prec@1 50.781 (56.801)
training time:  4.625135898590088
Test: [0/79]	Time 0.828 (0.828)	Loss 1.0131 (1.0131)	Prec@1 62.500 (62.500)
Test: [20/79]	Time 0.029 (0.075)	Loss 0.9923 (1.2657)	Prec@1 70.312 (54.055)
Test: [40/79]	Time 0.030 (0.054)	Loss 1.2722 (1.2621)	Prec@1 57.812 (54.459)
Test: [60/79]	Time 0.031 (0.047)	Loss 1.3415 (1.2748)	Prec@1 51.562 (54.022)
 * Prec@1 54.010
=> Saving checkpoint for epoch 40, with Prec@1 54.010000.
Epoch: [41][0/79]	Time 0.805 (0.805)	Loss 0.9189 (0.9189)	Prec@1 67.188 (67.188)
Epoch: [41][20/79]	Time 0.047 (0.083)	Loss 1.1590 (1.1375)	Prec@1 55.469 (57.329)
Epoch: [41][40/79]	Time 0.046 (0.065)	Loss 0.9891 (1.1073)	Prec@1 67.969 (58.251)
Epoch: [41][60/79]	Time 0.045 (0.059)	Loss 1.2761 (1.1113)	Prec@1 51.562 (58.222)
training time:  4.578911304473877
Test: [0/79]	Time 0.817 (0.817)	Loss 1.0992 (1.0992)	Prec@1 55.469 (55.469)
Test: [20/79]	Time 0.032 (0.068)	Loss 1.0361 (1.2507)	Prec@1 56.250 (51.228)
Test: [40/79]	Time 0.031 (0.050)	Loss 1.2227 (1.2445)	Prec@1 54.688 (51.429)
Test: [60/79]	Time 0.031 (0.044)	Loss 1.4967 (1.2582)	Prec@1 42.969 (51.268)
 * Prec@1 51.070
Epoch: [42][0/79]	Time 0.850 (0.850)	Loss 0.9137 (0.9137)	Prec@1 71.094 (71.094)
Epoch: [42][20/79]	Time 0.046 (0.084)	Loss 1.1475 (1.0534)	Prec@1 57.031 (61.272)
Epoch: [42][40/79]	Time 0.045 (0.066)	Loss 1.0582 (1.0556)	Prec@1 58.594 (60.595)
Epoch: [42][60/79]	Time 0.046 (0.059)	Loss 1.0294 (1.0668)	Prec@1 61.719 (59.913)
training time:  4.593160390853882
Test: [0/79]	Time 0.810 (0.810)	Loss 1.2010 (1.2010)	Prec@1 55.469 (55.469)
Test: [20/79]	Time 0.032 (0.069)	Loss 1.3086 (1.3171)	Prec@1 58.594 (51.414)
Test: [40/79]	Time 0.033 (0.051)	Loss 1.3306 (1.2998)	Prec@1 48.438 (51.829)
Test: [60/79]	Time 0.030 (0.045)	Loss 1.3269 (1.3050)	Prec@1 50.000 (51.268)
 * Prec@1 50.990
Epoch: [43][0/79]	Time 0.859 (0.859)	Loss 1.1366 (1.1366)	Prec@1 55.469 (55.469)
Epoch: [43][20/79]	Time 0.048 (0.085)	Loss 0.9900 (1.0357)	Prec@1 60.156 (62.128)
Epoch: [43][40/79]	Time 0.046 (0.066)	Loss 0.8576 (1.0156)	Prec@1 68.750 (62.309)
Epoch: [43][60/79]	Time 0.047 (0.060)	Loss 1.1863 (1.0182)	Prec@1 56.250 (62.295)
training time:  4.604435205459595
Test: [0/79]	Time 0.813 (0.813)	Loss 1.1370 (1.1370)	Prec@1 57.812 (57.812)
Test: [20/79]	Time 0.029 (0.067)	Loss 1.0192 (1.2797)	Prec@1 60.156 (54.464)
Test: [40/79]	Time 0.031 (0.050)	Loss 1.3372 (1.2863)	Prec@1 55.469 (54.059)
Test: [60/79]	Time 0.030 (0.044)	Loss 1.4366 (1.2929)	Prec@1 54.688 (53.906)
 * Prec@1 53.600
Epoch: [44][0/79]	Time 0.837 (0.837)	Loss 1.0581 (1.0581)	Prec@1 62.500 (62.500)
Epoch: [44][20/79]	Time 0.047 (0.084)	Loss 0.9785 (1.0433)	Prec@1 64.844 (60.789)
Epoch: [44][40/79]	Time 0.045 (0.065)	Loss 0.9964 (1.0158)	Prec@1 58.594 (61.833)
Epoch: [44][60/79]	Time 0.046 (0.059)	Loss 1.0312 (1.0013)	Prec@1 61.719 (62.205)
training time:  4.5600574016571045
Test: [0/79]	Time 0.816 (0.816)	Loss 1.0886 (1.0886)	Prec@1 62.500 (62.500)
Test: [20/79]	Time 0.032 (0.069)	Loss 1.2121 (1.2567)	Prec@1 60.156 (54.688)
Test: [40/79]	Time 0.032 (0.051)	Loss 1.1474 (1.2625)	Prec@1 54.688 (54.440)
Test: [60/79]	Time 0.032 (0.045)	Loss 1.5762 (1.2761)	Prec@1 45.312 (53.945)
 * Prec@1 53.530
Epoch: [45][0/79]	Time 0.859 (0.859)	Loss 0.9690 (0.9690)	Prec@1 57.812 (57.812)
Epoch: [45][20/79]	Time 0.046 (0.085)	Loss 0.9799 (1.0171)	Prec@1 64.844 (60.975)
Epoch: [45][40/79]	Time 0.046 (0.066)	Loss 1.0764 (1.0047)	Prec@1 60.156 (61.757)
Epoch: [45][60/79]	Time 0.048 (0.060)	Loss 0.9519 (0.9901)	Prec@1 63.281 (62.807)
training time:  4.630391597747803
Test: [0/79]	Time 0.810 (0.810)	Loss 0.9389 (0.9389)	Prec@1 64.844 (64.844)
Test: [20/79]	Time 0.032 (0.069)	Loss 1.1136 (1.2180)	Prec@1 60.938 (55.618)
Test: [40/79]	Time 0.032 (0.051)	Loss 1.2312 (1.2408)	Prec@1 53.125 (55.450)
Test: [60/79]	Time 0.031 (0.045)	Loss 1.4171 (1.2539)	Prec@1 48.438 (54.675)
 * Prec@1 54.850
=> Saving checkpoint for epoch 45, with Prec@1 54.850000.
Epoch: [46][0/79]	Time 0.858 (0.858)	Loss 0.8132 (0.8132)	Prec@1 70.312 (70.312)
Epoch: [46][20/79]	Time 0.045 (0.090)	Loss 0.9370 (0.9247)	Prec@1 61.719 (66.146)
Epoch: [46][40/79]	Time 0.045 (0.069)	Loss 0.9275 (0.9167)	Prec@1 64.062 (65.739)
Epoch: [46][60/79]	Time 0.046 (0.061)	Loss 1.0354 (0.9331)	Prec@1 63.281 (65.087)
training time:  4.734683990478516
Test: [0/79]	Time 0.817 (0.817)	Loss 1.0687 (1.0687)	Prec@1 61.719 (61.719)
Test: [20/79]	Time 0.033 (0.069)	Loss 1.0937 (1.3238)	Prec@1 60.938 (52.121)
Test: [40/79]	Time 0.032 (0.051)	Loss 1.4169 (1.3330)	Prec@1 51.562 (52.363)
Test: [60/79]	Time 0.030 (0.045)	Loss 1.5113 (1.3557)	Prec@1 51.562 (51.755)
 * Prec@1 51.470
Epoch: [47][0/79]	Time 0.816 (0.816)	Loss 0.9894 (0.9894)	Prec@1 60.938 (60.938)
Epoch: [47][20/79]	Time 0.047 (0.083)	Loss 0.8681 (0.9371)	Prec@1 71.875 (65.216)
Epoch: [47][40/79]	Time 0.047 (0.065)	Loss 0.9097 (0.9001)	Prec@1 65.625 (66.273)
Epoch: [47][60/79]	Time 0.045 (0.059)	Loss 0.9418 (0.9061)	Prec@1 62.500 (66.099)
training time:  4.585316896438599
Test: [0/79]	Time 0.779 (0.779)	Loss 1.1190 (1.1190)	Prec@1 56.250 (56.250)
Test: [20/79]	Time 0.031 (0.068)	Loss 1.1346 (1.2564)	Prec@1 57.031 (54.725)
Test: [40/79]	Time 0.033 (0.051)	Loss 1.3503 (1.2615)	Prec@1 50.000 (54.707)
Test: [60/79]	Time 0.029 (0.045)	Loss 1.4771 (1.2668)	Prec@1 45.312 (54.636)
 * Prec@1 54.540
Epoch: [48][0/79]	Time 0.854 (0.854)	Loss 0.9546 (0.9546)	Prec@1 66.406 (66.406)
Epoch: [48][20/79]	Time 0.047 (0.085)	Loss 0.8493 (0.8350)	Prec@1 72.656 (69.494)
Epoch: [48][40/79]	Time 0.046 (0.066)	Loss 0.9212 (0.8432)	Prec@1 65.625 (68.883)
Epoch: [48][60/79]	Time 0.045 (0.059)	Loss 0.9926 (0.8452)	Prec@1 63.281 (68.596)
training time:  4.570830821990967
Test: [0/79]	Time 0.816 (0.816)	Loss 1.3859 (1.3859)	Prec@1 47.656 (47.656)
Test: [20/79]	Time 0.032 (0.070)	Loss 1.1410 (1.4032)	Prec@1 53.906 (51.637)
Test: [40/79]	Time 0.032 (0.052)	Loss 1.5412 (1.3893)	Prec@1 50.000 (51.696)
Test: [60/79]	Time 0.031 (0.045)	Loss 1.6401 (1.3981)	Prec@1 42.969 (51.396)
 * Prec@1 51.190
Epoch: [49][0/79]	Time 0.807 (0.807)	Loss 0.8340 (0.8340)	Prec@1 65.625 (65.625)
Epoch: [49][20/79]	Time 0.048 (0.082)	Loss 0.9257 (0.8337)	Prec@1 61.719 (69.048)
Epoch: [49][40/79]	Time 0.049 (0.064)	Loss 0.7529 (0.8184)	Prec@1 71.875 (69.798)
Epoch: [49][60/79]	Time 0.045 (0.058)	Loss 0.8703 (0.8187)	Prec@1 70.312 (69.224)
training time:  4.524458408355713
Test: [0/79]	Time 0.825 (0.825)	Loss 1.1563 (1.1563)	Prec@1 53.906 (53.906)
Test: [20/79]	Time 0.032 (0.070)	Loss 1.1093 (1.2398)	Prec@1 57.031 (53.683)
Test: [40/79]	Time 0.033 (0.051)	Loss 1.4002 (1.2346)	Prec@1 51.562 (54.002)
Test: [60/79]	Time 0.031 (0.045)	Loss 1.5289 (1.2429)	Prec@1 52.344 (53.970)
 * Prec@1 53.640
Epoch: [50][0/79]	Time 0.847 (0.847)	Loss 0.9020 (0.9020)	Prec@1 65.625 (65.625)
Epoch: [50][20/79]	Time 0.046 (0.085)	Loss 0.6471 (0.8062)	Prec@1 77.344 (70.201)
Epoch: [50][40/79]	Time 0.046 (0.066)	Loss 0.7641 (0.7922)	Prec@1 70.312 (70.255)
Epoch: [50][60/79]	Time 0.045 (0.060)	Loss 0.7561 (0.7910)	Prec@1 72.656 (70.466)
training time:  4.6354734897613525
Test: [0/79]	Time 0.820 (0.820)	Loss 1.2456 (1.2456)	Prec@1 54.688 (54.688)
Test: [20/79]	Time 0.029 (0.067)	Loss 1.1397 (1.3852)	Prec@1 61.719 (54.464)
Test: [40/79]	Time 0.032 (0.049)	Loss 1.2551 (1.3776)	Prec@1 52.344 (53.982)
Test: [60/79]	Time 0.031 (0.044)	Loss 1.6117 (1.3756)	Prec@1 45.312 (53.855)
 * Prec@1 53.300
Epoch: [51][0/79]	Time 0.851 (0.851)	Loss 0.6967 (0.6967)	Prec@1 75.781 (75.781)
Epoch: [51][20/79]	Time 0.048 (0.084)	Loss 0.6907 (0.8050)	Prec@1 77.344 (70.015)
Epoch: [51][40/79]	Time 0.050 (0.066)	Loss 0.8278 (0.7902)	Prec@1 69.531 (70.751)
Epoch: [51][60/79]	Time 0.046 (0.060)	Loss 0.7306 (0.7725)	Prec@1 72.656 (71.094)
training time:  4.629875898361206
Test: [0/79]	Time 0.819 (0.819)	Loss 1.2290 (1.2290)	Prec@1 55.469 (55.469)
Test: [20/79]	Time 0.033 (0.076)	Loss 0.9912 (1.3347)	Prec@1 63.281 (57.440)
Test: [40/79]	Time 0.029 (0.054)	Loss 1.3537 (1.3198)	Prec@1 52.344 (57.355)
Test: [60/79]	Time 0.032 (0.047)	Loss 1.6088 (1.3359)	Prec@1 49.219 (56.878)
 * Prec@1 57.090
=> Saving checkpoint for epoch 51, with Prec@1 57.090000.
Epoch: [52][0/79]	Time 0.856 (0.856)	Loss 0.7836 (0.7836)	Prec@1 74.219 (74.219)
Epoch: [52][20/79]	Time 0.047 (0.085)	Loss 0.6692 (0.7162)	Prec@1 74.219 (73.884)
Epoch: [52][40/79]	Time 0.047 (0.066)	Loss 0.7116 (0.7115)	Prec@1 72.656 (73.628)
Epoch: [52][60/79]	Time 0.045 (0.059)	Loss 0.7010 (0.7161)	Prec@1 71.875 (73.604)
training time:  4.607511520385742
Test: [0/79]	Time 0.822 (0.822)	Loss 1.0932 (1.0932)	Prec@1 63.281 (63.281)
Test: [20/79]	Time 0.032 (0.069)	Loss 1.0129 (1.2683)	Prec@1 63.281 (58.445)
Test: [40/79]	Time 0.032 (0.051)	Loss 1.4269 (1.2804)	Prec@1 50.000 (58.289)
Test: [60/79]	Time 0.032 (0.045)	Loss 1.6455 (1.3098)	Prec@1 46.875 (57.800)
 * Prec@1 57.650
=> Saving checkpoint for epoch 52, with Prec@1 57.650000.
Epoch: [53][0/79]	Time 0.813 (0.813)	Loss 0.8970 (0.8970)	Prec@1 66.406 (66.406)
Epoch: [53][20/79]	Time 0.046 (0.083)	Loss 0.7841 (0.8358)	Prec@1 73.438 (69.159)
Epoch: [53][40/79]	Time 0.045 (0.065)	Loss 0.7871 (0.7835)	Prec@1 70.312 (71.494)
Epoch: [53][60/79]	Time 0.044 (0.058)	Loss 0.6480 (0.7708)	Prec@1 78.906 (71.849)
training time:  4.528525352478027
Test: [0/79]	Time 0.780 (0.780)	Loss 1.1082 (1.1082)	Prec@1 61.719 (61.719)
Test: [20/79]	Time 0.030 (0.067)	Loss 0.9261 (1.2644)	Prec@1 69.531 (57.999)
Test: [40/79]	Time 0.032 (0.050)	Loss 1.2646 (1.2715)	Prec@1 60.156 (57.565)
Test: [60/79]	Time 0.031 (0.044)	Loss 1.4259 (1.2788)	Prec@1 55.469 (57.223)
 * Prec@1 57.030
Epoch: [54][0/79]	Time 0.818 (0.818)	Loss 0.6340 (0.6340)	Prec@1 79.688 (79.688)
Epoch: [54][20/79]	Time 0.046 (0.083)	Loss 0.6073 (0.6730)	Prec@1 74.219 (75.000)
Epoch: [54][40/79]	Time 0.046 (0.065)	Loss 0.6886 (0.6633)	Prec@1 69.531 (75.553)
Epoch: [54][60/79]	Time 0.045 (0.059)	Loss 0.7127 (0.6589)	Prec@1 73.438 (75.692)
training time:  4.563917875289917
Test: [0/79]	Time 0.776 (0.776)	Loss 1.0779 (1.0779)	Prec@1 61.719 (61.719)
Test: [20/79]	Time 0.032 (0.067)	Loss 1.0192 (1.3215)	Prec@1 65.625 (57.924)
Test: [40/79]	Time 0.032 (0.050)	Loss 1.4595 (1.3211)	Prec@1 59.375 (58.022)
Test: [60/79]	Time 0.032 (0.044)	Loss 1.6325 (1.3415)	Prec@1 55.469 (57.979)
 * Prec@1 58.150
=> Saving checkpoint for epoch 54, with Prec@1 58.150000.
Epoch: [55][0/79]	Time 0.814 (0.814)	Loss 0.6214 (0.6214)	Prec@1 76.562 (76.562)
Epoch: [55][20/79]	Time 0.048 (0.083)	Loss 0.6133 (0.6172)	Prec@1 77.344 (76.414)
Epoch: [55][40/79]	Time 0.048 (0.065)	Loss 0.6722 (0.6165)	Prec@1 76.562 (76.715)
Epoch: [55][60/79]	Time 0.045 (0.059)	Loss 0.5771 (0.6111)	Prec@1 80.469 (77.075)
training time:  4.581485748291016
Test: [0/79]	Time 0.767 (0.767)	Loss 1.0094 (1.0094)	Prec@1 67.188 (67.188)
Test: [20/79]	Time 0.032 (0.067)	Loss 1.0871 (1.2637)	Prec@1 66.406 (60.082)
Test: [40/79]	Time 0.032 (0.050)	Loss 1.2482 (1.2721)	Prec@1 54.688 (59.508)
Test: [60/79]	Time 0.030 (0.044)	Loss 1.5645 (1.2762)	Prec@1 56.250 (59.042)
 * Prec@1 58.730
=> Saving checkpoint for epoch 55, with Prec@1 58.730000.
Epoch: [56][0/79]	Time 0.812 (0.812)	Loss 0.5482 (0.5482)	Prec@1 84.375 (84.375)
Epoch: [56][20/79]	Time 0.044 (0.083)	Loss 0.5848 (0.5542)	Prec@1 78.906 (79.055)
Epoch: [56][40/79]	Time 0.045 (0.065)	Loss 0.5716 (0.5401)	Prec@1 81.250 (80.126)
Epoch: [56][60/79]	Time 0.046 (0.059)	Loss 0.5764 (0.5459)	Prec@1 80.469 (79.892)
training time:  4.548767328262329
Test: [0/79]	Time 0.783 (0.783)	Loss 1.2716 (1.2716)	Prec@1 60.938 (60.938)
Test: [20/79]	Time 0.032 (0.067)	Loss 1.1825 (1.3975)	Prec@1 59.375 (57.738)
Test: [40/79]	Time 0.032 (0.050)	Loss 1.5084 (1.3735)	Prec@1 53.125 (57.927)
Test: [60/79]	Time 0.030 (0.044)	Loss 1.6516 (1.3857)	Prec@1 53.906 (57.646)
 * Prec@1 57.360
Epoch: [57][0/79]	Time 0.842 (0.842)	Loss 0.5603 (0.5603)	Prec@1 78.906 (78.906)
Epoch: [57][20/79]	Time 0.048 (0.089)	Loss 0.6353 (0.6012)	Prec@1 78.125 (78.125)
Epoch: [57][40/79]	Time 0.044 (0.068)	Loss 0.5502 (0.5744)	Prec@1 80.469 (78.773)
Epoch: [57][60/79]	Time 0.046 (0.060)	Loss 0.5342 (0.5721)	Prec@1 76.562 (78.932)
training time:  4.629871129989624
Test: [0/79]	Time 0.823 (0.823)	Loss 1.0453 (1.0453)	Prec@1 62.500 (62.500)
Test: [20/79]	Time 0.032 (0.068)	Loss 0.9414 (1.2133)	Prec@1 71.094 (61.756)
Test: [40/79]	Time 0.033 (0.050)	Loss 1.1764 (1.2152)	Prec@1 58.594 (61.433)
Test: [60/79]	Time 0.032 (0.044)	Loss 1.2865 (1.2298)	Prec@1 60.938 (60.912)
 * Prec@1 60.790
=> Saving checkpoint for epoch 57, with Prec@1 60.790000.
Epoch: [58][0/79]	Time 0.848 (0.848)	Loss 0.4339 (0.4339)	Prec@1 85.156 (85.156)
Epoch: [58][20/79]	Time 0.043 (0.083)	Loss 0.5645 (0.5324)	Prec@1 82.812 (81.138)
Epoch: [58][40/79]	Time 0.045 (0.065)	Loss 0.5551 (0.5192)	Prec@1 81.250 (81.612)
Epoch: [58][60/79]	Time 0.045 (0.059)	Loss 0.5635 (0.5174)	Prec@1 79.688 (81.596)
training time:  4.532402038574219
Test: [0/79]	Time 0.823 (0.823)	Loss 1.0780 (1.0780)	Prec@1 68.750 (68.750)
Test: [20/79]	Time 0.032 (0.068)	Loss 1.0695 (1.3032)	Prec@1 62.500 (60.491)
Test: [40/79]	Time 0.032 (0.050)	Loss 1.1659 (1.3092)	Prec@1 60.156 (60.004)
Test: [60/79]	Time 0.031 (0.044)	Loss 1.5088 (1.3085)	Prec@1 54.688 (60.003)
 * Prec@1 60.230
Epoch: [59][0/79]	Time 0.855 (0.855)	Loss 0.5749 (0.5749)	Prec@1 76.562 (76.562)
Epoch: [59][20/79]	Time 0.047 (0.084)	Loss 0.5325 (0.5516)	Prec@1 82.031 (79.278)
Epoch: [59][40/79]	Time 0.046 (0.065)	Loss 0.4980 (0.5090)	Prec@1 76.562 (80.888)
Epoch: [59][60/79]	Time 0.045 (0.059)	Loss 0.4321 (0.5035)	Prec@1 82.031 (81.340)
training time:  4.562255859375
Test: [0/79]	Time 0.827 (0.827)	Loss 1.1511 (1.1511)	Prec@1 60.156 (60.156)
Test: [20/79]	Time 0.032 (0.070)	Loss 0.9962 (1.2738)	Prec@1 67.188 (60.975)
Test: [40/79]	Time 0.028 (0.051)	Loss 1.2614 (1.2756)	Prec@1 60.156 (61.319)
Test: [60/79]	Time 0.031 (0.044)	Loss 1.5621 (1.2925)	Prec@1 54.688 (60.835)
 * Prec@1 60.970
=> Saving checkpoint for epoch 59, with Prec@1 60.970000.
Epoch: [60][0/79]	Time 0.859 (0.859)	Loss 0.5381 (0.5381)	Prec@1 78.906 (78.906)
Epoch: [60][20/79]	Time 0.045 (0.085)	Loss 0.4433 (0.4751)	Prec@1 83.594 (82.403)
Epoch: [60][40/79]	Time 0.043 (0.066)	Loss 0.3900 (0.4579)	Prec@1 89.062 (83.613)
Epoch: [60][60/79]	Time 0.048 (0.059)	Loss 0.4603 (0.4451)	Prec@1 85.156 (83.978)
training time:  4.570346832275391
Test: [0/79]	Time 0.815 (0.815)	Loss 0.9956 (0.9956)	Prec@1 69.531 (69.531)
Test: [20/79]	Time 0.032 (0.069)	Loss 1.1683 (1.4361)	Prec@1 61.719 (56.771)
Test: [40/79]	Time 0.032 (0.050)	Loss 1.4671 (1.4644)	Prec@1 57.031 (56.231)
Test: [60/79]	Time 0.030 (0.044)	Loss 1.6949 (1.4800)	Prec@1 50.000 (55.904)
 * Prec@1 55.700
Epoch: [61][0/79]	Time 0.855 (0.855)	Loss 0.4912 (0.4912)	Prec@1 82.031 (82.031)
Epoch: [61][20/79]	Time 0.046 (0.083)	Loss 0.3252 (0.4668)	Prec@1 89.062 (83.073)
Epoch: [61][40/79]	Time 0.043 (0.065)	Loss 0.4949 (0.4545)	Prec@1 83.594 (83.727)
Epoch: [61][60/79]	Time 0.046 (0.059)	Loss 0.5012 (0.4525)	Prec@1 81.250 (83.837)
training time:  4.548066139221191
Test: [0/79]	Time 0.810 (0.810)	Loss 1.0932 (1.0932)	Prec@1 63.281 (63.281)
Test: [20/79]	Time 0.032 (0.068)	Loss 1.1635 (1.2993)	Prec@1 64.062 (61.458)
Test: [40/79]	Time 0.032 (0.050)	Loss 1.2663 (1.2960)	Prec@1 58.594 (61.547)
Test: [60/79]	Time 0.030 (0.044)	Loss 1.5531 (1.3241)	Prec@1 55.469 (61.040)
 * Prec@1 60.780
Epoch: [62][0/79]	Time 0.845 (0.845)	Loss 0.4393 (0.4393)	Prec@1 83.594 (83.594)
Epoch: [62][20/79]	Time 0.047 (0.084)	Loss 0.4047 (0.4247)	Prec@1 85.938 (84.263)
Epoch: [62][40/79]	Time 0.043 (0.065)	Loss 0.3786 (0.4115)	Prec@1 85.156 (85.213)
Epoch: [62][60/79]	Time 0.046 (0.058)	Loss 0.3474 (0.4072)	Prec@1 88.281 (85.528)
training time:  4.532445907592773
Test: [0/79]	Time 0.810 (0.810)	Loss 1.0286 (1.0286)	Prec@1 64.844 (64.844)
Test: [20/79]	Time 0.032 (0.075)	Loss 1.1098 (1.2459)	Prec@1 69.531 (63.281)
Test: [40/79]	Time 0.032 (0.054)	Loss 1.2729 (1.2298)	Prec@1 62.500 (63.986)
Test: [60/79]	Time 0.031 (0.047)	Loss 1.4367 (1.2646)	Prec@1 63.281 (63.781)
 * Prec@1 64.060
=> Saving checkpoint for epoch 62, with Prec@1 64.060000.
Epoch: [63][0/79]	Time 0.854 (0.854)	Loss 0.4173 (0.4173)	Prec@1 82.812 (82.812)
Epoch: [63][20/79]	Time 0.047 (0.085)	Loss 0.4288 (0.4469)	Prec@1 85.156 (84.226)
Epoch: [63][40/79]	Time 0.046 (0.067)	Loss 0.3577 (0.4359)	Prec@1 85.156 (84.318)
Epoch: [63][60/79]	Time 0.046 (0.060)	Loss 0.3805 (0.4163)	Prec@1 85.156 (84.990)
training time:  4.6355743408203125
Test: [0/79]	Time 0.819 (0.819)	Loss 1.1235 (1.1235)	Prec@1 67.969 (67.969)
Test: [20/79]	Time 0.033 (0.069)	Loss 1.1857 (1.4325)	Prec@1 60.938 (60.305)
Test: [40/79]	Time 0.034 (0.051)	Loss 1.5234 (1.4276)	Prec@1 54.688 (60.290)
Test: [60/79]	Time 0.031 (0.045)	Loss 1.7791 (1.4416)	Prec@1 57.031 (59.926)
 * Prec@1 59.860
Epoch: [64][0/79]	Time 0.845 (0.845)	Loss 0.4673 (0.4673)	Prec@1 83.594 (83.594)
Epoch: [64][20/79]	Time 0.047 (0.084)	Loss 0.3776 (0.4656)	Prec@1 87.500 (82.812)
Epoch: [64][40/79]	Time 0.047 (0.066)	Loss 0.3689 (0.4282)	Prec@1 84.375 (84.299)
Epoch: [64][60/79]	Time 0.045 (0.060)	Loss 0.3309 (0.4088)	Prec@1 90.625 (85.028)
training time:  4.597856521606445
Test: [0/79]	Time 0.763 (0.763)	Loss 1.0570 (1.0570)	Prec@1 69.531 (69.531)
Test: [20/79]	Time 0.031 (0.067)	Loss 1.0915 (1.3903)	Prec@1 68.750 (59.449)
Test: [40/79]	Time 0.032 (0.050)	Loss 1.3429 (1.3674)	Prec@1 60.938 (60.118)
Test: [60/79]	Time 0.031 (0.044)	Loss 1.5463 (1.3796)	Prec@1 53.125 (59.183)
 * Prec@1 59.100
Epoch: [65][0/79]	Time 0.808 (0.808)	Loss 0.2655 (0.2655)	Prec@1 90.625 (90.625)
Epoch: [65][20/79]	Time 0.045 (0.083)	Loss 0.3392 (0.3449)	Prec@1 87.500 (87.240)
Epoch: [65][40/79]	Time 0.045 (0.064)	Loss 0.3274 (0.3403)	Prec@1 89.844 (88.186)
Epoch: [65][60/79]	Time 0.046 (0.058)	Loss 0.2754 (0.3298)	Prec@1 89.844 (88.601)
training time:  4.533669948577881
Test: [0/79]	Time 0.810 (0.810)	Loss 1.0131 (1.0131)	Prec@1 69.531 (69.531)
Test: [20/79]	Time 0.032 (0.069)	Loss 1.1246 (1.3362)	Prec@1 72.656 (63.132)
Test: [40/79]	Time 0.033 (0.051)	Loss 1.3573 (1.3180)	Prec@1 59.375 (63.034)
Test: [60/79]	Time 0.030 (0.045)	Loss 1.5167 (1.3260)	Prec@1 56.250 (62.577)
 * Prec@1 62.730
Epoch: [66][0/79]	Time 0.847 (0.847)	Loss 0.3093 (0.3093)	Prec@1 90.625 (90.625)
Epoch: [66][20/79]	Time 0.045 (0.084)	Loss 0.3938 (0.3519)	Prec@1 85.938 (87.277)
Epoch: [66][40/79]	Time 0.044 (0.066)	Loss 0.3178 (0.3210)	Prec@1 85.938 (88.624)
Epoch: [66][60/79]	Time 0.045 (0.059)	Loss 0.2821 (0.3089)	Prec@1 90.625 (89.319)
training time:  4.612035512924194
Test: [0/79]	Time 0.825 (0.825)	Loss 0.9040 (0.9040)	Prec@1 66.406 (66.406)
Test: [20/79]	Time 0.030 (0.068)	Loss 0.9846 (1.2287)	Prec@1 71.875 (65.067)
Test: [40/79]	Time 0.032 (0.051)	Loss 1.3232 (1.2428)	Prec@1 63.281 (64.901)
Test: [60/79]	Time 0.030 (0.045)	Loss 1.6009 (1.2579)	Prec@1 58.594 (64.447)
 * Prec@1 64.370
=> Saving checkpoint for epoch 66, with Prec@1 64.370000.
Epoch: [67][0/79]	Time 0.841 (0.841)	Loss 0.2134 (0.2134)	Prec@1 94.531 (94.531)
Epoch: [67][20/79]	Time 0.046 (0.084)	Loss 0.2385 (0.3252)	Prec@1 92.188 (88.988)
Epoch: [67][40/79]	Time 0.047 (0.066)	Loss 0.2907 (0.3025)	Prec@1 88.281 (89.653)
Epoch: [67][60/79]	Time 0.047 (0.059)	Loss 0.2777 (0.2827)	Prec@1 91.406 (90.266)
training time:  4.579495429992676
Test: [0/79]	Time 0.816 (0.816)	Loss 0.8759 (0.8759)	Prec@1 71.094 (71.094)
Test: [20/79]	Time 0.032 (0.069)	Loss 0.8535 (1.2496)	Prec@1 75.000 (65.997)
Test: [40/79]	Time 0.032 (0.051)	Loss 1.5023 (1.2931)	Prec@1 60.156 (64.768)
Test: [60/79]	Time 0.031 (0.045)	Loss 1.4378 (1.2956)	Prec@1 66.406 (64.165)
 * Prec@1 64.340
Epoch: [68][0/79]	Time 0.860 (0.860)	Loss 0.1928 (0.1928)	Prec@1 95.312 (95.312)
Epoch: [68][20/79]	Time 0.046 (0.092)	Loss 0.2863 (0.2577)	Prec@1 89.844 (91.481)
Epoch: [68][40/79]	Time 0.047 (0.070)	Loss 0.1832 (0.2413)	Prec@1 93.750 (92.130)
Epoch: [68][60/79]	Time 0.046 (0.062)	Loss 0.2548 (0.2332)	Prec@1 90.625 (92.316)
training time:  4.778071641921997
Test: [0/79]	Time 0.814 (0.814)	Loss 1.0478 (1.0478)	Prec@1 67.969 (67.969)
Test: [20/79]	Time 0.032 (0.069)	Loss 1.2415 (1.3381)	Prec@1 68.750 (64.993)
Test: [40/79]	Time 0.034 (0.051)	Loss 1.3315 (1.3324)	Prec@1 64.062 (64.996)
Test: [60/79]	Time 0.032 (0.045)	Loss 1.6589 (1.3582)	Prec@1 55.469 (64.152)
 * Prec@1 64.030
Epoch: [69][0/79]	Time 0.865 (0.865)	Loss 0.2152 (0.2152)	Prec@1 90.625 (90.625)
Epoch: [69][20/79]	Time 0.046 (0.084)	Loss 0.3119 (0.3023)	Prec@1 86.719 (88.393)
Epoch: [69][40/79]	Time 0.046 (0.066)	Loss 0.3034 (0.2865)	Prec@1 89.062 (89.177)
Epoch: [69][60/79]	Time 0.044 (0.059)	Loss 0.1920 (0.2742)	Prec@1 96.094 (89.921)
training time:  4.594518661499023
Test: [0/79]	Time 0.823 (0.823)	Loss 1.2521 (1.2521)	Prec@1 64.844 (64.844)
Test: [20/79]	Time 0.032 (0.069)	Loss 1.0068 (1.3879)	Prec@1 70.312 (62.760)
Test: [40/79]	Time 0.031 (0.051)	Loss 1.4972 (1.4020)	Prec@1 60.156 (62.976)
Test: [60/79]	Time 0.031 (0.045)	Loss 1.6115 (1.4258)	Prec@1 60.156 (62.859)
 * Prec@1 62.880
Epoch: [70][0/79]	Time 0.847 (0.847)	Loss 0.2727 (0.2727)	Prec@1 90.625 (90.625)
Epoch: [70][20/79]	Time 0.047 (0.084)	Loss 0.2932 (0.2739)	Prec@1 89.844 (90.625)
Epoch: [70][40/79]	Time 0.046 (0.066)	Loss 0.2531 (0.2460)	Prec@1 88.281 (91.616)
Epoch: [70][60/79]	Time 0.047 (0.059)	Loss 0.2435 (0.2284)	Prec@1 92.188 (92.213)
training time:  4.6122822761535645
Test: [0/79]	Time 0.819 (0.819)	Loss 1.0694 (1.0694)	Prec@1 65.625 (65.625)
Test: [20/79]	Time 0.031 (0.069)	Loss 1.1727 (1.3063)	Prec@1 64.062 (64.621)
Test: [40/79]	Time 0.031 (0.051)	Loss 1.4438 (1.3485)	Prec@1 62.500 (64.787)
Test: [60/79]	Time 0.031 (0.045)	Loss 1.4130 (1.3499)	Prec@1 63.281 (64.716)
 * Prec@1 64.680
=> Saving checkpoint for epoch 70, with Prec@1 64.680000.
Epoch: [71][0/79]	Time 0.805 (0.805)	Loss 0.1439 (0.1439)	Prec@1 96.875 (96.875)
Epoch: [71][20/79]	Time 0.047 (0.082)	Loss 0.2194 (0.2127)	Prec@1 91.406 (92.411)
Epoch: [71][40/79]	Time 0.047 (0.065)	Loss 0.1184 (0.1951)	Prec@1 97.656 (93.369)
Epoch: [71][60/79]	Time 0.046 (0.059)	Loss 0.2511 (0.1878)	Prec@1 92.188 (93.648)
training time:  4.566909313201904
Test: [0/79]	Time 0.789 (0.789)	Loss 0.9840 (0.9840)	Prec@1 69.531 (69.531)
Test: [20/79]	Time 0.032 (0.067)	Loss 1.1358 (1.3573)	Prec@1 69.531 (64.546)
Test: [40/79]	Time 0.031 (0.050)	Loss 1.3851 (1.4037)	Prec@1 61.719 (64.139)
Test: [60/79]	Time 0.031 (0.044)	Loss 1.6728 (1.4288)	Prec@1 54.688 (63.473)
 * Prec@1 63.400
Epoch: [72][0/79]	Time 0.821 (0.821)	Loss 0.2017 (0.2017)	Prec@1 92.969 (92.969)
Epoch: [72][20/79]	Time 0.047 (0.084)	Loss 0.2260 (0.2374)	Prec@1 90.625 (92.671)
Epoch: [72][40/79]	Time 0.047 (0.066)	Loss 0.1758 (0.2126)	Prec@1 94.531 (93.293)
Epoch: [72][60/79]	Time 0.047 (0.059)	Loss 0.1034 (0.1967)	Prec@1 98.438 (93.712)
training time:  4.612178087234497
Test: [0/79]	Time 0.792 (0.792)	Loss 1.0640 (1.0640)	Prec@1 69.531 (69.531)
Test: [20/79]	Time 0.032 (0.068)	Loss 1.0351 (1.2594)	Prec@1 71.094 (67.262)
Test: [40/79]	Time 0.031 (0.050)	Loss 1.3652 (1.2856)	Prec@1 60.938 (66.273)
Test: [60/79]	Time 0.031 (0.045)	Loss 1.5363 (1.2958)	Prec@1 65.625 (66.189)
 * Prec@1 66.050
=> Saving checkpoint for epoch 72, with Prec@1 66.050000.
Epoch: [73][0/79]	Time 0.810 (0.810)	Loss 0.1404 (0.1404)	Prec@1 96.094 (96.094)
Epoch: [73][20/79]	Time 0.047 (0.083)	Loss 0.1878 (0.1499)	Prec@1 92.969 (95.015)
Epoch: [73][40/79]	Time 0.046 (0.065)	Loss 0.0973 (0.1405)	Prec@1 96.094 (95.427)
Epoch: [73][60/79]	Time 0.046 (0.059)	Loss 0.0899 (0.1340)	Prec@1 98.438 (95.697)
training time:  4.577143907546997
Test: [0/79]	Time 0.817 (0.817)	Loss 1.2420 (1.2420)	Prec@1 66.406 (66.406)
Test: [20/79]	Time 0.031 (0.076)	Loss 1.2041 (1.3871)	Prec@1 71.094 (64.658)
Test: [40/79]	Time 0.034 (0.054)	Loss 1.3537 (1.4109)	Prec@1 58.594 (64.101)
Test: [60/79]	Time 0.032 (0.047)	Loss 1.7563 (1.4302)	Prec@1 58.594 (63.947)
 * Prec@1 63.520
Epoch: [74][0/79]	Time 0.839 (0.839)	Loss 0.0913 (0.0913)	Prec@1 97.656 (97.656)
Epoch: [74][20/79]	Time 0.046 (0.085)	Loss 0.2174 (0.1724)	Prec@1 91.406 (93.862)
Epoch: [74][40/79]	Time 0.046 (0.066)	Loss 0.2096 (0.1589)	Prec@1 92.969 (94.569)
Epoch: [74][60/79]	Time 0.049 (0.060)	Loss 0.0966 (0.1419)	Prec@1 99.219 (95.274)
training time:  4.6273274421691895
Test: [0/79]	Time 0.824 (0.824)	Loss 1.1103 (1.1103)	Prec@1 70.312 (70.312)
Test: [20/79]	Time 0.032 (0.069)	Loss 1.2095 (1.4156)	Prec@1 67.969 (65.439)
Test: [40/79]	Time 0.033 (0.051)	Loss 1.6491 (1.4607)	Prec@1 60.156 (65.415)
Test: [60/79]	Time 0.032 (0.045)	Loss 1.7565 (1.4753)	Prec@1 60.156 (65.254)
 * Prec@1 65.140
Epoch: [75][0/79]	Time 0.844 (0.844)	Loss 0.1817 (0.1817)	Prec@1 92.188 (92.188)
Epoch: [75][20/79]	Time 0.043 (0.082)	Loss 0.1257 (0.2057)	Prec@1 95.312 (93.043)
Epoch: [75][40/79]	Time 0.045 (0.064)	Loss 0.1050 (0.1810)	Prec@1 96.875 (93.883)
Epoch: [75][60/79]	Time 0.042 (0.058)	Loss 0.0884 (0.1665)	Prec@1 96.875 (94.339)
training time:  4.539904594421387
Test: [0/79]	Time 0.814 (0.814)	Loss 0.9728 (0.9728)	Prec@1 66.406 (66.406)
Test: [20/79]	Time 0.032 (0.068)	Loss 1.0367 (1.3895)	Prec@1 75.781 (65.588)
Test: [40/79]	Time 0.033 (0.050)	Loss 1.6175 (1.4278)	Prec@1 59.375 (65.187)
Test: [60/79]	Time 0.031 (0.044)	Loss 1.8252 (1.4523)	Prec@1 60.938 (64.933)
 * Prec@1 64.750
Epoch: [76][0/79]	Time 0.859 (0.859)	Loss 0.1263 (0.1263)	Prec@1 96.094 (96.094)
Epoch: [76][20/79]	Time 0.046 (0.085)	Loss 0.0728 (0.1276)	Prec@1 98.438 (95.796)
Epoch: [76][40/79]	Time 0.046 (0.066)	Loss 0.1771 (0.1204)	Prec@1 92.188 (96.189)
Epoch: [76][60/79]	Time 0.045 (0.059)	Loss 0.0955 (0.1142)	Prec@1 96.875 (96.414)
training time:  4.545505046844482
Test: [0/79]	Time 0.770 (0.770)	Loss 1.0850 (1.0850)	Prec@1 71.875 (71.875)
Test: [20/79]	Time 0.032 (0.065)	Loss 1.1646 (1.3165)	Prec@1 71.094 (67.113)
Test: [40/79]	Time 0.034 (0.049)	Loss 1.3577 (1.3583)	Prec@1 64.062 (66.597)
Test: [60/79]	Time 0.030 (0.043)	Loss 1.6428 (1.3688)	Prec@1 63.281 (66.124)
 * Prec@1 66.190
=> Saving checkpoint for epoch 76, with Prec@1 66.190000.
Epoch: [77][0/79]	Time 0.812 (0.812)	Loss 0.0805 (0.0805)	Prec@1 97.656 (97.656)
Epoch: [77][20/79]	Time 0.048 (0.081)	Loss 0.1134 (0.1266)	Prec@1 96.094 (95.722)
Epoch: [77][40/79]	Time 0.043 (0.062)	Loss 0.1191 (0.1076)	Prec@1 96.094 (96.570)
Epoch: [77][60/79]	Time 0.045 (0.057)	Loss 0.0629 (0.1048)	Prec@1 96.875 (96.542)
training time:  4.407351016998291
Test: [0/79]	Time 0.788 (0.788)	Loss 1.1734 (1.1734)	Prec@1 66.406 (66.406)
Test: [20/79]	Time 0.033 (0.068)	Loss 1.2338 (1.3793)	Prec@1 70.312 (65.848)
Test: [40/79]	Time 0.032 (0.050)	Loss 1.4007 (1.3819)	Prec@1 63.281 (65.968)
Test: [60/79]	Time 0.031 (0.044)	Loss 1.5663 (1.4024)	Prec@1 58.594 (65.266)
 * Prec@1 65.360
Epoch: [78][0/79]	Time 0.817 (0.817)	Loss 0.0521 (0.0521)	Prec@1 100.000 (100.000)
Epoch: [78][20/79]	Time 0.046 (0.083)	Loss 0.1451 (0.1271)	Prec@1 96.094 (96.057)
Epoch: [78][40/79]	Time 0.046 (0.065)	Loss 0.0790 (0.1157)	Prec@1 97.656 (96.341)
Epoch: [78][60/79]	Time 0.045 (0.059)	Loss 0.0609 (0.1074)	Prec@1 98.438 (96.721)
training time:  4.563807725906372
Test: [0/79]	Time 0.781 (0.781)	Loss 0.9038 (0.9038)	Prec@1 71.094 (71.094)
Test: [20/79]	Time 0.031 (0.067)	Loss 1.1568 (1.3909)	Prec@1 72.656 (66.853)
Test: [40/79]	Time 0.034 (0.050)	Loss 1.5391 (1.4171)	Prec@1 60.938 (66.292)
Test: [60/79]	Time 0.030 (0.044)	Loss 1.8532 (1.4387)	Prec@1 58.594 (65.779)
 * Prec@1 65.660
Epoch: [79][0/79]	Time 0.830 (0.830)	Loss 0.0352 (0.0352)	Prec@1 100.000 (100.000)
Epoch: [79][20/79]	Time 0.046 (0.090)	Loss 0.1310 (0.0817)	Prec@1 96.094 (97.507)
Epoch: [79][40/79]	Time 0.046 (0.068)	Loss 0.1225 (0.0815)	Prec@1 93.750 (97.389)
Epoch: [79][60/79]	Time 0.047 (0.061)	Loss 0.1006 (0.0769)	Prec@1 97.656 (97.631)
training time:  4.685095310211182
Test: [0/79]	Time 0.777 (0.777)	Loss 1.1998 (1.1998)	Prec@1 69.531 (69.531)
Test: [20/79]	Time 0.031 (0.067)	Loss 1.0834 (1.3334)	Prec@1 75.000 (67.671)
Test: [40/79]	Time 0.031 (0.050)	Loss 1.5016 (1.3698)	Prec@1 59.375 (67.302)
Test: [60/79]	Time 0.032 (0.044)	Loss 1.6419 (1.3781)	Prec@1 64.844 (67.175)
 * Prec@1 67.170
=> Saving checkpoint for epoch 79, with Prec@1 67.170000.
Epoch: [80][0/79]	Time 0.855 (0.855)	Loss 0.0399 (0.0399)	Prec@1 98.438 (98.438)
Epoch: [80][20/79]	Time 0.046 (0.084)	Loss 0.0277 (0.0715)	Prec@1 100.000 (98.103)
Epoch: [80][40/79]	Time 0.050 (0.065)	Loss 0.0357 (0.0703)	Prec@1 98.438 (98.095)
Epoch: [80][60/79]	Time 0.045 (0.058)	Loss 0.0659 (0.0635)	Prec@1 98.438 (98.309)
training time:  4.528842210769653
Test: [0/79]	Time 0.832 (0.832)	Loss 0.9936 (0.9936)	Prec@1 74.219 (74.219)
Test: [20/79]	Time 0.029 (0.069)	Loss 1.1244 (1.3510)	Prec@1 70.312 (67.560)
Test: [40/79]	Time 0.029 (0.050)	Loss 1.4493 (1.3812)	Prec@1 66.406 (67.416)
Test: [60/79]	Time 0.032 (0.044)	Loss 1.6341 (1.4032)	Prec@1 63.281 (67.239)
 * Prec@1 67.140
Epoch: [81][0/79]	Time 0.809 (0.809)	Loss 0.0166 (0.0166)	Prec@1 100.000 (100.000)
Epoch: [81][20/79]	Time 0.045 (0.082)	Loss 0.0280 (0.0435)	Prec@1 99.219 (98.921)
Epoch: [81][40/79]	Time 0.047 (0.064)	Loss 0.0373 (0.0468)	Prec@1 99.219 (98.723)
Epoch: [81][60/79]	Time 0.045 (0.058)	Loss 0.0301 (0.0470)	Prec@1 100.000 (98.809)
training time:  4.550779104232788
Test: [0/79]	Time 0.790 (0.790)	Loss 0.9652 (0.9652)	Prec@1 71.875 (71.875)
Test: [20/79]	Time 0.033 (0.068)	Loss 1.1384 (1.3374)	Prec@1 73.438 (68.936)
Test: [40/79]	Time 0.032 (0.051)	Loss 1.5512 (1.3866)	Prec@1 63.281 (67.511)
Test: [60/79]	Time 0.031 (0.045)	Loss 1.7780 (1.4067)	Prec@1 60.938 (67.303)
 * Prec@1 67.290
=> Saving checkpoint for epoch 81, with Prec@1 67.290000.
Epoch: [82][0/79]	Time 0.809 (0.809)	Loss 0.0400 (0.0400)	Prec@1 99.219 (99.219)
Epoch: [82][20/79]	Time 0.047 (0.083)	Loss 0.0709 (0.0589)	Prec@1 96.094 (98.438)
Epoch: [82][40/79]	Time 0.045 (0.065)	Loss 0.0287 (0.0564)	Prec@1 98.438 (98.361)
Epoch: [82][60/79]	Time 0.041 (0.058)	Loss 0.0595 (0.0517)	Prec@1 96.875 (98.553)
training time:  4.515093803405762
Test: [0/79]	Time 0.767 (0.767)	Loss 0.9097 (0.9097)	Prec@1 74.219 (74.219)
Test: [20/79]	Time 0.030 (0.065)	Loss 1.0872 (1.3555)	Prec@1 72.656 (68.527)
Test: [40/79]	Time 0.029 (0.048)	Loss 1.5688 (1.3928)	Prec@1 63.281 (67.569)
Test: [60/79]	Time 0.031 (0.042)	Loss 1.8401 (1.4270)	Prec@1 61.719 (67.495)
 * Prec@1 67.350
=> Saving checkpoint for epoch 82, with Prec@1 67.350000.
Epoch: [83][0/79]	Time 0.808 (0.808)	Loss 0.0278 (0.0278)	Prec@1 98.438 (98.438)
Epoch: [83][20/79]	Time 0.045 (0.083)	Loss 0.0334 (0.0408)	Prec@1 99.219 (99.107)
Epoch: [83][40/79]	Time 0.045 (0.065)	Loss 0.0525 (0.0403)	Prec@1 98.438 (98.971)
Epoch: [83][60/79]	Time 0.046 (0.059)	Loss 0.0157 (0.0402)	Prec@1 100.000 (98.950)
training time:  4.550725698471069
Test: [0/79]	Time 0.818 (0.818)	Loss 0.9480 (0.9480)	Prec@1 72.656 (72.656)
Test: [20/79]	Time 0.028 (0.067)	Loss 1.0748 (1.3340)	Prec@1 72.656 (68.155)
Test: [40/79]	Time 0.031 (0.050)	Loss 1.4403 (1.3831)	Prec@1 60.938 (67.168)
Test: [60/79]	Time 0.032 (0.044)	Loss 1.8652 (1.4149)	Prec@1 61.719 (66.867)
 * Prec@1 66.910
Epoch: [84][0/79]	Time 0.855 (0.855)	Loss 0.0205 (0.0205)	Prec@1 100.000 (100.000)
Epoch: [84][20/79]	Time 0.047 (0.085)	Loss 0.0230 (0.0360)	Prec@1 100.000 (99.144)
Epoch: [84][40/79]	Time 0.046 (0.066)	Loss 0.0455 (0.0355)	Prec@1 97.656 (99.143)
Epoch: [84][60/79]	Time 0.047 (0.059)	Loss 0.0202 (0.0341)	Prec@1 99.219 (99.206)
training time:  4.595573902130127
Test: [0/79]	Time 0.781 (0.781)	Loss 1.0350 (1.0350)	Prec@1 74.219 (74.219)
Test: [20/79]	Time 0.029 (0.073)	Loss 0.9995 (1.3768)	Prec@1 73.438 (68.787)
Test: [40/79]	Time 0.032 (0.053)	Loss 1.4512 (1.3956)	Prec@1 64.844 (67.854)
Test: [60/79]	Time 0.031 (0.046)	Loss 1.7951 (1.4213)	Prec@1 63.281 (67.418)
 * Prec@1 67.410
=> Saving checkpoint for epoch 84, with Prec@1 67.410000.
Epoch: [85][0/79]	Time 0.801 (0.801)	Loss 0.0177 (0.0177)	Prec@1 100.000 (100.000)
Epoch: [85][20/79]	Time 0.046 (0.082)	Loss 0.0781 (0.0368)	Prec@1 99.219 (99.182)
Epoch: [85][40/79]	Time 0.046 (0.064)	Loss 0.0192 (0.0359)	Prec@1 100.000 (99.181)
Epoch: [85][60/79]	Time 0.043 (0.058)	Loss 0.0182 (0.0345)	Prec@1 100.000 (99.180)
training time:  4.494812726974487
Test: [0/79]	Time 0.788 (0.788)	Loss 1.1443 (1.1443)	Prec@1 73.438 (73.438)
Test: [20/79]	Time 0.032 (0.068)	Loss 1.1003 (1.3979)	Prec@1 75.000 (68.713)
Test: [40/79]	Time 0.032 (0.051)	Loss 1.4695 (1.4329)	Prec@1 62.500 (67.854)
Test: [60/79]	Time 0.031 (0.045)	Loss 1.9580 (1.4658)	Prec@1 58.594 (67.405)
 * Prec@1 67.390
Epoch: [86][0/79]	Time 0.815 (0.815)	Loss 0.0167 (0.0167)	Prec@1 100.000 (100.000)
Epoch: [86][20/79]	Time 0.047 (0.082)	Loss 0.0294 (0.0356)	Prec@1 99.219 (99.107)
Epoch: [86][40/79]	Time 0.045 (0.064)	Loss 0.0747 (0.0350)	Prec@1 96.875 (99.123)
Epoch: [86][60/79]	Time 0.045 (0.058)	Loss 0.0512 (0.0355)	Prec@1 99.219 (99.142)
training time:  4.485530376434326
Test: [0/79]	Time 0.813 (0.813)	Loss 1.0603 (1.0603)	Prec@1 75.000 (75.000)
Test: [20/79]	Time 0.032 (0.069)	Loss 1.0275 (1.3593)	Prec@1 75.000 (69.159)
Test: [40/79]	Time 0.032 (0.051)	Loss 1.5013 (1.3854)	Prec@1 61.719 (68.083)
Test: [60/79]	Time 0.031 (0.045)	Loss 1.7923 (1.4177)	Prec@1 64.062 (67.482)
 * Prec@1 67.410
Epoch: [87][0/79]	Time 0.838 (0.838)	Loss 0.0192 (0.0192)	Prec@1 100.000 (100.000)
Epoch: [87][20/79]	Time 0.047 (0.084)	Loss 0.0247 (0.0267)	Prec@1 99.219 (99.554)
Epoch: [87][40/79]	Time 0.046 (0.065)	Loss 0.0134 (0.0235)	Prec@1 100.000 (99.619)
Epoch: [87][60/79]	Time 0.046 (0.059)	Loss 0.0097 (0.0239)	Prec@1 100.000 (99.590)
training time:  4.570274114608765
Test: [0/79]	Time 0.825 (0.825)	Loss 1.0428 (1.0428)	Prec@1 74.219 (74.219)
Test: [20/79]	Time 0.032 (0.068)	Loss 1.0805 (1.3452)	Prec@1 71.094 (68.862)
Test: [40/79]	Time 0.031 (0.051)	Loss 1.5335 (1.3868)	Prec@1 60.938 (68.007)
Test: [60/79]	Time 0.031 (0.044)	Loss 1.8651 (1.4084)	Prec@1 61.719 (67.700)
 * Prec@1 67.690
=> Saving checkpoint for epoch 87, with Prec@1 67.690000.
Epoch: [88][0/79]	Time 0.819 (0.819)	Loss 0.0277 (0.0277)	Prec@1 100.000 (100.000)
Epoch: [88][20/79]	Time 0.044 (0.083)	Loss 0.0305 (0.0298)	Prec@1 100.000 (99.479)
Epoch: [88][40/79]	Time 0.046 (0.065)	Loss 0.0223 (0.0283)	Prec@1 100.000 (99.428)
Epoch: [88][60/79]	Time 0.046 (0.060)	Loss 0.0090 (0.0284)	Prec@1 100.000 (99.398)
training time:  4.593427658081055
Test: [0/79]	Time 0.778 (0.778)	Loss 1.0544 (1.0544)	Prec@1 74.219 (74.219)
Test: [20/79]	Time 0.032 (0.067)	Loss 1.0784 (1.3437)	Prec@1 72.656 (70.015)
Test: [40/79]	Time 0.031 (0.050)	Loss 1.4450 (1.3793)	Prec@1 63.281 (68.998)
Test: [60/79]	Time 0.032 (0.044)	Loss 1.8426 (1.4095)	Prec@1 61.719 (68.366)
 * Prec@1 68.260
=> Saving checkpoint for epoch 88, with Prec@1 68.260000.
Epoch: [89][0/79]	Time 0.855 (0.855)	Loss 0.0230 (0.0230)	Prec@1 100.000 (100.000)
Epoch: [89][20/79]	Time 0.047 (0.085)	Loss 0.0073 (0.0257)	Prec@1 100.000 (99.405)
Epoch: [89][40/79]	Time 0.047 (0.066)	Loss 0.0226 (0.0242)	Prec@1 99.219 (99.447)
Epoch: [89][60/79]	Time 0.046 (0.059)	Loss 0.0113 (0.0245)	Prec@1 100.000 (99.488)
training time:  4.5888190269470215
Test: [0/79]	Time 0.822 (0.822)	Loss 1.1192 (1.1192)	Prec@1 72.656 (72.656)
Test: [20/79]	Time 0.032 (0.068)	Loss 1.0656 (1.3672)	Prec@1 76.562 (69.420)
Test: [40/79]	Time 0.032 (0.050)	Loss 1.5270 (1.4134)	Prec@1 60.938 (68.388)
Test: [60/79]	Time 0.032 (0.044)	Loss 1.8131 (1.4386)	Prec@1 62.500 (68.033)
 * Prec@1 67.810
Epoch: [90][0/79]	Time 0.825 (0.825)	Loss 0.0146 (0.0146)	Prec@1 100.000 (100.000)
Epoch: [90][20/79]	Time 0.047 (0.089)	Loss 0.0300 (0.0223)	Prec@1 99.219 (99.591)
Epoch: [90][40/79]	Time 0.049 (0.069)	Loss 0.0241 (0.0220)	Prec@1 99.219 (99.581)
Epoch: [90][60/79]	Time 0.042 (0.061)	Loss 0.0231 (0.0219)	Prec@1 100.000 (99.590)
training time:  4.684564828872681
Test: [0/79]	Time 0.794 (0.794)	Loss 1.0463 (1.0463)	Prec@1 75.781 (75.781)
Test: [20/79]	Time 0.031 (0.068)	Loss 1.0683 (1.3469)	Prec@1 77.344 (70.015)
Test: [40/79]	Time 0.029 (0.050)	Loss 1.5264 (1.4013)	Prec@1 61.719 (68.331)
Test: [60/79]	Time 0.031 (0.044)	Loss 1.8005 (1.4272)	Prec@1 64.062 (68.020)
 * Prec@1 67.940
Epoch: [91][0/79]	Time 0.831 (0.831)	Loss 0.0136 (0.0136)	Prec@1 100.000 (100.000)
Epoch: [91][20/79]	Time 0.048 (0.084)	Loss 0.0115 (0.0202)	Prec@1 100.000 (99.516)
Epoch: [91][40/79]	Time 0.046 (0.066)	Loss 0.0233 (0.0189)	Prec@1 100.000 (99.695)
Epoch: [91][60/79]	Time 0.047 (0.059)	Loss 0.0177 (0.0195)	Prec@1 100.000 (99.667)
training time:  4.580116033554077
Test: [0/79]	Time 0.803 (0.803)	Loss 1.0690 (1.0690)	Prec@1 73.438 (73.438)
Test: [20/79]	Time 0.032 (0.069)	Loss 1.0733 (1.3472)	Prec@1 78.125 (69.606)
Test: [40/79]	Time 0.032 (0.051)	Loss 1.4654 (1.3912)	Prec@1 62.500 (68.331)
Test: [60/79]	Time 0.030 (0.045)	Loss 1.7967 (1.4142)	Prec@1 61.719 (68.084)
 * Prec@1 68.010
Epoch: [92][0/79]	Time 0.855 (0.855)	Loss 0.0201 (0.0201)	Prec@1 100.000 (100.000)
Epoch: [92][20/79]	Time 0.045 (0.086)	Loss 0.0202 (0.0154)	Prec@1 99.219 (99.851)
Epoch: [92][40/79]	Time 0.046 (0.067)	Loss 0.0379 (0.0181)	Prec@1 99.219 (99.733)
Epoch: [92][60/79]	Time 0.044 (0.060)	Loss 0.0161 (0.0200)	Prec@1 100.000 (99.654)
training time:  4.651980876922607
Test: [0/79]	Time 0.818 (0.818)	Loss 1.0842 (1.0842)	Prec@1 73.438 (73.438)
Test: [20/79]	Time 0.033 (0.069)	Loss 1.0596 (1.3551)	Prec@1 78.906 (69.457)
Test: [40/79]	Time 0.033 (0.051)	Loss 1.4852 (1.3889)	Prec@1 61.719 (68.521)
Test: [60/79]	Time 0.032 (0.045)	Loss 1.7841 (1.4182)	Prec@1 61.719 (68.148)
 * Prec@1 68.190
Epoch: [93][0/79]	Time 0.862 (0.862)	Loss 0.0267 (0.0267)	Prec@1 100.000 (100.000)
Epoch: [93][20/79]	Time 0.047 (0.086)	Loss 0.0514 (0.0195)	Prec@1 97.656 (99.777)
Epoch: [93][40/79]	Time 0.045 (0.067)	Loss 0.0285 (0.0203)	Prec@1 100.000 (99.657)
Epoch: [93][60/79]	Time 0.046 (0.060)	Loss 0.0186 (0.0193)	Prec@1 100.000 (99.705)
training time:  4.659698247909546
Test: [0/79]	Time 0.816 (0.816)	Loss 1.0821 (1.0821)	Prec@1 74.219 (74.219)
Test: [20/79]	Time 0.028 (0.067)	Loss 1.0757 (1.3387)	Prec@1 77.344 (69.308)
Test: [40/79]	Time 0.032 (0.050)	Loss 1.4763 (1.3711)	Prec@1 62.500 (68.388)
Test: [60/79]	Time 0.031 (0.044)	Loss 1.7421 (1.3974)	Prec@1 64.062 (68.212)
 * Prec@1 68.110
Epoch: [94][0/79]	Time 0.801 (0.801)	Loss 0.0156 (0.0156)	Prec@1 100.000 (100.000)
Epoch: [94][20/79]	Time 0.047 (0.081)	Loss 0.0112 (0.0203)	Prec@1 100.000 (99.628)
Epoch: [94][40/79]	Time 0.049 (0.064)	Loss 0.0277 (0.0201)	Prec@1 99.219 (99.581)
Epoch: [94][60/79]	Time 0.045 (0.058)	Loss 0.0161 (0.0204)	Prec@1 100.000 (99.590)
training time:  4.528591632843018
Test: [0/79]	Time 0.780 (0.780)	Loss 1.0838 (1.0838)	Prec@1 72.656 (72.656)
Test: [20/79]	Time 0.032 (0.068)	Loss 1.1365 (1.3758)	Prec@1 74.219 (69.494)
Test: [40/79]	Time 0.032 (0.050)	Loss 1.5343 (1.4127)	Prec@1 62.500 (68.293)
Test: [60/79]	Time 0.032 (0.045)	Loss 1.8155 (1.4412)	Prec@1 62.500 (67.905)
 * Prec@1 67.980
Epoch: [95][0/79]	Time 0.805 (0.805)	Loss 0.0154 (0.0154)	Prec@1 99.219 (99.219)
Epoch: [95][20/79]	Time 0.049 (0.082)	Loss 0.0245 (0.0188)	Prec@1 99.219 (99.628)
Epoch: [95][40/79]	Time 0.047 (0.064)	Loss 0.0109 (0.0181)	Prec@1 100.000 (99.733)
Epoch: [95][60/79]	Time 0.049 (0.059)	Loss 0.0091 (0.0177)	Prec@1 100.000 (99.731)
training time:  4.552664279937744
Test: [0/79]	Time 0.821 (0.821)	Loss 1.0532 (1.0532)	Prec@1 76.562 (76.562)
Test: [20/79]	Time 0.033 (0.076)	Loss 1.0836 (1.3567)	Prec@1 74.219 (69.457)
Test: [40/79]	Time 0.034 (0.055)	Loss 1.4719 (1.3880)	Prec@1 63.281 (68.598)
Test: [60/79]	Time 0.032 (0.048)	Loss 1.8068 (1.4171)	Prec@1 61.719 (68.340)
 * Prec@1 68.180
Epoch: [96][0/79]	Time 0.844 (0.844)	Loss 0.0249 (0.0249)	Prec@1 100.000 (100.000)
Epoch: [96][20/79]	Time 0.048 (0.084)	Loss 0.0165 (0.0146)	Prec@1 99.219 (99.851)
Epoch: [96][40/79]	Time 0.047 (0.066)	Loss 0.0186 (0.0166)	Prec@1 100.000 (99.695)
Epoch: [96][60/79]	Time 0.045 (0.059)	Loss 0.0302 (0.0166)	Prec@1 99.219 (99.731)
training time:  4.599916934967041
Test: [0/79]	Time 0.814 (0.814)	Loss 1.0075 (1.0075)	Prec@1 73.438 (73.438)
Test: [20/79]	Time 0.029 (0.068)	Loss 1.0872 (1.3599)	Prec@1 75.000 (69.345)
Test: [40/79]	Time 0.031 (0.051)	Loss 1.4782 (1.3941)	Prec@1 61.719 (68.464)
Test: [60/79]	Time 0.032 (0.045)	Loss 1.8033 (1.4226)	Prec@1 64.062 (68.033)
 * Prec@1 67.930
Epoch: [97][0/79]	Time 0.814 (0.814)	Loss 0.0196 (0.0196)	Prec@1 99.219 (99.219)
Epoch: [97][20/79]	Time 0.047 (0.083)	Loss 0.0117 (0.0182)	Prec@1 100.000 (99.665)
Epoch: [97][40/79]	Time 0.048 (0.065)	Loss 0.0714 (0.0204)	Prec@1 98.438 (99.581)
Epoch: [97][60/79]	Time 0.046 (0.059)	Loss 0.0178 (0.0184)	Prec@1 99.219 (99.680)
training time:  4.561741352081299
Test: [0/79]	Time 0.773 (0.773)	Loss 1.0546 (1.0546)	Prec@1 74.219 (74.219)
Test: [20/79]	Time 0.031 (0.067)	Loss 1.1237 (1.3645)	Prec@1 73.438 (69.234)
Test: [40/79]	Time 0.032 (0.050)	Loss 1.5358 (1.4012)	Prec@1 62.500 (68.350)
Test: [60/79]	Time 0.031 (0.044)	Loss 1.8035 (1.4289)	Prec@1 64.844 (68.007)
 * Prec@1 67.910
Epoch: [98][0/79]	Time 0.811 (0.811)	Loss 0.0207 (0.0207)	Prec@1 100.000 (100.000)
Epoch: [98][20/79]	Time 0.046 (0.083)	Loss 0.0071 (0.0168)	Prec@1 100.000 (99.777)
Epoch: [98][40/79]	Time 0.046 (0.065)	Loss 0.0174 (0.0164)	Prec@1 100.000 (99.790)
Epoch: [98][60/79]	Time 0.050 (0.059)	Loss 0.0139 (0.0166)	Prec@1 100.000 (99.795)
training time:  4.583105087280273
Test: [0/79]	Time 0.774 (0.774)	Loss 1.0343 (1.0343)	Prec@1 72.656 (72.656)
Test: [20/79]	Time 0.033 (0.067)	Loss 1.1151 (1.3628)	Prec@1 74.219 (69.382)
Test: [40/79]	Time 0.032 (0.050)	Loss 1.4722 (1.3935)	Prec@1 60.938 (68.502)
Test: [60/79]	Time 0.032 (0.044)	Loss 1.7970 (1.4225)	Prec@1 61.719 (68.122)
 * Prec@1 68.030
Epoch: [99][0/79]	Time 0.858 (0.858)	Loss 0.0111 (0.0111)	Prec@1 100.000 (100.000)
Epoch: [99][20/79]	Time 0.044 (0.084)	Loss 0.0104 (0.0194)	Prec@1 100.000 (99.628)
Epoch: [99][40/79]	Time 0.047 (0.065)	Loss 0.0138 (0.0176)	Prec@1 100.000 (99.695)
Epoch: [99][60/79]	Time 0.047 (0.059)	Loss 0.0106 (0.0176)	Prec@1 100.000 (99.705)
training time:  4.571639060974121
Test: [0/79]	Time 0.817 (0.817)	Loss 1.0582 (1.0582)	Prec@1 71.094 (71.094)
Test: [20/79]	Time 0.032 (0.068)	Loss 1.1363 (1.3650)	Prec@1 74.219 (69.122)
Test: [40/79]	Time 0.031 (0.051)	Loss 1.5234 (1.4068)	Prec@1 62.500 (68.274)
Test: [60/79]	Time 0.030 (0.045)	Loss 1.8009 (1.4317)	Prec@1 63.281 (67.943)
 * Prec@1 67.870
training time:  811.3496811389923
| Best accuracy:  68.26