================== Exp 0 ==================
dataset: CIFAR10, model: ResNet18, selection: GraNd, num_ex: 1, epochs: 1, fraction: 0.2, seed: 61145, lr: 0.1, save_path: ./result, resume: , device: cuda, checkpoint_name: CIFAR10_ResNet18_GraNd_exp0_epoch1_2024-03-18 13:57:42.857460_0.2_
Files already downloaded and verified
Files already downloaded and verified
=> Early Training Epoch #0
| Epoch [  0/  1] Iter[  1/391]		Loss: 2.4088
| Epoch [  0/  1] Iter[ 21/391]		Loss: 2.2623
| Epoch [  0/  1] Iter[ 41/391]		Loss: 2.1467
| Epoch [  0/  1] Iter[ 61/391]		Loss: 2.0224
| Epoch [  0/  1] Iter[ 81/391]		Loss: 1.9491
| Epoch [  0/  1] Iter[101/391]		Loss: 1.8348
| Epoch [  0/  1] Iter[121/391]		Loss: 1.7732
| Epoch [  0/  1] Iter[141/391]		Loss: 2.0184
| Epoch [  0/  1] Iter[161/391]		Loss: 1.5860
| Epoch [  0/  1] Iter[181/391]		Loss: 1.6948
| Epoch [  0/  1] Iter[201/391]		Loss: 1.7113
| Epoch [  0/  1] Iter[221/391]		Loss: 1.7539
| Epoch [  0/  1] Iter[241/391]		Loss: 1.4433
| Epoch [  0/  1] Iter[261/391]		Loss: 1.5209
| Epoch [  0/  1] Iter[281/391]		Loss: 1.5563
| Epoch [  0/  1] Iter[301/391]		Loss: 1.6223
| Epoch [  0/  1] Iter[321/391]		Loss: 1.5319
| Epoch [  0/  1] Iter[341/391]		Loss: 1.3888
| Epoch [  0/  1] Iter[361/391]		Loss: 1.4031
| Epoch [  0/  1] Iter[381/391]		Loss: 1.4389
=> Early Training Epoch #0
| Epoch [  0/  1] Iter[  1/391]		Loss: 2.5032
| Epoch [  0/  1] Iter[ 21/391]		Loss: 2.4678
| Epoch [  0/  1] Iter[ 41/391]		Loss: 2.2503
| Epoch [  0/  1] Iter[ 61/391]		Loss: 2.2125
| Epoch [  0/  1] Iter[ 81/391]		Loss: 2.0400
| Epoch [  0/  1] Iter[101/391]		Loss: 1.9836
| Epoch [  0/  1] Iter[121/391]		Loss: 1.8428
| Epoch [  0/  1] Iter[141/391]		Loss: 1.7169
| Epoch [  0/  1] Iter[161/391]		Loss: 1.7685
| Epoch [  0/  1] Iter[181/391]		Loss: 1.7573
| Epoch [  0/  1] Iter[201/391]		Loss: 1.6303
| Epoch [  0/  1] Iter[221/391]		Loss: 1.6453
| Epoch [  0/  1] Iter[241/391]		Loss: 1.5235
| Epoch [  0/  1] Iter[261/391]		Loss: 1.6605
| Epoch [  0/  1] Iter[281/391]		Loss: 1.5732
| Epoch [  0/  1] Iter[301/391]		Loss: 1.4193
| Epoch [  0/  1] Iter[321/391]		Loss: 1.4770
| Epoch [  0/  1] Iter[341/391]		Loss: 1.5046
| Epoch [  0/  1] Iter[361/391]		Loss: 1.4773
| Epoch [  0/  1] Iter[381/391]		Loss: 1.4939
=> Early Training Epoch #0
| Epoch [  0/  1] Iter[  1/391]		Loss: 2.4476
| Epoch [  0/  1] Iter[ 21/391]		Loss: 2.8621
| Epoch [  0/  1] Iter[ 41/391]		Loss: 2.2969
| Epoch [  0/  1] Iter[ 61/391]		Loss: 2.2965
| Epoch [  0/  1] Iter[ 81/391]		Loss: 2.1517
| Epoch [  0/  1] Iter[101/391]		Loss: 2.0214
| Epoch [  0/  1] Iter[121/391]		Loss: 1.9356
| Epoch [  0/  1] Iter[141/391]		Loss: 1.8860
| Epoch [  0/  1] Iter[161/391]		Loss: 1.8523
| Epoch [  0/  1] Iter[181/391]		Loss: 1.8469
| Epoch [  0/  1] Iter[201/391]		Loss: 1.7940
| Epoch [  0/  1] Iter[221/391]		Loss: 1.6556
| Epoch [  0/  1] Iter[241/391]		Loss: 1.7305
| Epoch [  0/  1] Iter[261/391]		Loss: 1.8399
| Epoch [  0/  1] Iter[281/391]		Loss: 1.5786
| Epoch [  0/  1] Iter[301/391]		Loss: 1.6072
| Epoch [  0/  1] Iter[321/391]		Loss: 1.5549
| Epoch [  0/  1] Iter[341/391]		Loss: 1.5182
| Epoch [  0/  1] Iter[361/391]		Loss: 1.3630
| Epoch [  0/  1] Iter[381/391]		Loss: 1.5486
=> Early Training Epoch #0
| Epoch [  0/  1] Iter[  1/391]		Loss: 2.5357
| Epoch [  0/  1] Iter[ 21/391]		Loss: 2.3299
| Epoch [  0/  1] Iter[ 41/391]		Loss: 2.6234
| Epoch [  0/  1] Iter[ 61/391]		Loss: 2.2714
| Epoch [  0/  1] Iter[ 81/391]		Loss: 2.0303
| Epoch [  0/  1] Iter[101/391]		Loss: 2.1220
| Epoch [  0/  1] Iter[121/391]		Loss: 2.0901
| Epoch [  0/  1] Iter[141/391]		Loss: 2.0182
| Epoch [  0/  1] Iter[161/391]		Loss: 1.9174
| Epoch [  0/  1] Iter[181/391]		Loss: 1.9593
| Epoch [  0/  1] Iter[201/391]		Loss: 1.9286
| Epoch [  0/  1] Iter[221/391]		Loss: 2.0027
| Epoch [  0/  1] Iter[241/391]		Loss: 2.0124
| Epoch [  0/  1] Iter[261/391]		Loss: 1.7556
| Epoch [  0/  1] Iter[281/391]		Loss: 1.8039
| Epoch [  0/  1] Iter[301/391]		Loss: 1.7542
| Epoch [  0/  1] Iter[321/391]		Loss: 1.5837
| Epoch [  0/  1] Iter[341/391]		Loss: 1.7556
| Epoch [  0/  1] Iter[361/391]		Loss: 1.6512
| Epoch [  0/  1] Iter[381/391]		Loss: 1.6364
=> Early Training Epoch #0
| Epoch [  0/  1] Iter[  1/391]		Loss: 2.4627
| Epoch [  0/  1] Iter[ 21/391]		Loss: 2.3473
| Epoch [  0/  1] Iter[ 41/391]		Loss: 2.0156
| Epoch [  0/  1] Iter[ 61/391]		Loss: 1.9421
| Epoch [  0/  1] Iter[ 81/391]		Loss: 1.7831
| Epoch [  0/  1] Iter[101/391]		Loss: 1.7419
| Epoch [  0/  1] Iter[121/391]		Loss: 1.5847
| Epoch [  0/  1] Iter[141/391]		Loss: 1.5938
| Epoch [  0/  1] Iter[161/391]		Loss: 1.7003
| Epoch [  0/  1] Iter[181/391]		Loss: 1.7154
| Epoch [  0/  1] Iter[201/391]		Loss: 1.5445
| Epoch [  0/  1] Iter[221/391]		Loss: 1.5745
| Epoch [  0/  1] Iter[241/391]		Loss: 1.3325
| Epoch [  0/  1] Iter[261/391]		Loss: 1.5560
| Epoch [  0/  1] Iter[281/391]		Loss: 1.6080
| Epoch [  0/  1] Iter[301/391]		Loss: 1.5274
| Epoch [  0/  1] Iter[321/391]		Loss: 1.4298
| Epoch [  0/  1] Iter[341/391]		Loss: 1.3355
| Epoch [  0/  1] Iter[361/391]		Loss: 1.4578
| Epoch [  0/  1] Iter[381/391]		Loss: 1.3262
=> Early Training Epoch #0
| Epoch [  0/  1] Iter[  1/391]		Loss: 2.3280
| Epoch [  0/  1] Iter[ 21/391]		Loss: 2.1948
| Epoch [  0/  1] Iter[ 41/391]		Loss: 1.8258
| Epoch [  0/  1] Iter[ 61/391]		Loss: 1.9390
| Epoch [  0/  1] Iter[ 81/391]		Loss: 1.8002
| Epoch [  0/  1] Iter[101/391]		Loss: 1.7621
| Epoch [  0/  1] Iter[121/391]		Loss: 1.6716
| Epoch [  0/  1] Iter[141/391]		Loss: 1.4100
| Epoch [  0/  1] Iter[161/391]		Loss: 1.4888
| Epoch [  0/  1] Iter[181/391]		Loss: 1.4493
| Epoch [  0/  1] Iter[201/391]		Loss: 1.4978
| Epoch [  0/  1] Iter[221/391]		Loss: 1.2314
| Epoch [  0/  1] Iter[241/391]		Loss: 1.3738
| Epoch [  0/  1] Iter[261/391]		Loss: 1.2906
| Epoch [  0/  1] Iter[281/391]		Loss: 1.4493
| Epoch [  0/  1] Iter[301/391]		Loss: 1.2839
| Epoch [  0/  1] Iter[321/391]		Loss: 1.3094
| Epoch [  0/  1] Iter[341/391]		Loss: 1.1554
| Epoch [  0/  1] Iter[361/391]		Loss: 1.2057
| Epoch [  0/  1] Iter[381/391]		Loss: 1.2257
=> Early Training Epoch #0
| Epoch [  0/  1] Iter[  1/391]		Loss: 2.5361
| Epoch [  0/  1] Iter[ 21/391]		Loss: 2.4607
| Epoch [  0/  1] Iter[ 41/391]		Loss: 2.1119
| Epoch [  0/  1] Iter[ 61/391]		Loss: 1.9009
| Epoch [  0/  1] Iter[ 81/391]		Loss: 1.8077
| Epoch [  0/  1] Iter[101/391]		Loss: 1.9050
| Epoch [  0/  1] Iter[121/391]		Loss: 1.7038
| Epoch [  0/  1] Iter[141/391]		Loss: 1.6127
| Epoch [  0/  1] Iter[161/391]		Loss: 1.7346
| Epoch [  0/  1] Iter[181/391]		Loss: 1.5784
| Epoch [  0/  1] Iter[201/391]		Loss: 1.6023
| Epoch [  0/  1] Iter[221/391]		Loss: 1.5504
| Epoch [  0/  1] Iter[241/391]		Loss: 1.3644
| Epoch [  0/  1] Iter[261/391]		Loss: 1.5493
| Epoch [  0/  1] Iter[281/391]		Loss: 1.5942
| Epoch [  0/  1] Iter[301/391]		Loss: 1.3799
| Epoch [  0/  1] Iter[321/391]		Loss: 1.5659
| Epoch [  0/  1] Iter[341/391]		Loss: 1.2875
| Epoch [  0/  1] Iter[361/391]		Loss: 1.4813
| Epoch [  0/  1] Iter[381/391]		Loss: 1.3634
=> Early Training Epoch #0
| Epoch [  0/  1] Iter[  1/391]		Loss: 2.3667
| Epoch [  0/  1] Iter[ 21/391]		Loss: 2.2933
| Epoch [  0/  1] Iter[ 41/391]		Loss: 2.0937
| Epoch [  0/  1] Iter[ 61/391]		Loss: 1.9815
| Epoch [  0/  1] Iter[ 81/391]		Loss: 1.8839
| Epoch [  0/  1] Iter[101/391]		Loss: 1.9097
| Epoch [  0/  1] Iter[121/391]		Loss: 1.7753
| Epoch [  0/  1] Iter[141/391]		Loss: 1.6202
| Epoch [  0/  1] Iter[161/391]		Loss: 1.6833
| Epoch [  0/  1] Iter[181/391]		Loss: 1.6486
| Epoch [  0/  1] Iter[201/391]		Loss: 1.5791
| Epoch [  0/  1] Iter[221/391]		Loss: 1.6051
| Epoch [  0/  1] Iter[241/391]		Loss: 1.4474
| Epoch [  0/  1] Iter[261/391]		Loss: 1.4178
| Epoch [  0/  1] Iter[281/391]		Loss: 1.2879
| Epoch [  0/  1] Iter[301/391]		Loss: 1.2923
| Epoch [  0/  1] Iter[321/391]		Loss: 1.5594
| Epoch [  0/  1] Iter[341/391]		Loss: 1.3446
| Epoch [  0/  1] Iter[361/391]		Loss: 1.3501
| Epoch [  0/  1] Iter[381/391]		Loss: 1.4307
=> Early Training Epoch #0
| Epoch [  0/  1] Iter[  1/391]		Loss: 2.4126
| Epoch [  0/  1] Iter[ 21/391]		Loss: 2.4397
| Epoch [  0/  1] Iter[ 41/391]		Loss: 2.1558
| Epoch [  0/  1] Iter[ 61/391]		Loss: 1.9681
| Epoch [  0/  1] Iter[ 81/391]		Loss: 1.8981
| Epoch [  0/  1] Iter[101/391]		Loss: 1.7599
| Epoch [  0/  1] Iter[121/391]		Loss: 1.7928
| Epoch [  0/  1] Iter[141/391]		Loss: 1.6443
| Epoch [  0/  1] Iter[161/391]		Loss: 1.6697
| Epoch [  0/  1] Iter[181/391]		Loss: 1.7745
| Epoch [  0/  1] Iter[201/391]		Loss: 1.6790
| Epoch [  0/  1] Iter[221/391]		Loss: 1.5885
| Epoch [  0/  1] Iter[241/391]		Loss: 1.6357
| Epoch [  0/  1] Iter[261/391]		Loss: 1.6088
| Epoch [  0/  1] Iter[281/391]		Loss: 1.4355
| Epoch [  0/  1] Iter[301/391]		Loss: 1.5352
| Epoch [  0/  1] Iter[321/391]		Loss: 1.4953
| Epoch [  0/  1] Iter[341/391]		Loss: 1.4478
| Epoch [  0/  1] Iter[361/391]		Loss: 1.4129
| Epoch [  0/  1] Iter[381/391]		Loss: 1.6028
=> Early Training Epoch #0
| Epoch [  0/  1] Iter[  1/391]		Loss: 2.3312
| Epoch [  0/  1] Iter[ 21/391]		Loss: 2.3264
| Epoch [  0/  1] Iter[ 41/391]		Loss: 2.1291
| Epoch [  0/  1] Iter[ 61/391]		Loss: 2.0227
| Epoch [  0/  1] Iter[ 81/391]		Loss: 1.8571
| Epoch [  0/  1] Iter[101/391]		Loss: 1.6678
| Epoch [  0/  1] Iter[121/391]		Loss: 1.6162
| Epoch [  0/  1] Iter[141/391]		Loss: 1.6639
| Epoch [  0/  1] Iter[161/391]		Loss: 1.7359
| Epoch [  0/  1] Iter[181/391]		Loss: 1.3962
| Epoch [  0/  1] Iter[201/391]		Loss: 1.5161
| Epoch [  0/  1] Iter[221/391]		Loss: 1.5842
| Epoch [  0/  1] Iter[241/391]		Loss: 1.4120
| Epoch [  0/  1] Iter[261/391]		Loss: 1.5234
| Epoch [  0/  1] Iter[281/391]		Loss: 1.4825
| Epoch [  0/  1] Iter[301/391]		Loss: 1.3283
| Epoch [  0/  1] Iter[321/391]		Loss: 1.2284
| Epoch [  0/  1] Iter[341/391]		Loss: 1.4098
| Epoch [  0/  1] Iter[361/391]		Loss: 1.3740
| Epoch [  0/  1] Iter[381/391]		Loss: 1.2910
Traceback (most recent call last):
  File "/home/yancheng/code/DeepCore/main.py", line 345, in <module>
    main()
  File "/home/yancheng/code/DeepCore/main.py", line 172, in main
    subset = method.select()
  File "/home/yancheng/code/DeepCore/deepcore/methods/grand.py", line 62, in select
    self.run()
  File "/home/yancheng/code/DeepCore/deepcore/methods/earlytrain.py", line 146, in run
    return self.finish_run()
  File "/home/yancheng/code/DeepCore/deepcore/methods/grand.py", line 42, in finish_run
    targets.to(self.args.device)).sum()
KeyboardInterrupt
Traceback (most recent call last):
  File "/home/yancheng/code/DeepCore/main.py", line 345, in <module>
    main()
  File "/home/yancheng/code/DeepCore/main.py", line 172, in main
    subset = method.select()
  File "/home/yancheng/code/DeepCore/deepcore/methods/grand.py", line 62, in select
    self.run()
  File "/home/yancheng/code/DeepCore/deepcore/methods/earlytrain.py", line 146, in run
    return self.finish_run()
  File "/home/yancheng/code/DeepCore/deepcore/methods/grand.py", line 42, in finish_run
    targets.to(self.args.device)).sum()
KeyboardInterrupt